[
  {
    "objectID": "raster_narrative.html",
    "href": "raster_narrative.html",
    "title": "Raster Data Narrative",
    "section": "",
    "text": "Rasters!\nAll raster operations in this topic are accomplished using the raster library.\nlibrary(raster)\n\nLoading required package: sp\nRaster are representations of continuous, or semi-continuous, data. TYou can envision a raster just like an image. When me make a leaflet() map and how the tiles, each pixel is colored a particular value representing elevation, temperature, precipitation, habitat type, or whatever. This is exactly the same for rasters. The key point here is that each pixel represents some defined region on the earth and as such the raster itself is georeferenced. It has a coordinate reference system (CRS), boundaries, etc."
  },
  {
    "objectID": "raster_narrative.html#making-rasters-de-novo",
    "href": "raster_narrative.html#making-rasters-de-novo",
    "title": "Raster Data Narrative",
    "section": "Making Rasters de novo",
    "text": "Making Rasters de novo\nA raster is simply a matrix with rows and columns and each element has a value associated with it. You can create a raster de novo by making a matrix of data and filling it with values, then turning it into a raster.\nHere I make a raster with random numbrers selected from the Poisson Distribution (fishy, I know) using the rpois() function. I then turn it into a matrix with 7 rows (and 7 columns).\n\nvals &lt;- rpois(49, lambda=12)\nx &lt;- matrix( vals, nrow=7)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   14   10    7   15   17   12   15\n[2,]    9    8   13   13    9   14    8\n[3,]   11    7   15    8   11   10   15\n[4,]   14    9   14   10    8   13   12\n[5,]   15   14    7   13   18   11   11\n[6,]    6   12   13   11    8    7   11\n[7,]   13   10    7   14    8   14   15\n\n\nWhile we haven’t used matrices much thus far, it is a lot like a data.frame with respect to getting and setting values using numerical indices. For example, the value of the 3rd row and 5th column is:\n\nx[3,5]\n\n[1] 11\n\n\nTo convert this set of data, as a matrix, into a geospatially referenced raster() object we do the following:\n\nr &lt;- raster( x )\nr\n\nclass      : RasterLayer \ndimensions : 7, 7, 49  (nrow, ncol, ncell)\nresolution : 0.1428571, 0.1428571  (x, y)\nextent     : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : 6, 18  (min, max)\n\n\nNotice that when I plot it out, it does not show the data, but a summary of the data along with some key data about the contents, including:\n- A class definition\n- The dimensions of the underlying data matrix,\n- The resolution (e.g., the spatial extent of the sides of each pixel). Since we have no CRS here, it is equal to \\(nrows(x)^{-1}\\) and \\(ncols(x)^{-1}\\).\n- The extent (the bounding box) and again since we do not have a CRS defined it just goes from \\(0\\) to \\(1\\). - The crs (missing) - The source can be either memory if the raster is not that big or out of memory if it is just referencing.\nIf these data represent something on the planet, we can assign the dimensions and CRS values to it and use it in our normal day-to-day operations."
  },
  {
    "objectID": "raster_narrative.html#loading-rasters-from-files-or-urls",
    "href": "raster_narrative.html#loading-rasters-from-files-or-urls",
    "title": "Raster Data Narrative",
    "section": "Loading Rasters from Files or URLs",
    "text": "Loading Rasters from Files or URLs\nWe can also grab a raster object from the filesystem or from some online repository by passing the link to the raster() function. Here is the elevation, in meters, of the region in which Mexico is found. To load it in, pass the url.\n\nurl &lt;- \"https://github.com/DyerlabTeaching/Raster-Data/raw/main/data/alt_22.tif\"\nr &lt;- raster( url )\nr\n\nclass      : RasterLayer \ndimensions : 3600, 3600, 12960000  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -120, -90, 0, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : alt_22.tif \nnames      : alt_22 \nvalues     : -202, 5469  (min, max)\n\n\nNotice that this raster has a defined CRS and as such it is projected and the extent relates to the units of the datum (e.g., from -120 to -90 degrees longitude and 0 to 30 degrees latitude).\nIf we plot it, we can see the whole raster.\n\nplot(r)\n\n\n\n\n\n\n\n\nNow, this raster is elevation where there is land but where there is no land, it is full of NA values. As such, there is a ton of them.\n\nformat( sum( is.na( values(r) ) ), big.mark = \",\" )\n\n[1] \"10,490,650\""
  },
  {
    "objectID": "raster_narrative.html#cropping",
    "href": "raster_narrative.html#cropping",
    "title": "Raster Data Narrative",
    "section": "Cropping",
    "text": "Cropping\nOne of the first things to do is to crop the data down to represent the size and extent of our study area. If we over 10 million missing data points (the ocean) and most of Mexico in this raster above but we are only working with sites in Baja California (Norte y Sur), we would do well to excise (or crop) the raster to only include the area we are interested in working with.\nTop do this, we need to figure out a bounding box (e.g., the minimim and maximum values of longitude and latitude that enclose our data). Let’s assume we are working with the Beetle Data from the Spatial Points Slides and load in the Sex-biased dispersal data set and use those points as a starting estimate of the bounding box.\n\nlibrary( sf )\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary( tidyverse )\nbeetle_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Araptus_Disperal_Bias.csv\"\n\nread_csv( beetle_url ) %&gt;%\n  st_as_sf( coords=c(\"Longitude\",\"Latitude\"), crs=4326 ) -&gt; beetles\n\nRows: 31 Columns: 9\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Site\ndbl (8): Males, Females, Suitability, MFRatio, GenVarArapat, GenVarEuphli, L...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beetles )\n\n     Site               Males          Females       Suitability    \n Length:31          Min.   : 9.00   Min.   : 5.00   Min.   :0.0563  \n Class :character   1st Qu.:16.00   1st Qu.:15.50   1st Qu.:0.2732  \n Mode  :character   Median :21.00   Median :21.00   Median :0.3975  \n                    Mean   :25.68   Mean   :23.52   Mean   :0.4276  \n                    3rd Qu.:31.50   3rd Qu.:29.00   3rd Qu.:0.5442  \n                    Max.   :64.00   Max.   :63.00   Max.   :0.9019  \n    MFRatio        GenVarArapat     GenVarEuphli             geometry \n Min.   :0.5938   Min.   :0.0500   Min.   :0.0500   POINT        :31  \n 1st Qu.:0.8778   1st Qu.:0.1392   1st Qu.:0.1777   epsg:4326    : 0  \n Median :1.1200   Median :0.2002   Median :0.2171   +proj=long...: 0  \n Mean   :1.1598   Mean   :0.2006   Mean   :0.2203                     \n 3rd Qu.:1.3618   3rd Qu.:0.2592   3rd Qu.:0.2517                     \n Max.   :2.2000   Max.   :0.3379   Max.   :0.5122                     \n\n\nNow, we can take the bounding box of these points and get a first approximation.\n\nbeetles %&gt;% st_bbox()\n\n      xmin       ymin       xmax       ymax \n-114.29353   23.28550 -109.32700   29.32541 \n\n\nOK, so this is the strict bounding box for these points. This means that the minimum and maximum values for these points are defined by the original locations—for both the latitude and longitude (both minimum and maximum)—we have sites on each of the edges. This is fine here but we could probably add a little bit of a buffer around that bounding box so that we do not have our sites on the very edge of the plot. We can do this by either eyeballing-it to round up to some reasonable area around the points or apply a buffer (st_buffer) to the union of all the points with some distance and then take the boounding box. I’ll go for the former and make it into an extent object.\n\nbaja_extent &lt;- extent( c(-116, -109, 22, 30 ) )\nbaja_extent\n\nclass      : Extent \nxmin       : -116 \nxmax       : -109 \nymin       : 22 \nymax       : 30 \n\n\nThen we can crop() the original raster using this extent object to create our working raster. I can then dump my points onto the same raster plot by indicaating add=TRUE\n\nalt &lt;- crop( r, baja_extent )\nplot(alt)\nplot( beetles[\"Suitability\"], pch=16, add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n⚠️\n\n\n\n   \n\n\nYou need to be careful here. When you use built-in graphics processes in a markdown document such as this and intend to add subsequent plots to an existing plot you cannot run the lines individual. They must be all executed as the whole chunk. So there is no CTRL/CMD + RETURN action here, it will plot the first one and then complain throughout the remaining ones saying something like plot.new has not been called yet. So you have to either knit the whole document or just run the whole chunk to get them to overlay."
  },
  {
    "objectID": "raster_narrative.html#masking",
    "href": "raster_narrative.html#masking",
    "title": "Raster Data Narrative",
    "section": "Masking",
    "text": "Masking\nThere is another way to grab just a portion of the raster—similar to cropping—which is to mask. A mask will not change the size of the raster but just put NA values in the cells that are not in the are of interest. So if we were to just mask above, it would never actually reduce the size of the raster, just add a lot more NA values. However, the setup is the same.\n\nbeetles %&gt;%\n  filter( Site != 32 ) %&gt;%\n  st_union() %&gt;%\n  st_buffer( dist = 1 ) %&gt;%\n  st_convex_hull() -&gt; hull\n\nbaja &lt;- mask( alt, as(hull, \"Spatial\"))\nbaja\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : -202, 1838  (min, max)\n\n\nAnd it looks like.\n\nplot(baja)"
  },
  {
    "objectID": "raster_narrative.html#plotting-with-ggplot",
    "href": "raster_narrative.html#plotting-with-ggplot",
    "title": "Raster Data Narrative",
    "section": "Plotting with GGPlot",
    "text": "Plotting with GGPlot\nAs you may suspect, our old friend ggplot has some tricks up its sleave for us. The main thing here is that ggplot requires a data.frame object and a raster is not a data.frame — Unless we turn it into one (hehehe) using a cool function called rasterToPoints(). This takes the cells of the raster (and underlying matrix) and makes points from it.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  head()\n\n             x        y alt_22\n[1,] -115.7958 29.99583     55\n[2,] -115.7875 29.99583    126\n[3,] -115.7792 29.99583     94\n[4,] -115.7708 29.99583     99\n[5,] -115.7625 29.99583    106\n[6,] -115.7542 29.99583    120\n\n\nHowever, they are not a data.frame but a matrix.\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  class()\n\n[1] \"matrix\" \"array\" \n\n\nSo, if we are going to use this, w need to transform it from a matrix object into a data.frame object. We can do this using the as.data.frame() function. Remember from the lecture on data.frame objects that we can coerce columns of data (either matrix or array) into a data.frame this way.\nSo here it is in one pipe, using the following tricks:\n- Converting raster to points and then to data.frame so it will go into ggplot\n- Renaming the columns of data I am going to keep so I don’t have to make xlab and ylab\n\nalt %&gt;%\n  rasterToPoints() %&gt;%\n  as.data.frame() %&gt;% \n  transmute(Longitude=x,\n            Latitude=y,\n            Elevation=alt_22)  -&gt; alt.df\nhead( alt.df )\n\n  Longitude Latitude Elevation\n1 -115.7958 29.99583        55\n2 -115.7875 29.99583       126\n3 -115.7792 29.99583        94\n4 -115.7708 29.99583        99\n5 -115.7625 29.99583       106\n6 -115.7542 29.99583       120\n\n\nThen we can plot it by:\n- Plotting it using geom_raster() and setting the fill color to the value of elevation. - Making the coordinates equal (e.g., roughtly equal in area for longitude and latitude), and - Applying only a minimal theme.\n\nalt.df %&gt;%\n  ggplot()  + \n  geom_raster( aes( x = Longitude, \n                    y = Latitude, \n                    fill = Elevation) ) + \n  coord_equal() +\n  theme_minimal() -&gt; baja_elevation\n\nbaja_elevation\n\n\n\n\n\n\n\n\nThat looks good but we should probably do something with the colors. There is a built-in terrain.colors() and tell ggplot to use this for the fill gradient.\n\nbaja_elevation + \n  scale_fill_gradientn( colors=terrain.colors(100))\n\n\n\n\n\n\n\n\nOr you can go dive into colors and set your own, you can set up your own gradient for ggplot using independent colors and then tell it where the midpoint is along that gradient and it will do the right thing©.\n\nbaja_elevation + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 1000 ) -&gt; baja_map\nbaja_map\n\n\n\n\n\n\n\n\nNow that looks great. Now, how about overlaying the points onto the plot and indicate the size of the point by the ♂♀ ratio.\n\nbaja_map + \n  geom_sf( aes(size = MFRatio ), \n           data = beetles, \n           color = \"dodgerblue2\",\n           alpha = 0.75) \n\n\n\n\n\n\n\n\nNow that looks nice."
  },
  {
    "objectID": "raster_narrative.html#identifying-points",
    "href": "raster_narrative.html#identifying-points",
    "title": "Raster Data Narrative",
    "section": "Identifying Points",
    "text": "Identifying Points\nYou can get some information from a raster plot interactively by using the click function. This must be done with an active raster plot. After that, you use the click() function to grab what you need. Your mouse will turn from an arrow into a cross hair and you can position it where you like and get information such as the corrdinates (spatial) of the point and the value of the raster pixel at that location.\nIf you do not specify n= in the function then it will continue to collect data until you click outside the graphing area. If you set id=TRUE it will plot the number of the point onto the map so you can see where you had clicked. Since this is interactive, you will not see the process when you execute the code below, but it will look like.\n\nplot( alt )\nclick(alt, xy=TRUE, value=TRUE, n=3 ) -&gt; points\n\n\n\n\nmap with points\n\n\nHere are what the points look like.\n\npoints\n\n          x        y value\n1 -113.6292 28.45417   870\n2 -112.4792 26.85417  1185\n3 -111.2458 24.83750   135\n4 -109.9958 23.48750  1145\n\n\nI’m going to rename the column names\n\npoints %&gt;%\n  transmute( Longitude = x,\n             Latitude = y,\n             Value = value) -&gt; sites\n\nAnd then I can plot those points (using geom_point()) onto our background map.\n\nbaja_map + \n  geom_point( aes(x = Longitude,\n                  y = Latitude, \n                  size = Value), data=sites, color=\"red\") \n\n\n\n\n\n\n\n\nMexellent!"
  },
  {
    "objectID": "raster_narrative.html#reprojecting-rasters",
    "href": "raster_narrative.html#reprojecting-rasters",
    "title": "Raster Data Narrative",
    "section": "Reprojecting Rasters",
    "text": "Reprojecting Rasters\nJust like points, we can reproject the entire raster using the projectRaster function. HJere I am going to project the raster into UTM Zone 12N, a common projection for this part of Mexico from epsg.io.\nUnfortunatly, the raster library does not use epsg codes so we’ll have to use the large description of that projection. See the page for this projection and scroll down to the proj.4 definition.\n\nnew.proj &lt;- \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \"\n\nCopy this into a character variable and then use the projectRaster() function and assign that new value as the CRS.\n\nalt.utm &lt;- projectRaster( alt, crs=new.proj)\nplot( alt.utm, xlab=\"Easting\", ylab=\"Northing\" )\n\n\n\n\n\n\n\n\nEasy."
  },
  {
    "objectID": "raster_narrative.html#raster-operations",
    "href": "raster_narrative.html#raster-operations",
    "title": "Raster Data Narrative",
    "section": "Raster Operations",
    "text": "Raster Operations\nOK, so now we can make and show a raster but what about doing some operations? A raster is just a matrix decorated with more geospatial information. This allows us to do normal R like data manipulations on the underlying data.\nConsider the following question.\n\nWhat are the parts of Baja California that are within 100m of the elevation of site named San Francisquito (sfran)?\n\nTo answer this, we have the following general outline of operations.\n\nFind the coordinates of the site named sfran\n\nExtract the elevation from the alt raster that is within 100m (+/-) of that site.\nPlot the whole baja data as a background\n\nOverlay all the locations within that elevation band.\n\nTo do this we will use both the alt and the beetles data objects.\nFirst, we find out the coordinates of the site.\n\nsfran &lt;- beetles$geometry[ beetles$Site == \"sfran\"]\nsfran\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -112.964 ymin: 27.3632 xmax: -112.964 ymax: 27.3632\nGeodetic CRS:  WGS 84\n\n\nPOINT (-112.964 27.3632)\n\n\nNow, we need to figure out what the value of elevation in the alt raster is at this site. This can be done with the extract() function from the raster library.\nHowever, the this function doesn’t work directly with sf objects so we need to cast it into a Spatial object1. Fortunatly, that is a pretty easy coercion.\n\nraster::extract(alt, as(sfran,\"Spatial\") ) \n\n[1] 305\n\n\n\nWarning: in the above code, I used the function extract() to extract the data from the alt raster for the coordinate of the target locale. However, there is also an extract() function that has been brought in from the dplyr library (as part of tidyverse). In this file, I loaded library(raster) before library(tidyverse) and as such the dplyr::extract() function has overridden the one from raster—they cannot both be available. As a consequence, I use the full name of the function with package::function when I call it as raster::extract() to remove all ambiguity. If I had not, I got a message saying something like, Error in UseMethod(\"extract_\") : no applicable method for 'extract_' applied to an object of class \"c('RasterLayer', 'Raster', 'BasicRaster')\". Now, I know there is an extract() function in raster so this is the dead giveaway that it has been overwritten by a subsequent library call.\n\n\nOption 1 - Manipulate the Raster\nTo work on a raster directly, we can access the values within it using the values() function (I know, these statistican/programmers are quite cleaver).\nSo, to make a copy and make only the values that are +/- 100m of sfran we can.\n\nalt_band &lt;- alt\nvalues( alt_band )[ values(alt_band) &lt;= 205 ] &lt;- NA\nvalues( alt_band )[ values(alt_band) &gt;= 405 ] &lt;- NA\nalt_band\n\nclass      : RasterLayer \ndimensions : 960, 840, 806400  (nrow, ncol, ncell)\nresolution : 0.008333333, 0.008333333  (x, y)\nextent     : -116, -109, 22, 30  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : alt_22 \nvalues     : 206, 404  (min, max)\n\n\nThen we can plot overlay plots of each (notice how I hid the legend for the first alt raster).\n\nplot( alt, col=\"gray\", legend=FALSE, xlab=\"Longitude\", ylab=\"Latitude\")\nplot( alt_band, add=TRUE )\n\n\n\n\n\n\n\n\n\n\nOption 2 - Manipulate the Data Frames\nWe can also proceed by relying upon the data.frame objects representing the elevation. So let’s go back to our the alt.df object and use that in combination with a filter and plot both data.frame objects (the outline of the landscape in gray and the elevation range as a gradient). I then overlay the beetle data with the ratios as sizes and label the locales with ggrepel. Notice here that you can use the sf::geometry object from beetles if you pass it through the st_coordinates function as a statistical tranform making it regular coordinates and not sf objects (yes this is kind of a trick and hack but KEEP IT HANDY!).\n\nlibrary( ggrepel )\nalt.df %&gt;%\n  filter( Elevation &gt;= 205,\n          Elevation &lt;= 405) %&gt;%\n  ggplot() + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude),\n               fill = \"gray80\", \n               data=alt.df ) + \n  geom_raster( aes( x = Longitude,\n                    y = Latitude, \n                    fill = Elevation ) ) + \n  scale_fill_gradient2( low = \"darkolivegreen\",\n                        mid = \"yellow\",\n                        high = \"brown\", \n                        midpoint = 305 ) +\n  geom_sf( aes(size=MFRatio), \n           alpha=0.5, \n           color=\"dodgerblue3\", \n           data=beetles) +\n  geom_text_repel( aes( label = Site,\n                        geometry = geometry),\n                   data = beetles,\n                   stat = \"sf_coordinates\", \n                   size = 4, \n                   color = \"dodgerblue4\") + \n  coord_sf() + \n  theme_minimal() \n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nVery nice indeed.\n\nplot(r)\nclick(r)\n\n\n\n\n\n\n\n\nNULL"
  },
  {
    "objectID": "raster_narrative.html#footnotes",
    "href": "raster_narrative.html#footnotes",
    "title": "Raster Data Narrative",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA Spatial object is from the sp library. This is an older library that is still used by some. It is a robust library but it is put together in a slightly different way that complicates situations a bit, which is not why we are covering it in this topic.↩︎"
  },
  {
    "objectID": "regression_narrative.html",
    "href": "regression_narrative.html",
    "title": "Regression",
    "section": "",
    "text": "If we think of all the variation in a data set, we can partition it into the following components:\n\\[\n\\sigma_{Total}^2 = \\sigma_{Model}^2 + \\sigma_{Residual}^2\n\\]\nIn that some of the underlying variation goes towards explaining the patterns in the data and the rest of the variation is residual (or left over). For regression analyses, consider the simple linear regression model.\n\\[\ny_{ij} = \\beta_0 + \\beta_1 x_{i} + \\epsilon_j\n\\]\nWhere th terms \\(\\beta_0\\) is the where the expected line interscepts the y-axis when \\(x = 0\\), the coefficient \\(\\beta_1\\) is the rate at which the \\(y\\) (the results) changes per unit change in \\(x\\) (the predictor, and \\(\\epsilon\\) is the left over variation (residual) that each point has.\nThe null hypothesis for this kind of regression model is\n\\(H_O: \\beta_1 = 0\\)\nWe could have a hypothesis that \\(\\beta_0 = 0\\) but that is often not that interesting of an idea since that is a constant term in the equation (n.b., we could subtract it out from both sides). If we have more than one predictor variable, the null hypothesis becomes \\(H_O: \\beta_i = 0; \\forall i\\) (that upside down triangle is ‘for all’).\nGraphically, let us look at the following data as an example for basic regression models.\ndf &lt;- data.frame( x = 1:10,\n                  y = c( 15.5, 28.1, 22.3, \n                         32.3, 31.1, 26.8, \n                         41.8, 30.3, 47.9, \n                         41.1) )\nggplot( df, aes(x,y) ) + \n  stat_smooth( method=\"lm\", \n               formula = y ~ x, \n               se=FALSE, \n               color=\"red\", size=0.5) + \n  geom_point() \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nThe notion here is to be estimate the underlying formula for that red line that describes the variation in the original values in how y changes systematically across measured values of x."
  },
  {
    "objectID": "regression_narrative.html#least-squares-fitting",
    "href": "regression_narrative.html#least-squares-fitting",
    "title": "Regression",
    "section": "Least Squares Fitting",
    "text": "Least Squares Fitting\nSo how do we figure this out? One of the most common ways is to uses a methods called Least Squared Distance fitting. To describe this, consider a set of hypothetical random models with random values estimated for both the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) coefficients. These could be close to a good models or not.\n\nmodels &lt;- data.frame( beta0 = runif(250,-20,40),\n                      beta1 = runif(250, -5, 5))\nsummary( models )\n\n     beta0             beta1        \n Min.   :-19.990   Min.   :-4.9989  \n 1st Qu.: -6.765   1st Qu.:-2.1013  \n Median :  7.830   Median : 0.4019  \n Mean   :  9.006   Mean   : 0.2317  \n 3rd Qu.: 23.127   3rd Qu.: 2.7025  \n Max.   : 39.840   Max.   : 4.9964  \n\n\nWe can plot these and the original data and all these randomly defined models.\n\nggplot() + \n  geom_abline( aes(intercept = beta0, \n                   slope = beta1), \n               data = models,\n               alpha = 0.1) + \n  geom_point( aes(x,y), \n              data=df )\n\n\n\n\n\n\n\n\nA least squares fit is one that minimizes the distances of each point (in the y-axis) from the line created by the model. In the graph below, we can see this would be the distances (squared so we do not have positive and negative values) along the y-axis, between each point and the fitted line.\nThe “best model” here is one that minimizes the sum of squared distances distances.\n\n\n\n\n\n\n\n\n\nLet’s look at those hypothetical models. I’m going to make a few little functions to help make the code look easy.\nFirst, here is a function that returns the distances between the original points and a hypothesized regression line defined by an interscept and slope from the original points.\n\nmodel_distance &lt;- function( interscept, slope, X, Y ) {\n  yhat &lt;- interscept + slope * X\n  diff &lt;- Y - yhat\n  return( sqrt( mean( diff ^ 2 ) ) )\n}\n\nNow, let’s go through all the models and estimate the mean squared distances between the proposed line (from intercept and slope) and the original data.\n\nmodels$dist &lt;- NA\nfor( i in 1:nrow(models) ) {\n  models$dist[i] &lt;- model_distance( models$beta0[i],\n                                    models$beta1[i],\n                                    df$x,\n                                    df$y )\n}\nhead( models )\n\n       beta0       beta1      dist\n1 -15.984972  2.21207216 35.942432\n2  32.048510  3.13132571 18.373898\n3  -6.630418 -1.13814142 46.199418\n4  29.774480 -0.07922108  9.668481\n5  34.111651 -0.46184002 10.301177\n6  32.607851  1.08566858  9.698700\n\n\nIf we look through these models, we can see which are better than others by sorting in increasing squared distance.\n\nggplot()  + \n  geom_abline( aes(intercept = beta0,\n                   slope = beta1, \n                   color = -dist),\n               data = filter( models, rank(dist) &lt;= 10 ),\n               alpha = 0.5) + \n  geom_point( aes(x,y),\n              data=df) \n\n\n\n\n\n\n\n\nThese models in the parameter space of intercepts and slopes can be visualized as this. These red-circles are close to where the best models are located.\n\nggplot( models, aes(x = beta0, \n                    y = beta1,\n                    color = -dist)) + \n  geom_point( data = filter( models, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n    geom_point()\n\n\n\n\n\n\n\n\nIn addition to a random search, we can be a bit more systematic about it and make a grid of interscept and slope values, using a grid search.\n\ngrid &lt;- expand.grid( beta0 = seq(15,20, length = 25),\n                     beta1 = seq(2, 3.5, length = 25))\ngrid$dist &lt;- NA\nfor( i in 1:nrow(grid) ) {\n  grid$dist[i] &lt;- model_distance( grid$beta0[i],\n                                  grid$beta1[i],\n                                  df$x,\n                                  df$y )\n}\n\nggplot( grid, aes(x = beta0, \n                  y = beta1,\n                  color = -dist)) + \n  geom_point( data = filter( grid, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n  geom_point()\n\n\n\n\n\n\n\n\nYou could imagine that we could iteratively soom in this grid and find the best fit combination of \\(\\beta_0\\) and \\(\\beta_1\\) values until we converged on a really well fit set.\nThere is a more direct way to get to these results (though is much less pretty to look at) using the lm() linear models function."
  },
  {
    "objectID": "regression_narrative.html#our-friend-lm",
    "href": "regression_narrative.html#our-friend-lm",
    "title": "Regression",
    "section": "Our Friend lm()",
    "text": "Our Friend lm()\nTo specify a potential model, we need to get the function the form we are interested in using.\n\nfit &lt;- lm( y ~ x, data = df )\nfit\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nCoefficients:\n(Intercept)            x  \n     17.280        2.625  \n\n\nWe can see that for the values of the coefficients (labeled Interscept and x), it has a model_distance() of\n\nmodel_distance( -1.76, 3.385, df$x, df$y )\n\n[1] 15.90948\n\n\nwhich we can see is pretty close in terms of the coefficients and has a smaller model distance than those examined in the grid.\n\ngrid %&gt;%\n  arrange( dist ) %&gt;%\n  head( n = 1) \n\n     beta0 beta1     dist\n1 17.29167 2.625 5.240071\n\n\nFortunately, we have a lot of additional information available to us because we used the lm() function."
  },
  {
    "objectID": "regression_narrative.html#model-fit",
    "href": "regression_narrative.html#model-fit",
    "title": "Regression",
    "section": "Model Fit",
    "text": "Model Fit\nWe can estimate a bunch of different models but before we look to see if it well behaved. There are several interesting plots that we can examine from the model object such as:\n\nplot( fit, which = 1 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 2 )\n\n\n\n\n\n\n\n\n\nplot( fit, which = 5 )"
  },
  {
    "objectID": "regression_narrative.html#analysis-of-variance-tables---decomposing-variation",
    "href": "regression_narrative.html#analysis-of-variance-tables---decomposing-variation",
    "title": "Regression",
    "section": "Analysis of Variance Tables - Decomposing Variation",
    "text": "Analysis of Variance Tables - Decomposing Variation\nThus far, we’ve been able to estiamte a model, but is it one that explains a significant amount of variation? To determine this, we use the analysis of variance table.\n\nanova( fit )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1 568.67  568.67  16.568 0.003581 **\nResiduals  8 274.58   34.32                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe terms in this table are:\n\nDegrees of Freedom (df): representing 1 degree of freedom for the model, and N-1 for the residuals.\nSums of Squared Deviations:\n\n\\(SS_{Total} = \\sum_{i=1}^N (y_i - \\bar{y})^2\\)\n\\(SS_{Model} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2\\), and\n\\(SS_{Residual} = SS_{Total} - SS_{Model}\\)\n\nMean Squares (Standardization of the Sums of Squares for the degrees of freedom)\n\n\\(MS_{Model} = \\frac{SS_{Model}}{df_{Model}}\\)\n\\(MS_{Residual} = \\frac{SS_{Residual}}{df_{Residual}}\\)\n\nThe \\(F\\)-statistic is from a known distribution and is defined by the ratio of Mean Squared values.\nPr(&gt;F) is the probability associated the value of the \\(F\\)-statistic and is dependent upon the degrees of freedom for the model and residuals."
  },
  {
    "objectID": "regression_narrative.html#variance-explained",
    "href": "regression_narrative.html#variance-explained",
    "title": "Regression",
    "section": "Variance Explained",
    "text": "Variance Explained\nThere is a correlative measurement in regression models to the Pearson Product Moment Coefficient, (\\(\\rho\\)) in a statistic called \\(R^2\\). This parameter tells you, How much of the observed variation in y is explained by the model?\nThe equation for R^2 is:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}}\n\\]\nThe value of this parameter is bound by 0 (the model explains no variation) and 1.0 (the model explains all the variation in the data). We can get to this and a few other parameters in the regression model by taking its summary.\n\nsummary( fit )\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9836 -4.0182 -0.8709  5.3064  6.9909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   17.280      4.002   4.318  0.00255 **\nx              2.626      0.645   4.070  0.00358 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.859 on 8 degrees of freedom\nMultiple R-squared:  0.6744,    Adjusted R-squared:  0.6337 \nF-statistic: 16.57 on 1 and 8 DF,  p-value: 0.003581\n\n\nJust like the model itself, the summary.lm object also has all these data contained within it in case you need to access them in textual format or to annotate graphical output.\n\nnames( summary( fit ) )\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nNotice that the p-value is not in this list… It is estimable from the fstatistic and df values and here is a quick function that returns the raw p-value by looking up the are under the curve equal to or greater than the observed fstatistic with those degrees of freedom.\n\nget_pval &lt;- function( model ) {\n  f &lt;- summary( model )$fstatistic[1]\n  df1 &lt;- summary( model )$fstatistic[2]\n  df2 &lt;- summary( model )$fstatistic[3]\n  p &lt;- as.numeric( 1.0 - pf( f, df1, df2 ) )\n  return( p  )\n}\n\nget_pval( fit )\n\n[1] 0.0035813\n\n\nAs an often-overlooked side effect, the \\(R^2\\) from a simple one predictor regression model and the correlation coefficient \\(r\\) from cor.test(method='pearson') are related as follows:\n\nc( `Regression R^2` = summary( fit )$r.squared,\n   `Squared Correlation` = as.numeric( cor.test( df$x, df$y )$estimate^2 ) )\n\n     Regression R^2 Squared Correlation \n          0.6743782           0.6743782 \n\n\n(e.g., the square of the correlation estimate \\(r\\) is equal to \\(R^2\\))."
  },
  {
    "objectID": "regression_narrative.html#extensions-of-the-model",
    "href": "regression_narrative.html#extensions-of-the-model",
    "title": "Regression",
    "section": "Extensions of the Model",
    "text": "Extensions of the Model\nThere are several helper functions for dealing with regression models such as finding the predicted values.\n\npredict( fit ) -&gt; yhat \nyhat \n\n       1        2        3        4        5        6        7        8 \n19.90545 22.53091 25.15636 27.78182 30.40727 33.03273 35.65818 38.28364 \n       9       10 \n40.90909 43.53455 \n\n\nAnd we can plot it as:\n\nplot( yhat ~ df$x, type='l', bty=\"n\", col=\"red\" )\n\n\n\n\n\n\n\n\nThe residual values (e.g., the distance between the original data on the y-axis and the fitted regression model).\n\nresiduals( fit ) -&gt; resids\nresids \n\n         1          2          3          4          5          6          7 \n-4.4054545  5.5690909 -2.8563636  4.5181818  0.6927273 -6.2327273  6.1418182 \n         8          9         10 \n-7.9836364  6.9909091 -2.4345455 \n\n\nWe almost always need to look at the residuals of a regression model to help diagnose any potential problems (as shown above in the plots of the raw model itself).\n\nplot( resids ~ yhat, bty=\"n\", xlab=\"Predicted Values\", ylab=\"Residuals (yhat - y)\", pch=16 )\nabline(0, 0, lty=2, col=\"red\")"
  },
  {
    "objectID": "regression_narrative.html#comparing-models",
    "href": "regression_narrative.html#comparing-models",
    "title": "Regression",
    "section": "Comparing Models",
    "text": "Comparing Models\nOK, so we have a model that appears to suggest that the predicted values in x can explain the variation observed in y. Great. But, is this the best model or only one that is sufficiently meh such that we can reject the null hypothesis. How can we tell?\nThere are two parameters that we have already looked at that may help. These are:\n\nThe P-value: Models with smaller probabilities could be considered more informative.\nThe \\(R^2\\): Models that explain more of the variation may be considered more informative.\n\nLet’s start by looking at some airquality data we have played with previously when working on data.frame objects.\n\nairquality %&gt;%\n  select( -Month, -Day ) -&gt; df.air\nsummary( df.air )\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n\n\nLet’s assume that we are interested in trying to explain the variation in Ozone (the response) by one or more of the other variables as predictors.\n\nfit.solar &lt;- lm( Ozone ~ Solar.R, data = df.air )\nanova( fit.solar )\n\nAnalysis of Variance Table\n\nResponse: Ozone\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSolar.R     1  14780 14779.7  15.053 0.0001793 ***\nResiduals 109 107022   981.9                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s look at all the predictors and take a look at both the p-value and R-squared.\n\nfit.temp &lt;- lm( Ozone ~ Temp, data = df.air )\nfit.wind &lt;- lm( Ozone ~ Wind, data = df.air )\n\ndata.frame( Model = c( \"Ozone ~ Solar\",\n                       \"Ozone ~ Temp\",\n                       \"Ozone ~ Wind\"), \n            R2 = c( summary( fit.solar )$r.squared,\n                    summary( fit.temp )$r.squared,\n                    summary( fit.wind )$r.squared ), \n            P = c( get_pval( fit.solar), \n                   get_pval( fit.temp ),\n                   get_pval( fit.wind ) ) ) -&gt; df.models\n\ndf.models %&gt;%\n  arrange( -R2 ) %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\n\n\n\n\n\nSo if we look at these results, we see that in both \\(R^2\\) and \\(P\\), the model with Temp seems to be most explanatory as well as having the lowest probability. But is is significantly better?\nHow about if we start adding more than one variable to the equation so that we now have two variables (multiple regression) with the general model specified as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + beta_2 x_2 + \\epsilon\n\\]\nNow, we are estimating two regression coefficients and an interscept. For three predictors, this gives us 3 more models.\n\nfit.temp.wind &lt;- lm( Ozone ~ Temp + Wind, data = df.air )\nfit.temp.solar &lt;- lm( Ozone ~ Temp + Solar.R, data = df.air )\nfit.wind.solar &lt;- lm( Ozone ~ Wind + Solar.R, data = df.air )\n\nNow, we can add these output to the table.\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind\",\n                                           \"Ozone ~ Temp + Solar\",\n                                           \"Ozone ~ Wind + Solar\" ),\n                                R2 = c( summary( fit.temp.wind )$r.squared,\n                                        summary( fit.temp.solar )$r.squared,\n                                        summary( fit.wind.solar )$r.squared ),\n                                P = c( get_pval( fit.temp.wind),\n                                       get_pval( fit.temp.solar),\n                                       get_pval( fit.wind.solar) )\n                                ))\ndf.models %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Temp + Wind\n0.569\n0.00e+00\n\n\nOzone ~ Temp + Solar\n0.510\n0.00e+00\n\n\nOzone ~ Wind + Solar\n0.449\n9.99e-15\n\n\n\n\n\n\n\nHmmmmmm.\nAnd for completeness, let’s just add the model that has all three predictors\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\n\\]\n\nfit.all &lt;- lm( Ozone ~ Solar.R + Temp + Wind, data = df.air )\n\nNow let’s add that one\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind + Solar\"),\n                                R2 = c( summary( fit.all )$r.squared ),\n                                P = c( get_pval( fit.all)  )\n                                ))\n\n\ndf.models$P = cell_spec( format( df.models$P, \n                                 digits=3, \n                                 scientific=TRUE), \n                         color = ifelse( df.models$P == min(df.models$P), \n                                         \"red\",\n                                         \"black\"))\ndf.models$R2 = cell_spec( format( df.models$R2, \n                                  digits=3, \n                                  scientific=TRUE), \n                          color = ifelse( df.models$R2 == max( df.models$R2), \n                                          \"green\",\n                                          \"black\"))\n\ndf.models %&gt;%\n  mutate( P = format( P, digits=3, scientific = TRUE) ) %&gt;% \n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.  Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\",\n         escape = FALSE) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973. Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n&lt;span style=\" color: black !important;\" &gt;1.21e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;1.79e-04&lt;/span&gt;\n\n\nOzone ~ Temp\n&lt;span style=\" color: black !important;\" &gt;4.88e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind\n&lt;span style=\" color: black !important;\" &gt;3.62e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.27e-13&lt;/span&gt;\n\n\nOzone ~ Temp + Wind\n&lt;span style=\" color: black !important;\" &gt;5.69e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Temp + Solar\n&lt;span style=\" color: black !important;\" &gt;5.10e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\nOzone ~ Wind + Solar\n&lt;span style=\" color: black !important;\" &gt;4.49e-01&lt;/span&gt;\n&lt;span style=\" color: black !important;\" &gt;9.99e-15&lt;/span&gt;\n\n\nOzone ~ Temp + Wind + Solar\n&lt;span style=\" color: green !important;\" &gt;6.06e-01&lt;/span&gt;\n&lt;span style=\" color: red !important;\" &gt;0.00e+00&lt;/span&gt;\n\n\n\n\n\n\n\nSo how do we figure out which one is best?\n\nEffects of Adding Parameters\nBefore we can answer this, we should be clear about one thing. We are getting more variance explained by adding more predictor variables. In fact, by adding any variable, whether they are informative or not, one can explain some amount of the Sums of Squares in a model. Taken to the extreme, this means that we could add an infinite number of explanatory variables to a model and explain all the variation there is!\nHere is an example using our small data set. I’m going to make several models, one of which is the original one and the remaining add one more predeictor varible that is made up of a random variables. We will then look at the \\(R^2\\) of each of these models.\n\nrandom.models  &lt;- list()\nrandom.models[[\"Ozone ~ Temp\"]] &lt;- fit.temp\nrandom.models[[\"Ozone ~ Wind\"]] &lt;- fit.wind\nrandom.models[[\"Ozone ~ Solar\"]] &lt;- fit.solar\nrandom.models[[\"Ozone ~ Temp + Wind\"]] &lt;- fit.temp.wind\nrandom.models[[\"Ozone ~ Temp + Solar\"]] &lt;- fit.temp.solar\nrandom.models[[\"Ozone ~ Wind + Solar\"]] &lt;- fit.wind.solar\nrandom.models[[ \"Ozone ~ Temp + Wind + Solar\" ]] &lt;- fit.all\n\ndf.tmp &lt;- df.air\n\nfor( i in 1:8 ) {\n  lbl &lt;- paste(\"Ozone ~ Temp + Wind + Solar + \", i, \" Random Variables\", sep=\"\")\n  df.tmp[[lbl]] &lt;- rnorm( nrow(df.tmp) )\n  random.models[[lbl]] &lt;- lm( Ozone ~ ., data = df.tmp ) \n}\n\ndata.frame( Models = names( random.models ),\n            R2 = sapply( random.models, \n                          FUN = function( x ) return( summary( x )$r.squared), \n                          simplify = TRUE ),\n            P = sapply( random.models, \n                        FUN = get_pval ) ) -&gt; df.random\n\ndf.random %&gt;%\n  kable( caption = \"Fraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\",\n         digits=4,\n         row.names = FALSE ) %&gt;%\n  kable_paper(\"striped\", full_width = FALSE )\n\n\nFraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\n\n\nModels\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.4877\n0e+00\n\n\nOzone ~ Wind\n0.3619\n0e+00\n\n\nOzone ~ Solar\n0.1213\n2e-04\n\n\nOzone ~ Temp + Wind\n0.5687\n0e+00\n\n\nOzone ~ Temp + Solar\n0.5103\n0e+00\n\n\nOzone ~ Wind + Solar\n0.4495\n0e+00\n\n\nOzone ~ Temp + Wind + Solar\n0.6059\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.6136\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.6162\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.6179\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.6194\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.6287\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.6298\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.6301\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.6371\n0e+00\n\n\n\n\n\n\n\nSo if we just add random data to a model, we get a better fit!!!! Sounds great. That is easy! I can always get the best fit there is!\nThis is a well-known situation in statistics. An in fact, we must be very careful when we are examining the differences between models and attempting to decide which set of models are actually better than other sets of models."
  },
  {
    "objectID": "regression_narrative.html#model-fitting",
    "href": "regression_narrative.html#model-fitting",
    "title": "Regression",
    "section": "Model Fitting",
    "text": "Model Fitting\nTo get around this, we have a few tools at our disposal. The most common approach is to look at the information content in each model relative to the amount of pedictor variables. In essence, we must punish ourselves for adding more predictors so that we do not all run around and add random data to our models. The most common one is called Akaike Information Criterion (AIC), and provide a general framework for comparing several models.\n\\[\nAIC = -2 \\ln L + 2p\n\\]\nWhere \\(L\\) is the log likelihood estimate of the variance and \\(p\\) is the number of parameters. What this does is allow you to evaluate different models with different subsets of parameters. In general, the best model is the one with the smallest value for AIC.\nWe can also evaluate the relative values of all the models by looking in the difference between the “best” model and the rest by taking the difference\n\\[\n\\delta AIC = AIC - min(AIC)\n\\]\nThe prevailing notion is that models that have \\(\\delta AIC &lt; 2.0\\) should be considered as almost equally informative, where as those whose \\(\\delta AIC &gt; 5.0\\) are to be rejected as being informative. That \\(2.0 \\le \\delta AIC \\le 5.0\\) range is where it gets a bit fuzzy.\n\ndf.random$AIC &lt;- sapply( random.models, \n                         FUN = AIC, \n                         simplify = TRUE )\n\ndf.random$deltaAIC = df.random$AIC - min( df.random$A)\n\ndf.random %&gt;%\n  select( -P ) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\",\n         escape = FALSE,\n         row.names = FALSE, \n         digits = 3) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\n\n\nModels\nR2\nAIC\ndeltaAIC\n\n\n\n\nOzone ~ Temp\n0.488\n1067.706\n69.185\n\n\nOzone ~ Wind\n0.362\n1093.187\n94.666\n\n\nOzone ~ Solar\n0.121\n1083.714\n85.193\n\n\nOzone ~ Temp + Wind\n0.569\n1049.741\n51.219\n\n\nOzone ~ Temp + Solar\n0.510\n1020.820\n22.298\n\n\nOzone ~ Wind + Solar\n0.449\n1033.816\n35.294\n\n\nOzone ~ Temp + Wind + Solar\n0.606\n998.717\n0.195\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.614\n998.522\n0.000\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.616\n999.772\n1.251\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.618\n1001.296\n2.775\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.619\n1002.832\n4.310\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.629\n1002.098\n3.576\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.630\n1003.777\n5.256\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.630\n1005.695\n7.173\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.637\n1005.555\n7.033\n\n\n\n\n\n\n\nSo as we look at the data here, we see that the best fit model is the full model though others may be considered as informative and this is where we need to look at the biological importance of variables added to the models."
  },
  {
    "objectID": "aov_narrative.html",
    "href": "aov_narrative.html",
    "title": "Analysis of Variance",
    "section": "",
    "text": "Photo by Tolga Ulkan on Unsplash"
  },
  {
    "objectID": "aov_narrative.html#one-sample-hypotheses",
    "href": "aov_narrative.html#one-sample-hypotheses",
    "title": "Analysis of Variance",
    "section": "One Sample Hypotheses",
    "text": "One Sample Hypotheses\nAt the most basic level, we can take a set of data and test to see if the mean of those values are equated to some particular value, \\(H_O: \\mu = x\\) (or \\(H_O: \\mu = 0\\) in some cases). The idea here is to determine, by specifying a value for the null hypothesis, what we expect the mean value to be equal to. Going back to our idea of hypothesis testing, the null hypothesis is the thing we are trying to disprove (with some level of statistical confidence) and in doing so we need to define a test statistic that we have an idea about its behavior. In this case, we will define Student’s \\(t\\)-test statistic as:\n\\(t =\\frac{\\bar{x}-\\mu}{s_{\\bar{x}}}\\)\nwhere \\(\\bar{x}\\) is the observed mean of the data, \\(\\mu\\) is the mean value specified under the null hypothesis, and \\(s_{\\bar{x}}\\) is the standard deviation of the data. The value of the \\(t\\)-statistic can be defined based upon the sample size (e.g., the degrees of freedom, \\(df\\)). Here is what the probability density function looks like for \\(df = (1,3,\\infty)\\).\n\nlibrary( ggplot2 )\nx &lt;- seq(-5,5,by=0.02)\nd &lt;- data.frame( t=c(x,x,x),\n                  f=c(dt(x,df=1),\n                      dt(x,df=3),\n                      dt(x,df=Inf)),\n                  df=rep(c(\"1\",\"3\",\"Inf\"),each=length(x)))\nggplot( d, aes(x=t,y=f,color=df)) + geom_line() \n\n\n\n\n\n\n\n\nWhen \\(df=\\infty\\) then \\(PDF(t) = Normal\\). As such, we do not need to make corrections to understand the area under the curve, we can just use the normal probability density function. In fact, when \\(df=\\infty\\) then \\(t_{\\alpha,\\infty} = Z_{\\alpha} = \\sqrt{\\chi^2_{\\alpha,df=1}}\\)! The take home message here is that all your statistics become much easier when \\(N=\\infty\\), so go collect some more data!\nFor \\(df &lt; \\infty\\) (all the cases we will be dealing with), we will use the approximation defined by the \\(t\\) distribution. If you look at the distributions above, you see that as we increase the number of samples (e.g., as \\(df\\) increases), the distribution becomes more restricted. The actual function is defined (where \\(df = v\\) for simplicity in nomenclature) as:\n\\(P(t|x,v)= \\frac{ \\Gamma\\left( \\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}\\Gamma\\left( \\frac{v}{2}\\right)} \\left( 1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\\)\nwhere \\(\\Gamma\\) is the Gamma function. Not pretty! Fortunately, we have some built-in facilities in R that can make it easy for us.\nFor a single set of data, we can use the function above to estimate a value of the \\(t\\) statistic. The probability distribution, defined by the degrees of freedom, identifies regions within which we may suspect the statistic to be abnormally large. In our case, though it is quite arbitrary, we can define either one or two regions of the distribution whose values would be extreme enough such that we would consider a significant deviation. For a two-tailed test, the distribution below illustrates this concept. If the estimated value of the \\(t\\) statistic is in either of the shaded regions, we would reject the null hypothesis of \\(H_O: \\mu = 0\\) where \\(\\alpha=0.05\\).\n\nd1 &lt;- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), \n                 f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269))\nd2 &lt;- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064),\n                 f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269))\nd3 &lt;- data.frame( x=c(2.5,-2.5), y=0.02719, label=\"2.5%\")\nggplot() + \n  geom_polygon(aes(t,f),data=d1, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_polygon(aes(t,f),data=d2, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_line( aes(t,f),data=d[d$df==1,], color=\"#F8766D\") + \n  geom_text( aes(x,y,label=label),data=d3)\n\n\n\n\n\n\n\n\nIn R, we can use the t.test() function. I’m going to go back to the Iris data set and use that as it has three categories (the species) and many measurements on sepals and pedals. Here I separate the species into their own data.frame objects.\n\ndf.se &lt;- iris[ iris$Species == \"setosa\",] \ndf.ve &lt;- iris[ iris$Species == \"versicolor\",] \ndf.vi &lt;- iris[ iris$Species == \"virginica\",]\n\nLets look at the Sepal.Length feature in these species and create some hypotheses about it.\n\nggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)\n\n\n\n\n\n\n\n\nWe could test the hypothesis, \\(H_O: mean(Sepal.Length)=6\\) for each of the species.\n\nfit.se &lt;- t.test(df.se$Sepal.Length, mu = 6.0)\nfit.se\n\n\n    One Sample t-test\n\ndata:  df.se$Sepal.Length\nt = -19.94, df = 49, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006 \n\n\nFrom the output, it appears that we can reject that null hypothesis (\\(t =\\) -19.9; \\(df =\\) 49; \\(P =\\) 3.7e-25).\nFor I. versicolor, we see that the mean does appear to be equal to 6.0 (and thus fail to reject the null hypothesis):\n\nt.test( df.ve$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.ve$Sepal.Length\nt = -0.87674, df = 49, p-value = 0.3849\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 5.789306 6.082694\nsample estimates:\nmean of x \n    5.936 \n\n\nand for I. virginica, we find that it is significantly larger than 6.0 and again reject the null hypothesis:\n\nt.test( df.vi$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.vi$Sepal.Length\nt = 6.5386, df = 49, p-value = 3.441e-08\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588 \n\n\nIn all the output, we are also given an estimate of the Confidence Interval around the mean. This confidence interval is determined as:\n\\(\\bar{x} - t_{\\alpha, df} s_{\\bar{x}} &lt; \\mu &lt; \\bar{x} + t_{\\alpha, df} s_{\\bar{x}}\\)\nor the mean plus or minus standard deviation of the data times the value of the \\(t\\)-statistic for a given level of \\(\\alpha\\) and \\(df\\).\n\nData Variability\nThere are times when reporting some confidence around a parameter is important, particularly when using tabular data as output.\n\nSpecies &lt;- c(\"Iris setosa\",\"Iris versicolor\",\"Iris virginia\")\nSepal.Length &lt;- c(mean(df.se$Sepal.Length), mean(df.ve$Sepal.Length), mean( df.vi$Sepal.Length))\nSepal.Length.SE &lt;- c(sd(df.se$Sepal.Length), sd(df.ve$Sepal.Length), sd( df.vi$Sepal.Length))\nSepal.Length.SEM &lt;- Sepal.Length.SE / sqrt(50)\n\nThere are two ways we can talk about the data and it is important for you to think about what you are trying to communicate to your readers. These alternatives include:\n\nsd &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SE, digits=3))\nse &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SEM, digits=3))\ndf &lt;- data.frame( Species, sd, se )\nnames(df) &lt;- c(\"Species\",\"Mean +/- SD\", \"Sepal Length +/- SE\")\nknitr::kable(df,row.names = FALSE,digits = 3,align = \"lcc\")\n\n\n\n\nSpecies\nMean +/- SD\nSepal Length +/- SE\n\n\n\n\nIris setosa\n5.0 +/- 0.352\n5.0 +/- 0.0498\n\n\nIris versicolor\n5.9 +/- 0.516\n5.9 +/- 0.0730\n\n\nIris virginia\n6.6 +/- 0.636\n6.6 +/- 0.0899\n\n\n\n\n\nThe two columns of data tell us something different. The middle column tells us the mean and the standard deviation of the data. This tells us about the variability (and confidence) of the data itself. The last column is the Standard Error of the Mean (\\(\\frac{s}{\\sqrt{N}}\\)) and gives us an idea of the confidence we have about the mean estimate of the data (as opposed to the variation of the data itself). These are two different statements about the data and you need to make sure you are confident about which way you want to use to communicate to your audience."
  },
  {
    "objectID": "aov_narrative.html#two-sample-hypotheses",
    "href": "aov_narrative.html#two-sample-hypotheses",
    "title": "Analysis of Variance",
    "section": "Two Sample Hypotheses",
    "text": "Two Sample Hypotheses\nIn addition to a single sample test, evaluating if the mean of a set of data is equal to some specified value, we can test the equality of two different samples. It may be the case that the average sepal length for I. versicolor is not significantly different than 6.0 whereas I. virginia is. However, this does not mean that the mean of both of these species are significantly different from each other. This is a two-sampled hypothesis, stating that \\(H_O: \\mu_X = \\mu_Y\\).\nVisually, these data look like:\n\ndf &lt;- iris[ (iris$Species %in% c(\"versicolor\",\"virginica\")),]\nggplot( df, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\nwhich clearly overlap in their distributions but are the mean values different? This sets up the null hypothesis:\n\\(H_O: \\mu_1 - \\mu_2 = 0\\)\nUnder this hypothesis, we can use a t-test like before but just rearranged as:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}_1-\\bar{x}_2}}\\)\nAs before, if the difference in the numerator is small we would reject but here we need to standardize the differences in the means by a measure of the standard deviation that is based upon both sets of data. This is called a the standard error of the difference in two means (real catchy title, no?). This is defined as:\n\\(s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{ \\frac{s_1^2}{N_1}+\\frac{s_2^2}{N}}\\)\nTo test this, we use the same approach as before but instead of defining \\(\\mu = 6.0\\) in the t.test() function, we instead give it both data sets.\n\nt.test( x=df.vi$Sepal.Length, y = df.ve$Sepal.Length )\n\n\n    Welch Two Sample t-test\n\ndata:  df.vi$Sepal.Length and df.ve$Sepal.Length\nt = 5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.4220269 0.8819731\nsample estimates:\nmean of x mean of y \n    6.588     5.936 \n\n\nHere we get a few bits of new information from the analysis. It is obvious that we would reject the null hypothesis given the magnitude of the estimated \\(P\\)-value. The output also provides us an estimate of the mean values for each group as well as the confidence around the difference in the mean values. This confidence interval does not overlap 0.0, as it shouldn’t if we reject \\(H_O: \\mu_X = \\mu_Y\\)."
  },
  {
    "objectID": "aov_narrative.html#many-sample-hypotheses",
    "href": "aov_narrative.html#many-sample-hypotheses",
    "title": "Analysis of Variance",
    "section": "Many Sample Hypotheses",
    "text": "Many Sample Hypotheses\nIf we have more than two samples, we could do a bunch of paired \\(t\\)-test statistics but this is not the best idea. In fact, if we do this to our data, each time testing at a confidence level of, say, \\(\\alpha = 0.05\\), then for each time we test at \\(0.05\\) but over all pairs, we test at an overall level of \\(0.05^k\\) (where \\(k\\) is the number of tests) value. We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests. You may have heard about a Bonferroni correction—this does exactly that, it allows us to modify the \\(\\alpha\\) level we use to take into consideration the number of tests we are going to use. While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier.\nConsider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal. The null hypothesis for this is, \\(H_O: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\), where there are \\(k\\) different treatment levels. This is essentially what we’d want to do by doing a bunch of \\(t\\)-tests but we can use another approach that we don’t have to penalize ourselves for multiple tests. Here is how it works.\nIn the Iris data, we can visualize the means and variation around them by using box plots. Here is an example.\n\nggplot( iris, aes(x=Species, y=Sepal.Length)) + \n  geom_boxplot(notch = TRUE) + \n  ylab(\"Sepal Length\")\n\n\n\n\n\n\n\n\nFor us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them. We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares). Formally, we can estimate the sum of squares within each of the \\(K\\) groupings as:\n\\(SS_{Within} = \\sum_{i=1}^K\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\nwhose degrees of freedom are defined as:\n\\(df_{W} = \\sum_{i=1}^K \\left( N_i - 1 \\right) = N-K\\)\nThese parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares:\n\\(SS_{Among} = \\sum_{i=1}^K N_i\\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\nor the deviation among the mean of each treatment compared to the overall mean of all the data. This parameter has degrees of freedom equal to\n\\(df_{A} = K - 1\\)\nThese two parameters describe all the data and as such \\(SS_{Total} = SS_{Within} + SS_{Among}\\). Formally, we see that\n\\(SS_{Total} = \\sum_{i=1}^K\\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\nwhose degrees of freedom are\n\\(df_{T} = N - 1\\)\nFor each of these Sums of Squared deviations, we can standardize them using the degrees of freedom. The notion here is that with more samples, and more treatments, we will have greater \\(SS\\) values. However, if we standardize these parameters by the \\(df\\), we can come up with a standardized Mean Squared values (simplified as \\(MS = \\frac{SS}{df}\\) for each level).\nIf we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation.\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\n\n\n\n\nAmong\n\\(K-1\\)\n\\(\\sum_{i=1}^K N_i \\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\n\\(\\frac{SS_A}{K-1}\\)\n\n\nWithin\n\\(N-K\\)\n\\(\\sum_{i=1}^Kn_i\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\n\\(\\frac{SS_W}{N-K}\\)\n\n\nTotal\n\\(N-1\\)\n\\(\\sum_{i=1}^K \\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\n\n\n\n\nIn R, we can evaluate the equality of means by partitioning our data as depicted above. Essentially, if at least one of our treatments means deviate significantly, then the \\(MS_A\\) will be abnormally large relative to the variation within each treatment \\(MS_W\\). This gives us a statistic, defined by the American statistician Snedekor as:\n\\(F = \\frac{MS_A}{MS_W}\\)\nas an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions. This statistic has an expectation of:\n\\(f(x | df_A, df_W) = \\frac{\\sqrt{\\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\\mathbf{B}\\left( \\frac{df_A}{2}, \\frac{df_W}{2} \\right)}\\)\nwhich is even more of a mess than that for the \\(t\\)-test! Luckily, we have a bit of code to do this for us.\nHere is an example using the Iris data. Here we test the hypothesis that the Sepal Lengths are all the same (e.g., \\(H_O: \\mu_{se} = \\mu_{ve} = \\mu_{vi}\\))\n\nfit.aov &lt;- aov( Sepal.Length ~ Species, data=iris)\nfit.aov\n\nCall:\n   aov(formula = Sepal.Length ~ Species, data = iris)\n\nTerms:\n                 Species Residuals\nSum of Squares  63.21213  38.95620\nDeg. of Freedom        2       147\n\nResidual standard error: 0.5147894\nEstimated effects may be unbalanced\n\n\nThe function called here, aov() is the one that does the Analysis of Variance. It returns an object that has the necessary data we need. To estimate the ANOVA table as outlined above we ask for it as:\n\nanova(fit.aov)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwhich shows that the “Species” treatment are significantly different from each other, with an \\(F\\) statistic equal to \\(F = 119.3\\), which with 2 and 147 degrees of freedom is assigned a probability equal to \\(2e^{-16}\\), a very small value!\n\nPost-Hoc Tests\nWhat this analysis tells us is that at least one of the treatment means are different from the rest. What it does not tell us is which one or which subset. It could be that I. setosa is significantly smaller than both I. versitosa and I. virginia. It could be that I. virginia is significantly larger than the others, who are not different. It could also mean that they are all different. To address this, we can estimate a post hoc test, to evaluate the difference between treatment means within this model itself.\nOne of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey. The so-called “Honest Significant Differences” post hoc test is given by\n\ntuk &lt;- TukeyHSD(fit.aov)\ntuk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nwhich breaks down the pair-wise differences in the mean of each treatment. Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences. In this example, all three comparisons are highly unlikely (e.g., \\(P\\) is very small and in this case essentially zero). As a result, we can interpret these results as suggesting that each of the three species have significantly different. If we plot these results, we see which ones are larger and which are smaller.\n\nplot( tuk )\n\n\n\n\n\n\n\n\nWhich shows the difference between treatment mean between all pairs of treatments. Overall, we see that the Iris species are all significantly different."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "This text is constructed from the narrative documents that supported each learning module in the ENVS 543 Environmental Data Literacy course taught at Virginia Commonwealth University in the Fall of 2024.\n\nSemester course; 3 lecture hours. 3 credits. Enrollment is restricted to students with graduate standing or those with one course in statistics and permission of instructor. Develop quantitative skills for the visualization, manipulation, analysis, and communication of environmental “big data.” This course focuses on spatial environmental data analysis, interpretation, and communication, using real-time data from the Rice Rivers Center and the R statistical analysis environment.\n\n\n\nTo understand data analytics, one needs to recognize the entire workflow. Below is a brief graphical depiction of how analysis actualin the real world. In this class, we will work on all of the components using the open-source R language.\n\nCollect: Getting data from an external source into a format that you use is often the most time-consuming step in the analysis. The content of this class will provide training in data import from local, online, and database sources.\n\nVisualize: Visualizing data is key to understanding. In the image below, notice that the variables X and Y in all the displayed data sets have equivalent means, standard deviations, and correlation up to 2 decimal places! We will emphasize visualization, both static and dynamic, throughout this class.\n\nTransform: Pulling data into your analysis ecosystem is not sufficient. Often the data need to be reformatted and reconfigured before it is actually used.\n\nModel: The application of models to subsets of data is often the step that takes the least amount of effort. However, the application of a model to data is not the endpoint. The model must be visualized and, many times, the underlying data or derivate data must be transformed and submitted to subsequent models.\n\nCommunicate: The effort we put into research and analyses is meaningless without effective communication of your data and findings to a broad audience. Here, we will focus on how to develop effective data communication strategies and formats.\n\n\n\n\nNormal Workflow\n\n\n\n\n\nThe purpose of this course is to help you build your data skills and to develop a foundational understanding upon which subsequent courses will build. The overarching goal here is to develop a working knowledge of the R statistical computing language and enough proficiency to import raw data and then iterate through the visualization, manipulation, and analysis steps in the creation of output that is easily communicated to a scientific audience.\nThe content of this course is built upon the following general student learning objectives (SLO):\n\nSLO 1: Identity, manipulate, and summarize numerical, categorical, ordinal, logical, date, string, and spatial data types.\nSLO 2: Create habits and took the knowledge to support reproducible research.\nSLO 3: Create an informative and effective graphical display of various data types of suitable quality for inclusion in published manuscripts.\nSLO 4: Effectively choose appropriate statistical models based on the types of data at hand and the questions being addressed.\nSLO 5: Demonstrate a general understanding of spatial data types and the creation of both appropriate static and dynamic maps.\n\n\n\n\nThis course is designed as a sequence of individual, stand-alone modules. Each is self-contained and includes a lecture, slides, a larger narrative document, a video demonstration, and an assessment.\n\n\n\n\n\n\n\n\nDeliverable\nDetails\nSLO\n\n\n\n\nWelcome & Logistics\nSetting up the logistics for the class, getting R, RStudio, and Quarto installed on each of your machines, and getting a tour of the IDE.\nNA\n\n\nGit, Github, & Markdown\nEstablish a functional working knowledge of git as a collaborative tool for reproducible research and begin working with Markdown as an output for data analysis.\n2\n\n\nData Types & Containers\nUnderstanding the fundamental data types and containers within R and how to import, work with easily, and export raw data.\n1, 2\n\n\nTidyverse\nData manipulation. Like a boss.\n1, 2\n\n\nBasic Graphics\nNormal graphics built into the R system.\n2, 3\n\n\nGraphics that DON’T suck\nHello publication quality graphics, using the grammar of graphics approach\n2, 3\n\n\nPoints, Lines, & Polygons\nSpatial data in vector format vectors\n3, 5\n\n\nRaster Data\nContinuously distributed spatial data\n3, 5\n\n\nStatistical Confidence\nBase understanding of statistical inferences and the properties of sampled data\n1, 2, 4\n\n\nBinomial Inferences\nAnalyses based upon expectations.\n4\n\n\nCategorical~f(Categorical)\nContingency table and categorical count data\n4\n\n\nContinuous~f(Categorical)\nAnalysis of Variance (or equality of means)\n4\n\n\nContinuous~f(Continuous)\nCorrelation & Regression approaches\n4\n\n\nCategorical~f(Continuous)\nLogistic regression\n4\n\n\n\n\n\n\nThis repository uses the following libraries, which you may not have installed on your computer. When this file is rendered, it will deteremine if there are any libraries lacking and install them for you.\n\n\nCourse Instructor: Professor Rodney Dyer\nEmail: rjdyer@vcu.edu\nWebpage: rodneydyer.com."
  },
  {
    "objectID": "index.html#workflow-data-analysis",
    "href": "index.html#workflow-data-analysis",
    "title": "Preface",
    "section": "",
    "text": "To understand data analytics, one needs to recognize the entire workflow. Below is a brief graphical depiction of how analysis actualin the real world. In this class, we will work on all of the components using the open-source R language.\n\nCollect: Getting data from an external source into a format that you use is often the most time-consuming step in the analysis. The content of this class will provide training in data import from local, online, and database sources.\n\nVisualize: Visualizing data is key to understanding. In the image below, notice that the variables X and Y in all the displayed data sets have equivalent means, standard deviations, and correlation up to 2 decimal places! We will emphasize visualization, both static and dynamic, throughout this class.\n\nTransform: Pulling data into your analysis ecosystem is not sufficient. Often the data need to be reformatted and reconfigured before it is actually used.\n\nModel: The application of models to subsets of data is often the step that takes the least amount of effort. However, the application of a model to data is not the endpoint. The model must be visualized and, many times, the underlying data or derivate data must be transformed and submitted to subsequent models.\n\nCommunicate: The effort we put into research and analyses is meaningless without effective communication of your data and findings to a broad audience. Here, we will focus on how to develop effective data communication strategies and formats.\n\n\n\n\nNormal Workflow"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Preface",
    "section": "",
    "text": "The purpose of this course is to help you build your data skills and to develop a foundational understanding upon which subsequent courses will build. The overarching goal here is to develop a working knowledge of the R statistical computing language and enough proficiency to import raw data and then iterate through the visualization, manipulation, and analysis steps in the creation of output that is easily communicated to a scientific audience.\nThe content of this course is built upon the following general student learning objectives (SLO):\n\nSLO 1: Identity, manipulate, and summarize numerical, categorical, ordinal, logical, date, string, and spatial data types.\nSLO 2: Create habits and took the knowledge to support reproducible research.\nSLO 3: Create an informative and effective graphical display of various data types of suitable quality for inclusion in published manuscripts.\nSLO 4: Effectively choose appropriate statistical models based on the types of data at hand and the questions being addressed.\nSLO 5: Demonstrate a general understanding of spatial data types and the creation of both appropriate static and dynamic maps."
  },
  {
    "objectID": "index.html#course-content-assessment",
    "href": "index.html#course-content-assessment",
    "title": "Preface",
    "section": "",
    "text": "This course is designed as a sequence of individual, stand-alone modules. Each is self-contained and includes a lecture, slides, a larger narrative document, a video demonstration, and an assessment.\n\n\n\n\n\n\n\n\nDeliverable\nDetails\nSLO\n\n\n\n\nWelcome & Logistics\nSetting up the logistics for the class, getting R, RStudio, and Quarto installed on each of your machines, and getting a tour of the IDE.\nNA\n\n\nGit, Github, & Markdown\nEstablish a functional working knowledge of git as a collaborative tool for reproducible research and begin working with Markdown as an output for data analysis.\n2\n\n\nData Types & Containers\nUnderstanding the fundamental data types and containers within R and how to import, work with easily, and export raw data.\n1, 2\n\n\nTidyverse\nData manipulation. Like a boss.\n1, 2\n\n\nBasic Graphics\nNormal graphics built into the R system.\n2, 3\n\n\nGraphics that DON’T suck\nHello publication quality graphics, using the grammar of graphics approach\n2, 3\n\n\nPoints, Lines, & Polygons\nSpatial data in vector format vectors\n3, 5\n\n\nRaster Data\nContinuously distributed spatial data\n3, 5\n\n\nStatistical Confidence\nBase understanding of statistical inferences and the properties of sampled data\n1, 2, 4\n\n\nBinomial Inferences\nAnalyses based upon expectations.\n4\n\n\nCategorical~f(Categorical)\nContingency table and categorical count data\n4\n\n\nContinuous~f(Categorical)\nAnalysis of Variance (or equality of means)\n4\n\n\nContinuous~f(Continuous)\nCorrelation & Regression approaches\n4\n\n\nCategorical~f(Continuous)\nLogistic regression\n4"
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "Preface",
    "section": "",
    "text": "This repository uses the following libraries, which you may not have installed on your computer. When this file is rendered, it will deteremine if there are any libraries lacking and install them for you.\n\n\nCourse Instructor: Professor Rodney Dyer\nEmail: rjdyer@vcu.edu\nWebpage: rodneydyer.com."
  },
  {
    "objectID": "correlation_narrative.html",
    "href": "correlation_narrative.html",
    "title": "Correlation",
    "section": "",
    "text": "This topic is going to focus on developing the theory of correlations.\nConsider the following data consisting of the the decade from 1999 - 2009 and recording the number of movies each year by the actor Nicolas Cage (source IMDB) and the number of people who accidentally died by falling into a swimming pool (source U.S. Centers for Disease Control).\ndf &lt;- data.frame( Year = 1999:2009 )\ndf$`Nicolas Cage Movies` &lt;- c( 2, 2, 2, 3, 1, 1, 2, 3, 4, 1, 4)\ndf$`Drowning Deaths in Pools` &lt;- c( 109, 102, 102, 98, 85, 95, 96, 98, 123, 94, 102 ) \ndf\n\n   Year Nicolas Cage Movies Drowning Deaths in Pools\n1  1999                   2                      109\n2  2000                   2                      102\n3  2001                   2                      102\n4  2002                   3                       98\n5  2003                   1                       85\n6  2004                   1                       95\n7  2005                   2                       96\n8  2006                   3                       98\n9  2007                   4                      123\n10 2008                   1                       94\n11 2009                   4                      102\nIf we look at these data by year, it does not look like there is much of a trend (at least temporally).\nlibrary( reshape2 )\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\ndf %&gt;%\n  melt( id = \"Year\" ) %&gt;%\n  ggplot( aes( Year, value, color = variable) ) + \n  geom_line()  +\n  geom_point()\nHowever, if we look at the two variables together we see an entirely different thing.\ndf %&gt;%\n  ggplot( aes(`Nicolas Cage Movies`, \n              `Drowning Deaths in Pools` ) ) +\n  geom_point( size=2 ) + \n  stat_smooth( formula=y ~ x,\n               method='lm',\n               se=FALSE,\n               color = \"red\",\n               size = 0.5) +\n  geom_text( aes(x=1.5,\n                 y=115,\n                 label = paste( \"Correlation = \", \n                                format( cor( df[,2],\n                                             df[,3]), \n                                        digits=3) ) ) )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_text(aes(x = 1.5, y = 115, label = paste(\"Correlation = \", : All aesthetics have length 1, but the data has 11 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAnd in fact, if we run the statistical test on these data.\ncor.test( df$`Nicolas Cage Movies`, df$`Drowning Deaths in Pools`)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$`Nicolas Cage Movies` and df$`Drowning Deaths in Pools`\nt = 2.6785, df = 9, p-value = 0.02527\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1101273 0.9045101\nsample estimates:\n      cor \n0.6660043\nWe do in fact see a significant (P = 0.025) relationship.\nNow, do we think that because Nicolas Cage makes more movies people are dying at an increased rate? No. These are spurious correlations, though do prove a point about causation."
  },
  {
    "objectID": "correlation_narrative.html#some-new-data",
    "href": "correlation_narrative.html#some-new-data",
    "title": "Correlation",
    "section": "Some New Data",
    "text": "Some New Data\nFor this topic, I thought I would turn to a bit of a more digestible set of data—data describing beer styles! There is a new CSV data set on the GitHub site located at the following URL.\n\nbeer_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Beer_Styles.csv\"\nbeer &lt;- read_csv( beer_url )\n\nRows: 100 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Styles, Yeast\ndbl (10): ABV_Min, ABV_Max, IBU_Min, IBU_Max, SRM_Min, SRM_Max, OG_Min, OG_M...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( beer )\n\n    Styles             Yeast              ABV_Min         ABV_Max      \n Length:100         Length:100         Min.   :2.400   Min.   : 3.200  \n Class :character   Class :character   1st Qu.:4.200   1st Qu.: 5.475  \n Mode  :character   Mode  :character   Median :4.600   Median : 6.000  \n                                       Mean   :4.947   Mean   : 6.768  \n                                       3rd Qu.:5.500   3rd Qu.: 8.000  \n                                       Max.   :9.000   Max.   :14.000  \n    IBU_Min         IBU_Max          SRM_Min         SRM_Max     \n Min.   : 0.00   Min.   :  8.00   Min.   : 2.00   Min.   : 3.00  \n 1st Qu.:15.00   1st Qu.: 25.00   1st Qu.: 3.50   1st Qu.: 7.00  \n Median :20.00   Median : 35.00   Median : 8.00   Median :17.00  \n Mean   :21.97   Mean   : 38.98   Mean   : 9.82   Mean   :17.76  \n 3rd Qu.:25.00   3rd Qu.: 45.00   3rd Qu.:14.00   3rd Qu.:22.00  \n Max.   :60.00   Max.   :120.00   Max.   :30.00   Max.   :40.00  \n     OG_Min          OG_Max          FG_Min          FG_Max     \n Min.   :1.026   Min.   :1.032   Min.   :0.998   Min.   :1.006  \n 1st Qu.:1.040   1st Qu.:1.052   1st Qu.:1.008   1st Qu.:1.012  \n Median :1.046   Median :1.060   Median :1.010   Median :1.015  \n Mean   :1.049   Mean   :1.065   Mean   :1.009   Mean   :1.016  \n 3rd Qu.:1.056   3rd Qu.:1.075   3rd Qu.:1.010   3rd Qu.:1.018  \n Max.   :1.080   Max.   :1.130   Max.   :1.020   Max.   :1.040  \n\n\nThe data consist of the following categories of data. For all but he first two columns of data, a range is given for the appropriate values for each style with Min and Max values.\n\nStyles - The official name of the beer style. Yes, there is an international standard that is officiated by the Beer Judge Certification Program.\nYeast Type - The species of yeast most commonly used for fermenation, consists of top fermenting Ale yeasts and bottom fermenting Lager yeasts.\n\nABV - The amount of alcohol in the finished beer as a percentage of the volume. This is a non-negative numerical value.\nIBU - The ‘International Bitterness Unit’ which roughly measures the amont of \\(\\alpha\\)-acids (asymptotically) added to the beer by the hops. This is a non-negative numerical value, with higher values indicating more bitter beer, though human ability to taste increasingly bitter beer is asymptotic.\nSRM - The Standard Reference Method calibration measuring the color of the finished beer. This is a non-negative integer going from 1 - 40 (light straw color - dark opaque).\nOG - The amount of dissolved sugars in the wort (the pre-beer liquid prior to putting in yeast and the initiation of fermentation), relative to pure water. This is a measurement ‘relative’ to water, which is 1.0. Values less than 1.0 have lower liquid densities than pure water and those greater than 1.0 have more dissolved sugars than pure water.\nFG - The amount of dissolved sugars in the beer after fermentation has been completed. Same as above but the difference in OG and FG can tell us what the ABV should be. Hihger FG beers are more sweet and have more body than lower OG beers (which may appear to have a cleaner, drier, mouth feel—yes that is a real term as well).\n\nAs we talk about correlations, we will use these as examples."
  },
  {
    "objectID": "correlation_narrative.html#parameters-estimates",
    "href": "correlation_narrative.html#parameters-estimates",
    "title": "Correlation",
    "section": "Parameters & Estimates",
    "text": "Parameters & Estimates\nIn statistics, we have two kinds of entities, parameters and estimates, which are dualities of each other. The TRUE mean of a set of data is referred to by \\(\\mu\\) whereas the mean of the data we measured is referred to as \\(\\bar{x}\\). The greek version is the idealized value for the parameter, something that we are striving to find the real estimate of. However, as a Frequentist, we can never actually get to that parameter (remember the actual population of data is infinite but we can only sample a small amount of it) and when we talk about the data associated with what we collect, we refer to it as a estimate and use normal variable names."
  },
  {
    "objectID": "correlation_narrative.html#parametric-assumptions",
    "href": "correlation_narrative.html#parametric-assumptions",
    "title": "Correlation",
    "section": "Parametric Assumptions",
    "text": "Parametric Assumptions\nFor much of the statistics we use, there are underlying assumptions about the form of the data that we shold look at.\n\nTesting for Normality.\n\nThe data can be estimated by a normal density function, or at least can be transformed into data that is reasonably normal in distribution.\n\nThe normal distribution function is defined as:\n[ f(x) = e^{-()} ]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the true value of the underlying mean and standard deviation. This distribution is denoted as \\(N(\\mu,\\sigma)\\) and the differences in the mean value (\\(\\mu\\)) and the variation measured by the standard deviation (\\(\\sigma\\)) are shown below for \\(N(0,1)\\), \\(N(0,5)\\), and \\(N(10,1)\\).\n\nN &lt;- 1000\ndata.frame( Distribution = rep(c(\"N(0,1)\",\"N(10,1)\", \"N(0,5)\"), each=N ),\n            Data = c( rnorm(N,0,1),\n                      rnorm(N,10,1),\n                      rnorm(N,0,5) ) ) %&gt;%\n  ggplot( aes( Data ) ) + \n  geom_histogram( alpha=0.75, \n                  bins = 50) + \n  facet_grid(Distribution ~.)\n\n\n\n\n\n\n\n\nThere are a couple of ways to look at our data to see if they can be considered as normal. First, visually we can plot the theoretical (parameter) quantiles of the data against the sample quantiles using the qqnorm() plot. What this does is sort the data by expectation and observation and plot them and if the data are normal, then they should roughly be in a straight line. The qqline() function shows the expected line (n.b., this is another one of those things where you have to run the whole chunk to get both points and lines on the same graph if you are working in Markdown).\n\nqqnorm( beer$ABV_Min )\nqqline( beer$ABV_Min, col=\"red\")\n\n\n\n\n\n\n\n\nSo, what we commonly see is most of the data falling along the line throughout the middle portion of the distribution and then deviating around the edges. What this does not do is give you a statistic to test to see if we can reject the hypothesis \\(H_O: Data\\;is\\;normal\\). For this, we can use the Shapiro-Wilkes Normality test which produces the statistic:\n[ W = ]\nwhere \\(N\\) is the number of samples, \\(a_i\\) is a standardizing coeeficient, \\(x_i\\) is the \\(i^{th}\\) value of \\(x\\), \\(\\bar{x}\\) is the mean of the observed values, and \\(R_{x_i}\\) is the rank of the \\(x_i^{th}\\) observation.\n\nshapiro.test( beer$ABV_Min )\n\n\n    Shapiro-Wilk normality test\n\ndata:  beer$ABV_Min\nW = 0.94595, p-value = 0.0004532\n\n\nRejection of the null hypothesis (e.g., a small p-value from the test) indicates that the data are not to be considered as coming from a normal distribution. So, for the ABV_Min data above, it appears that it is not actually normally distributed. So what do we do?\n\n\nTransformations\nIf the data are not normal, we can look towards trying to see if we can transform it to a normally distributed variable. There are a lot of\nStudentized Data - One way to standardize the data is to make it have a mean of 0.0 and a standard deviation of 1.0. To do this, we subtract the mean() and divide by the sd().\n\nx &lt;- beer$ABV_Min \nx.std &lt;- (x - mean(x)) / sd( x )\n\nThere are times when this can be a nice way to compare the\nBox Cox - In 1964, Box & Cox defined a family of transformations known as the Box/Cox. This family is defined by a single parameter, \\(\\lambda\\), whose value may vary depending upon the data. The original data, \\(x\\), is then transformed using the following relationship\n[ = ]\nAs long as \\(\\lambda \\ne 0\\) (else we would be dividing by zero, which is not a good thing)!\nOne way to use this transformation is to look at a range of values for \\(\\lambda\\) and determine if the transformation\n\ntest_boxcox &lt;- function( x, lambdas = seq(-1.1, 1.1, by = 0.015) ) {\n  ret &lt;- data.frame( Lambda = lambdas,\n                     W = NA,\n                     P = NA)\n  \n  for( lambda in lambdas ) {\n    x.tilde &lt;- (x^lambda - 1) / lambda   \n    w &lt;- shapiro.test( x.tilde )\n    ret$W[ ret$Lambda == lambda ] &lt;- w$statistic\n    ret$P[ ret$Lambda == lambda ] &lt;- w$p.value\n  }\n  \n  return( ret )\n}\n\nvals &lt;- test_boxcox( beer$ABV_Min ) \n\n\nvals %&gt;%\n  ggplot( aes(Lambda, P) ) + \n  geom_line() + \n  ylab(\"P-Value\")\n\n\n\n\n\n\n\n\nSo if you look at this plot, it shows the P-value of the Shapiro-Wilkes test across a range of values. Depending upon the level of rigor, this approaches the \\(\\alpha = 0.05\\) value closest at:\n\nvals[ which(vals$P == max( vals$P)),]\n\n   Lambda        W          P\n82  0.115 0.973805 0.04351988\n\n\nwith \\(\\lambda = 0.115\\) and a \\(P = 0.044\\).\nArc-Sine Square Root When dealing with fractions, it is common that they do not behave very well when they are very close to 0.0 or 1.0. One of the common transformations to use with these kinds of data is the arc-sin square root transformation. For us, the ABV columns in the data is a percentage (but listed in numerical form as percent not as fraction). So to transform it we can do the following.\n\nabv &lt;- beer$ABV_Min / 100.0\nasin( sqrt( abv ) ) -&gt; abv.1\nshapiro.test( abv.1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  abv.1\nW = 0.96746, p-value = 0.01418"
  },
  {
    "objectID": "correlation_narrative.html#equal-variance",
    "href": "correlation_narrative.html#equal-variance",
    "title": "Correlation",
    "section": "Equal Variance",
    "text": "Equal Variance\nAnother parametric assumption is the equality of variance across a range of the data. This means, for example, that the variance from one part of the experiment should not be different than the variance in samples from another portion of data. We will return to this when we evaluate regression models."
  },
  {
    "objectID": "correlation_narrative.html#independence-of-data",
    "href": "correlation_narrative.html#independence-of-data",
    "title": "Correlation",
    "section": "Independence of Data",
    "text": "Independence of Data\nThe samples you collect, and the way that you design your experiments are most important to ensure that your data are individually independent. You need to think about this very carefully as you design your experiments."
  },
  {
    "objectID": "correlation_narrative.html#parametric-test-pearson-product-moment-correlations",
    "href": "correlation_narrative.html#parametric-test-pearson-product-moment-correlations",
    "title": "Correlation",
    "section": "Parametric Test: Pearson Product Moment Correlations",
    "text": "Parametric Test: Pearson Product Moment Correlations\nBy far, the most common correlation statistic we see is the Pearson Product Moment Correlation, denoted as \\(\\rho\\). For two variables, \\(x\\) and \\(y\\), the correlation parameter is estimated as:\n[ = ]\nThe values of these data fall wihtin the range of: \\(-1 \\le \\rho \\le +1\\) with negative values indicating that when one variable goes up, the other goes down. Positive values of a correlation indicate that both variable change systematically in the same direction (e.g., both up or both down).\nHere are some examples of the distribution of two variables and their associated correlation coefficient.\n\n\n\nFigure 1: Data and associated correlation statistics.\n\n\nSignificance testing for a correlation such as \\(\\rho\\) determine the extent to which we thing the value of is deviant from zero. The Null Hypothesis is \\(H_O: \\rho \\ne 0\\) and can be evaluated using the Student’s t.test. With large enough sample sizes, it can be approximated by:\n[ t = r ]\nHowever, we should probably rely upon R to look up the critical values of the statistic.\nThe default value for cor.test() is the Pearson. Here is an example of its use and the output that we’ve seen before.\n\ncor.test( beer$OG_Max, beer$FG_Max ) -&gt; OG.FG.pearson\nOG.FG.pearson\n\n\n    Pearson's product-moment correlation\n\ndata:  beer$OG_Max and beer$FG_Max\nt = 15.168, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7671910 0.8878064\nsample estimates:\n      cor \n0.8374184 \n\n\nOf particular note are the components associated with the results object that allows you to gain access to specifics for any analysis.\n\nnames( OG.FG.pearson )\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\""
  },
  {
    "objectID": "correlation_narrative.html#non-parametric-test-spearmans-rho",
    "href": "correlation_narrative.html#non-parametric-test-spearmans-rho",
    "title": "Correlation",
    "section": "Non-Parametric Test: Spearman’s Rho",
    "text": "Non-Parametric Test: Spearman’s Rho\nAnother way to de a correlation test that does not rely upon parametric assumptions is to use non-parametric approaches. Most non-parametric tests are based upon ranks of the data rather than the assumption of normality of the data that is necessary for the Pearson Product Moment statistic. One of the constraints for non-parametric statistics is that they are often evaluated for probability based upon permutations.\nThe form of the estimator for this is almost identical to that of the Pearson statistic except that instead of the raw data, we are replacing values with the ranks of each value instead. In doing so, there is a loss of the breadth of the raw data since we are just using ranks, and if the underlying data are poorly behaved because of outliers or other issues, this takes care of it.\n[ _{Spearman} = ]\nWith the same data, it does provide potentially different estimates of the amount of correlation between the variables.\n\nOG.FG.spearman &lt;- cor.test( beer$OG_Max, beer$FG_Max, \n                            method = \"spearman\" )\n\nWarning in cor.test.default(beer$OG_Max, beer$FG_Max, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nOG.FG.spearman\n\n\n    Spearman's rank correlation rho\n\ndata:  beer$OG_Max and beer$FG_Max\nS = 39257, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7644328"
  },
  {
    "objectID": "correlation_narrative.html#permutation-testing-for-significance",
    "href": "correlation_narrative.html#permutation-testing-for-significance",
    "title": "Correlation",
    "section": "Permutation Testing for Significance",
    "text": "Permutation Testing for Significance\nIn both of the previous methods, we used specific approaches to evaluate the significance of the statistic. For Pearson, we approximated using the \\(t\\). For the Spearman test with small numbers of samples, an approximation of the \\(t\\) test is used, based upon counting ranks and the number of ways we can get different combinations of ranks. For larger sample size tests using Spearman, an approximation using the \\(t\\) test can be used.\nAnother way of doing this is based upon permutation and this approach can be applied to a wide array of questions. For correlation’s, if we consider the null hypothesis \\(H_O: \\rho = 0\\) we can make a few inferences. If this hypothesis is true then we are, essentially, saying that the current relationship between \\(x_i\\) and \\(y_i\\) has no intrinsic relationship as there is no correlation. This is, by default, what the null hypothesis says.\nIf that is true, however, that means that any permutation of one of the variables, say \\(y\\), should produce a correlation statistic that is just as large as any other permutation of the data. This is key.\nSo, if we assume the \\(H_O\\) is true then we should be able to shuffle one of the data and estimate a correlation statistic a large number of times. We can then create a permuted distribution of values for the correlation, Assuming the NULL Hypothesis is true. To this distribution, we can evaluate the magnitude of the original correlation. Here is an example using the data from above.\n\nx &lt;- beer$OG_Max\ny &lt;- beer$FG_Max\ndf &lt;- data.frame( Estimate = factor( c( \"Original\",\n                                        rep(\"Permuted\", 999))), \n                  rho =  c( cor.test( x, y )$estimate,\n                            rep(NA, 999)) )\n\nsummary( df )\n\n     Estimate        rho        \n Original:  1   Min.   :0.8374  \n Permuted:999   1st Qu.:0.8374  \n                Median :0.8374  \n                Mean   :0.8374  \n                3rd Qu.:0.8374  \n                Max.   :0.8374  \n                NA's   :999     \n\n\nNow, we can go through the 999 NA values we put into that data frame and:\n1. Permute one of the variables 2. Run the analysis\n3. Store the statistic.\n\nfor( i in 2:1000) {\n  yhat &lt;- sample( y,   # this shuffles the data in y\n                  size = length(y), \n                  replace = FALSE)\n  model &lt;- cor.test( x, yhat )\n  df$rho[i] &lt;- model$estimate \n}\n\nNow we can look at the distribution of permuted values and the original one and see the relationship. If:\n\nThe observed value is within the body of the permuted values, then it is not too rare—given \\(H_O\\), or\nIf the observed value is way outside those permuted values, then it appears to be somewhat rare.\n\n\nggplot( df ) + \n  geom_histogram( aes(rho, fill=Estimate ) )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIf you look at the graph above, you see that the original value is way bigger than the values that would be found if and only if \\(H_O\\) were true. This suggests that the correlation is not zero and in fact it is the largest observation of the 1000 observations (a P estimate of \\(\\frac{1}{1000}\\)…)."
  },
  {
    "objectID": "classic_graphics_narrative.html",
    "href": "classic_graphics_narrative.html",
    "title": "Basic R Graphics",
    "section": "",
    "text": "Data visualization is key to providing effective communication of your data."
  },
  {
    "objectID": "classic_graphics_narrative.html#the-data",
    "href": "classic_graphics_narrative.html#the-data",
    "title": "Basic R Graphics",
    "section": "The Data",
    "text": "The Data\nThe iris flower data set (also known as Fisher’s Iris data set) is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper entitled, The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\nThese data are part of the base R distribution and contain sepal and pedal measurements for three species if congeneric plants, Iris setosa, I. versicolor, and I. virginica.\n\n\n\nThe three species of iris in the default data set.\n\n\nHere is what the data summary looks like.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "classic_graphics_narrative.html#basic-plotting-in-r",
    "href": "classic_graphics_narrative.html#basic-plotting-in-r",
    "title": "Basic R Graphics",
    "section": "Basic Plotting in R",
    "text": "Basic Plotting in R\nThe base R comes with several built-in plotting functions, each of which is accessed through a single function with a wide array of optional arguments that modify the overall appearance.\nHistograms - The Density of A Single Data Vector\n\nhist( iris$Sepal.Length)\n\n\n\n\n\n\n\n\nYou can see that the default values for the hist() function label the x-axis & title on the graph have the names of the variable passed to it, with a y-axis is set to “Frequency”.\n\nxlab & ylab: The names attached to both x- and y-axes.\nmain: The title on top of the graph.\nbreaks: This controls the way in which the original data are partitioned (e.g., the width of the bars along the x-axis).\n\nIf you pass a single number, n to this option, the data will be partitioned into n bins.\nIf you pass a sequence of values to this, it will use this sequence as the boundaries of bins.\n\ncol: The color of the bar (not the border)\nprobability: A flag as either TRUE or FALSE (the default) to have the y-axis scaled by total likelihood of each bins rather than a count of the numbrer of elements in that range.\n\nDensity - Estimating the continuous density of data\n\nd_sepal.length &lt;- density( iris$Sepal.Length )\nd_sepal.length\n\n\nCall:\n    density.default(x = iris$Sepal.Length)\n\nData: iris$Sepal.Length (150 obs.); Bandwidth 'bw' = 0.2736\n\n       x               y            \n Min.   :3.479   Min.   :0.0001495  \n 1st Qu.:4.790   1st Qu.:0.0341599  \n Median :6.100   Median :0.1534105  \n Mean   :6.100   Mean   :0.1905934  \n 3rd Qu.:7.410   3rd Qu.:0.3792237  \n Max.   :8.721   Max.   :0.3968365  \n\n\nThe density() function estimates a continuous probability density function for the data and returns an object that has both x and y values. In fact, it is a special kind of object.\n\nclass(d_sepal.length)\n\n[1] \"density\"\n\n\nBecause of this, the general plot() function knows how to plot these kinds of things.\n\nplot( d_sepal.length )\n\n\n\n\n\n\n\n\nNow, the general plot() function has A TON of options and is overloaded to be able to plot all kinds of data. In addition to xlab and ylab, we modify the following:\n\ncol: Color of the line.\nlwd: Line width\nbty: This covers the ‘box type’, which is the square box around the plot area. I typically use bty=\"n\" because I hate those square boxes around my plots (compare the following 2 plots to see the differences). But you do you.\nxlim & ylim: These dictate the range on both the x- and y-axes. It takes a pair of values such as c(min,max) and then limits (or extends) that axis to to fill that range.\n\nScatter Plots - Plotting two variables\n\nplot( iris$Sepal.Length, iris$Sepal.Width  )\n\n\n\n\n\n\n\n\nHere is the most general plot(). The form of the arguments to this function are x-data and then y-data. The visual representation of the data is determined by the optional values you pass (or if you do not pass any optional values, the default is the scatter plot shown above)\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntype\nThe kind of plot to show (’p’oint, ’l’ine, ’b’oth, or ’o’ver). A point plot is the default.\n\n\npch\nThe character (or symbol) being used to plot. There 26 recognized general characters to use for plotting. The default is pch=1.\n\n\ncol\nThe color of the symbols/lines that are plot.\n\n\ncex\nThe magnification size of the character being plot. The default is cex=1 and deviation from that will increase (\\(cex &gt; 1\\)) or decrease (\\(0 &lt; cex &lt; 1\\)) the scaling of the symbols.\n\n\nlwd\nThe width of any lines in the plot.\n\n\nlty\nThe type of line to be plot (solid, dashed, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the relevant things you can use the parameter pch for is to differentiate between groups of observations (such as different species for example). Instead of giving it one value, pass it a vector of values whose length is equal to that for x- and y-axis data.\nHere is an example where I coerce the iris$Species data vector into numeric types and use that for symbols.\n\nsymbol &lt;- as.numeric(iris$Species)\nsymbol\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\nplot( iris$Sepal.Length, iris$Sepal.Width, pch=symbol )\n\n\n\n\n\n\n\n\nWe can use the same technique to use col instead of pch. Here I make a vector of color names and then use the previously defined in the variable symbol.\n\nraw_colors &lt;- c(\"red\",\"gold\",\"forestgreen\")\ncolors &lt;- raw_colors[ symbol ]\ncolors[1:10]\n\n [1] \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\"\n\n\nIn addition to the general form for the function plot(x,y) we used above, we can use an alternative designation based upon what is called the functional form. The functional form is how we designate functions in R, such as regression anlaysis. This basic syntax for this is y ~ x, that is the response variable (on the y-axis) is a function of the predictor (on the x-axis).\nFor simplicty, I’ll make x and y varibles pointing to the same same data as in the previous graph.\n\ny &lt;- iris$Sepal.Width\nx &lt;- iris$Sepal.Length\n\nThen, the plot() function can be written as (including all the fancy additional stuff we just described):\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\n\n\n\n\n\n\n\n\nThis is much easier to read (also notice how I used serveral lines to put in all the options to the plot function for legibility).\nBar Plots - Quantifying Counts\nThe barplot function takes a set of heights, one for each bar. Let’s quickly grab the mean length for sepals across all three species. There are many ways to do this, here are two, the first being more pedantic and the second more concise.\nThe iris data is in a data.frame that has a column designating the species. We can see which ones using unique().\n\nunique( iris$Species )\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n\nTo estimate the mean for each species, we can take values in iris$Sepal.Length for each level of iris$Species using indices.\n\nmu.Setosa &lt;- mean( iris$Sepal.Length[ iris$Species == \"setosa\" ])\nmu.Versicolor &lt;- mean( iris$Sepal.Length[ iris$Species == \"versicolor\" ])\nmu.Virginica &lt;- mean( iris$Sepal.Length[ iris$Species == \"virginica\" ])\n\nmeanSepalLength &lt;- c( mu.Setosa, mu.Versicolor, mu.Virginica )\nmeanSepalLength\n\n[1] 5.006 5.936 6.588\n\n\nWhen we plot these data using barplot() we pass the values and set the names of the bars us\n\nbarplot( meanSepalLength, \n         names.arg = c(\"setosa\",\"versicolor\",\"virginica\"), \n         xlab=\"Iris Species\",\n         ylab=\"Mean Sepal Length\")\n\n\n\n\n\n\n\n\nThe second way to do this is to use the by() function (see ?by for the complete help file). The by function takes the following objects:\n\nThe raw data to use as measurements. Here we will use iris$Sepal.Length as the raw data.\nData designating groups to partition the raw data into (we will use iris$Species).\nThe function that you want to use on each group. (here we will ask for the mean).\n\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nThe data returned from this function is both numeric and has a name set for each value.\n\nis.numeric( meanSepalLength )\n\n[1] TRUE\n\nnames( meanSepalLength )\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThen when we pass that to barplot() the column labels are set automatically (e.g., no need to set names.arg as above).\n\nbarplot( meanSepalLength, \n         xlab = \"Iris Species\",\n         ylab = \"Average Sepal Length\")\n\n\n\n\n\n\n\n\nBoxplots - High density information\nA boxplot contains a high amount of information content and is appropriate when the groupings on the x-axis are categorical. For each category, the graphical representation includes:\n\nThe median value for the raw data\nA box indicating the area between the first and third quartile (e.g,. the values enclosing the 25% - 75% of the data). The top and bottoms are often referred to as the hinges of the box.\nA notch (if requested), represents confidence around the estimate of the median.\nWhiskers extending out to shows \\(\\pm 1.5 * IQR\\) (the Inner Quartile Range)\nAny points of the data that extend beyond the whiskers are plot as points.\n\nFor legibility, we can use the functional form for the plots as well as separate out the data.frame from the columns using the optional data= argument.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         notch=TRUE, \n         ylab=\"Sepal Length\" )"
  },
  {
    "objectID": "classic_graphics_narrative.html#colors",
    "href": "classic_graphics_narrative.html#colors",
    "title": "Basic R Graphics",
    "section": "Colors",
    "text": "Colors\nNamed Colors - There are 657 pre-defined, named colors built into the base R distribution. Here is a random selection of those values.\n\nrandomColors &lt;- sample( colors(), size = nrow(iris) )\nhead(randomColors)\n\n[1] \"olivedrab2\"   \"grey35\"       \"peachpuff1\"   \"lightyellow4\" \"azure\"       \n[6] \"grey13\"      \n\n\nTo use these colors, you can specify them by name for either all the elements\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nor for each element individually.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1:3],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nHex Colors: You can also use hexadecimal representations of colors, which is most commonly used on the internet. A hex representation of colors consists of red, green, and blue values encoded as numbers in base 16 (e.g., the single digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F). There are a lot of great resources on the internet for color themes that report red, green, blue and hex values. I often use the coolors.co website to look for themes that go well together for slides or presentations.\nColor Brewer Finally, there is an interesting website at colorbrewer2.org that has some interesting built-in palettes. There is an associated library that makes creating palettes for plots really easy and as you get more expreienced with R, you will find this very helpful. For quick visualizations and estimation of built-in color palettes, you can look at the website (below).\n or look at the colors in R\n\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nThere are three basic kinds of palettes: divergent, qualitative, and sequential. Each of these built-in palletes has a maximum number of colors available (though as you see below we can use them to interpolate larger sets) as well as indications if the palette is safe for colorblind individuals.\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE\n\n\nIt is very helpful to look at the different kinds of data palettes available and I’ll show you how to use them below when we color in the states based upon population size at the end of this document."
  },
  {
    "objectID": "classic_graphics_narrative.html#annotations",
    "href": "classic_graphics_narrative.html#annotations",
    "title": "Basic R Graphics",
    "section": "Annotations",
    "text": "Annotations\nYou can easily add text onto a graph using the text() function. Here is the correlation between the sepal length and width (the function cor.test() does the statistical test).\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nWe can put the correlation and the p-value on the plot\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThe we can the overlay this onto an existing plot. For the text() function, we need to give the x- and y- coordinates where you want it put onto the coordinate space of the existing graph.\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\ntext( 7.4, 4.2, cor.text )"
  },
  {
    "objectID": "tidyverse_narrative.html#the-tidyverse-approach",
    "href": "tidyverse_narrative.html#the-tidyverse-approach",
    "title": "Tidyverse",
    "section": "The Tidyverse Approach",
    "text": "The Tidyverse Approach\nThis is the first introduction to tidyverse and is the key skill necessary to become proficient at data analysis.\n\nlibrary( tidyverse )\nlibrary( lubridate )\n\n\nThe Data\nFor this topic we will use some example data from the Rice Rivers Center. These data represent both atmospheric and water data collected from instrumentation on-site. I have stored these data in a spreadsheet that is shared on Google Drive as a CSV file.\nYou can look at it here.\n\n\nThe Data in R\nSo let’s load it into memory and take a look at it.\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/1Mk1YGH9LqjF7drJE-td1G_JkdADOU0eMlrP01WFBT8s/pub?gid=0&single=true&output=csv\"\nrice &lt;- read_csv( url )\n\nRows: 8199 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): DateTime\ndbl (22): RecordID, PAR, WindSpeed_mph, WindDir, AirTempF, RelHumidity, BP_H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( rice )\n\n   DateTime            RecordID          PAR           WindSpeed_mph   \n Length:8199        Min.   :43816   Min.   :   0.000   Min.   : 0.000  \n Class :character   1st Qu.:45866   1st Qu.:   0.000   1st Qu.: 2.467  \n Mode  :character   Median :47915   Median :   0.046   Median : 4.090  \n                    Mean   :47915   Mean   : 241.984   Mean   : 5.446  \n                    3rd Qu.:49964   3rd Qu.: 337.900   3rd Qu.: 7.292  \n                    Max.   :52014   Max.   :1957.000   Max.   :30.650  \n                                                                       \n    WindDir          AirTempF       RelHumidity        BP_HG      \n Min.   :  0.00   Min.   : 3.749   Min.   :15.37   Min.   :29.11  \n 1st Qu.: 37.31   1st Qu.:31.545   1st Qu.:42.25   1st Qu.:29.87  \n Median :137.30   Median :37.440   Median :56.40   Median :30.01  \n Mean   :146.20   Mean   :38.795   Mean   :58.37   Mean   :30.02  \n 3rd Qu.:249.95   3rd Qu.:46.410   3rd Qu.:76.59   3rd Qu.:30.21  \n Max.   :360.00   Max.   :74.870   Max.   :93.00   Max.   :30.58  \n                                                                  \n    Rain_in            H2O_TempC       SpCond_mScm      Salinity_ppt   \n Min.   :0.0000000   Min.   :-0.140   Min.   :0.0110   Min.   :0.0000  \n 1st Qu.:0.0000000   1st Qu.: 3.930   1st Qu.:0.1430   1st Qu.:0.0700  \n Median :0.0000000   Median : 5.450   Median :0.1650   Median :0.0800  \n Mean   :0.0008412   Mean   : 5.529   Mean   :0.1611   Mean   :0.0759  \n 3rd Qu.:0.0000000   3rd Qu.: 7.410   3rd Qu.:0.1760   3rd Qu.:0.0800  \n Max.   :0.3470000   Max.   :13.300   Max.   :0.2110   Max.   :0.1000  \n                     NA's   :1        NA's   :1        NA's   :1       \n       PH           PH_mv        Turbidity_ntu       Chla_ugl    \n Min.   :6.43   Min.   :-113.8   Min.   :  6.20   Min.   :  1.3  \n 1st Qu.:7.50   1st Qu.: -47.8   1st Qu.: 15.50   1st Qu.:  3.7  \n Median :7.58   Median : -43.8   Median : 21.80   Median :  6.7  \n Mean   :7.60   Mean   : -44.5   Mean   : 24.54   Mean   :137.3  \n 3rd Qu.:7.69   3rd Qu.: -38.9   3rd Qu.: 30.30   3rd Qu.:302.6  \n Max.   :9.00   Max.   :  28.5   Max.   :187.70   Max.   :330.1  \n NA's   :1      NA's   :1        NA's   :1        NA's   :1      \n   BGAPC_CML        BGAPC_rfu         ODO_sat         ODO_mgl     \n Min.   :   188   Min.   :  0.10   Min.   : 87.5   Min.   :10.34  \n 1st Qu.:   971   1st Qu.:  0.50   1st Qu.: 99.2   1st Qu.:12.34  \n Median :  1369   Median :  0.70   Median :101.8   Median :12.88  \n Mean   :153571   Mean   : 72.91   Mean   :102.0   Mean   :12.88  \n 3rd Qu.:345211   3rd Qu.:163.60   3rd Qu.:104.1   3rd Qu.:13.34  \n Max.   :345471   Max.   :163.70   Max.   :120.8   Max.   :14.99  \n NA's   :1        NA's   :1        NA's   :1       NA's   :1      \n    Depth_ft        Depth_m      SurfaceWaterElev_m_levelNad83m\n Min.   :12.15   Min.   :3.705   Min.   :-32.53                \n 1st Qu.:14.60   1st Qu.:4.451   1st Qu.:-31.78                \n Median :15.37   Median :4.684   Median :-31.55                \n Mean   :15.34   Mean   :4.677   Mean   :-31.55                \n 3rd Qu.:16.12   3rd Qu.:4.913   3rd Qu.:-31.32                \n Max.   :17.89   Max.   :5.454   Max.   :-30.78                \n                                                               \n\n\nThese data represent measurements taken every 15 minutes, 24 hours a day, 7 days a week, 365 days a year. For brevity, this file contains measurements starting at 1/1/2014 12:00:00 AM and ending at 3/27/2014 9:30:00 AM (only 8199 records here…).\nIf you look at the summary of the data above, you will see several things, including:\n\nDate and time objects are character\nSome measurements are in Standard and some in Imperial with units in the same file include both °F and °C, as well as measurements in meters, feet, and inches. In fact, there are duplication of data columns in different units (guess what kind of correlation they might have…)"
  },
  {
    "objectID": "tidyverse_narrative.html#verbs-of-analysis",
    "href": "tidyverse_narrative.html#verbs-of-analysis",
    "title": "Tidyverse",
    "section": "Verbs of Analysis",
    "text": "Verbs of Analysis\nWhen we perform any type of data manipulation, we use specific verbs. There is a limited lexicon for us to use, but the key here is how we perform these actions, and in which order they are deployed for a huge diversity in outcomes. For now, these basic verbs include:\n\nSelect: Used to grab or reorder columns of data.\nFilter: Used to grab subsets of records (rows) based upon some criteria.\nMutate: Create new columns of data based upon manipulations of existing columns.\nArrange: Order the records (rows) based upon some criteria.\nGroup: Gather records together to perform operations on chunks of them similar to by().\nSummarize: Extract summaries of data (or grouped data) based upon some defined criteria.\n\nIn the following examples, we’ll be using the rice data above. For each verb, I’m going to use the pipe operator (%&gt;%) to send the data into the example functions and then assign the result to a dummy data.frame named df. The arguments passed to each of the verbs are where the magic happens.\n\nThe Output\nThe key to these activities is that every one of these functions takes a data.frame as input, does its operations on it, then return a data.frame object as output. The data.frame is the core data container for all of these actions.\n\n\nSelect Operator\nThe select() function allows you to choose which columns of data to work with.\n\nrice %&gt;%\n  select( DateTime, AirTempF ) -&gt; df \nhead(df)\n\n# A tibble: 6 × 2\n  DateTime             AirTempF\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 1/1/2014 12:00:00 AM     31.0\n2 1/1/2014 12:15:00 AM     30.7\n3 1/1/2014 12:30:00 AM     31.2\n4 1/1/2014 12:45:00 AM     30.5\n5 1/1/2014 1:00:00 AM      30.9\n6 1/1/2014 1:15:00 AM      30.6\n\n\nSelect can also be used to reorder the columns in a data.frame object. Here are the names of the data columns as initially loaded.\n\nnames( rice )\n\n [1] \"DateTime\"                       \"RecordID\"                      \n [3] \"PAR\"                            \"WindSpeed_mph\"                 \n [5] \"WindDir\"                        \"AirTempF\"                      \n [7] \"RelHumidity\"                    \"BP_HG\"                         \n [9] \"Rain_in\"                        \"H2O_TempC\"                     \n[11] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[13] \"PH\"                             \"PH_mv\"                         \n[15] \"Turbidity_ntu\"                  \"Chla_ugl\"                      \n[17] \"BGAPC_CML\"                      \"BGAPC_rfu\"                     \n[19] \"ODO_sat\"                        \"ODO_mgl\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\nLet’s say that you wanted to reorder the columns as RecordID, ODO_mgl and PH as the first three columns and leave everything else as is. There is this cool function everthying() that helps out.\n\nrice %&gt;%\n  select( RecordID, ODO_mgl, PH, everything() ) -&gt; df\nnames( df )\n\n [1] \"RecordID\"                       \"ODO_mgl\"                       \n [3] \"PH\"                             \"DateTime\"                      \n [5] \"PAR\"                            \"WindSpeed_mph\"                 \n [7] \"WindDir\"                        \"AirTempF\"                      \n [9] \"RelHumidity\"                    \"BP_HG\"                         \n[11] \"Rain_in\"                        \"H2O_TempC\"                     \n[13] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[15] \"PH_mv\"                          \"Turbidity_ntu\"                 \n[17] \"Chla_ugl\"                       \"BGAPC_CML\"                     \n[19] \"BGAPC_rfu\"                      \"ODO_sat\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\n\n\nFilter\nThe function filter() works to select records (rows) based upon some criteria. So for example, if I am interested in just records when the airtemp was freezing (and the raw data are in °F). The range of values in the original data was:\n\nrange( rice$AirTempF )\n\n[1]  3.749 74.870\n\n\nbut after filtering using the name of the variable and a logical operator.\n\nrice %&gt;%\n  filter( AirTempF &lt; 32 ) -&gt; df\nrange( df$AirTempF )\n\n[1]  3.749 31.990\n\n\nJust like select(), it is possible to have several conditions, that are compounded (using a logical AND operator) by adding them to the filter() function. Here I also split the conditionals requiring the data to be above freezing air temperatures, not missing data from the PH meter, and water turbidity &lt; 15 ntu’s. I also put each of these onto their own lines and auto-indent does a great job of making it reasonably readable.\n\nrice %&gt;%\n  filter( AirTempF &gt; 32, \n          !is.na(PH), \n          Turbidity_ntu &lt; 15) -&gt; df\nnrow(df)\n\n[1] 1449\n\n\n\n\nMutate\nThe mutate() function changes values in the table and is quite versatile. Here I will jump back to our old friend mdy_hms() from lubridate and convert the DateTime column, which is\n\nclass( rice$DateTime )\n\n[1] \"character\"\n\n\nand convert it into a real date and time object\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\") ) -&gt; df\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\nsummary( df$Date )\n\n                 Min.               1st Qu.                Median \n\"2014-01-01 00:00:00\" \"2014-01-22 08:22:30\" \"2014-02-12 16:45:00\" \n                 Mean               3rd Qu.                  Max. \n\"2014-02-12 16:45:00\" \"2014-03-06 01:07:30\" \"2014-03-27 09:30:00\" \n\n\nYou can also create several mutations in one mutation step.\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\"), \n          Month = month(Date, label = TRUE) ) -&gt; df\nsummary( df$Month )\n\n Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec \n2976 2688 2535    0    0    0    0    0    0    0    0    0 \n\n\n\n\nArrange\nWe can sort entire data.frame objects based upon the values in one or more of the columns using the arrange() function.\n\nrice %&gt;%\n  arrange( WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 0\n\n\nBy default, it is in ascending order, to reverse it, use the negative operator on the column name object in the function.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 30.65\n\n\nAs above, it is possible to combine many columns of data as criteria for sorting by adding more arguments to the function call.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph, WindDir ) -&gt; df\n\n\n\nSummarise\nThis function is the first one that does not return some version of the original data that was passed to it. Rather, this performs operations on the data and makes a brand new data.frame object.\nEach argument you give to the function performs one or more operations on the data and returns a brand new data.frame object with only the the values specified.\nHere is an example where I am taking the mean air and water temperature (n.b., one is in °F and the other is in °C). Notice the result is a new data.frame object with one row and two new columns defined by how I asked for the summary in the first place. I used single tick notation so I can have a space in the column names.\n\nrice %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean(H2O_TempC, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  `Air Temp` `Water Temp`\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       38.8         5.53\n\n\n\n\nGroup & Summarize\nTo get more than one row in the resulting data.frame from summary(), we need to group the data in some way. The function group_by() does this and is used prior to summary(). Let’s take a look at how we can get the average air and water temp by month. To do this, I’m going to have to do several steps. I’m just going to chain them together using the %&gt;% operator.\n\nrice %&gt;%\n  mutate( Date = mdy_hms( DateTime, \n                          tz=\"EST\"),\n          Month = month( Date, \n                         abbr = FALSE, \n                         label=TRUE) ) %&gt;%\n  group_by( Month ) %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean( H2O_TempC, \n                                  na.rm=TRUE) )\n\n# A tibble: 3 × 3\n  Month    `Air Temp` `Water Temp`\n  &lt;ord&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 January        34.7         3.68\n2 February       39.7         5.29\n3 March          42.6         7.96\n\n\nAs you read the code, notice how easy it is to understand what is going on because of both the pipes and because of the way I am formatting the code itself."
  },
  {
    "objectID": "tidyverse_narrative.html#flows",
    "href": "tidyverse_narrative.html#flows",
    "title": "Tidyverse",
    "section": "Flows",
    "text": "Flows\nThis last part really showed off the process of multi-step data manipulations using the pipe operator and the several verbs we introduced. These are both efficient in terms of typing as well as efficient in the way of producing research that makes sense to look at.\nHere are some strategies that I use when building up these manipulation workflows.\n\nDo not think that you have to do the whole thing at once. I typically build up the workflow, one line at a time. Make sure the output from the previous line is what you think it should be then add the next one.\nKeep your code open and airy, it makes it easier to read and to catch any logical errors that may arrise.\nYou can pipe into a lot of different functions. In fact, any function that takes a data frame can be the recipient of a pipe. While developing a workflow, I will often pipe into things like head(), summary(), or View() to take a look at what is coming out of my workflow to make sure it resembles what I think it should look like."
  },
  {
    "objectID": "tidyverse_narrative.html#questions",
    "href": "tidyverse_narrative.html#questions",
    "title": "Tidyverse",
    "section": "Questions",
    "text": "Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "counting_stats_narrative.html",
    "href": "counting_stats_narrative.html",
    "title": "Non-Parametric & Counting Statistics",
    "section": "",
    "text": "library( tidyverse )\nlibrary( forcats )\ntheme_set( theme_minimal() )"
  },
  {
    "objectID": "counting_stats_narrative.html#counting-or-non-parametric-statistics",
    "href": "counting_stats_narrative.html#counting-or-non-parametric-statistics",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Counting, or ‘Non-Parametric’ Statistics",
    "text": "Counting, or ‘Non-Parametric’ Statistics\nTo date, we’ve been talking about the kinds of statistical approaches we use based upon the kind of data that we have for predictors and responses. Keeping it really simple, the graphic below attempts to capture this dynamic.\nIn this display, counting statistics are those that have factor data for both predictors and responses. However, the methods that we use in this quadrant, like \\(\\chi^2\\) tests, are a small fraction of available methods that we can classify as NonParametric statistics.\nNon-parametric counting statistics are essential tools in data analysis because they provide robust methods for understanding data without making strong assumptions about the underlying distribution. Many traditional statistical methods rely on parametric assumptions, such as normality, homoscedasticity, or linearity. However, real-world data often violate these assumptions due to skewness, outliers, or categorical variables. Non-parametric methods excel in these scenarios by leveraging the rank or order of data rather than its precise values, making them ideal for analyzing ordinal data, small sample sizes, or data sets with unknown distributions.\nCounting statistics, like chi-square tests, Fisher’s exact test, or Wilcoxon rank-sum tests, allow researchers to test hypotheses about relationships or differences without assuming normality. For example, in environmental studies or survey analysis, datasets may include counts of species occurrences, categorical survey responses, or presence/absence data. Non-parametric methods ensure reliable results in these cases, enabling data-driven decisions even with non-normal or heterogeneous data that extend beyond just the lower right quadrant of the image above.\nSo let’s start simple and I’ll walk through a set of these kinds of models and highlight how we can apply a non-parametric approach to data whose identity may suggest other parametric statistics would be appropriate but whose normality or properties suggest that we not use those more traditional methods."
  },
  {
    "objectID": "counting_stats_narrative.html#the-binomial-test",
    "href": "counting_stats_narrative.html#the-binomial-test",
    "title": "Non-Parametric & Counting Statistics",
    "section": "The Binomial Test",
    "text": "The Binomial Test\nWe’ve discussed the Binomial as part of the underlying generative functions for Bayesian Statistics already, but let’s revisit it for just a moment to get more specific on its general status. As a reminder, this is a test where the outcomes can be classified into either “Group A” or “Group B” (a 2-level factor in R).\n\nAssumptions\nThe binomial test assumes:\n\nThe \\(n\\) samples are mutually independent.\n\nEach sample has an underlying probability, \\(p\\), of being classified in “Group A” and a probability of \\(1-p\\) of being assigned to “Group B”.\n\nThe test statistic for this analysis is the number of values observed in “Group A” as a count. Because we are testing for the value of \\(p\\) (or our confidence in the range of values it can assume), the null hypothesis is:\n\\(H_O: p = \\hat{p}\\)\nIn R, we can test this using the binom.test() function, whose signature is:\n\nYou can either give the function a single values for x and n or you can have x be a 2-element vector and skip the n part.\nBorrowing from a previous example, let’s assume we are sampling fish in the James River. The hypothesis is that the frequency of catfish is\n\\[\np_{catfish} = \\frac{N_{catfish}}{N_{catfish} + N_{other}} = \\frac{37}{50} \\approx 0.74\n\\]\nSo, if we were to go out and do some more sampling, the null hypotehsis would be:\n\\(H_O: p_{catfish} = 0.74\\)\nLet’s assume you just got back from a great sampling day and found the following data.\n\ncatfish &lt;- 52\nnon_catfish &lt;- 47 \n\nTo test if this sampling exercise is consistent with the previous assumption of a 74% catfish presence, we do:\n\nsamples &lt;- c( catfish, non_catfish )\np_catfish &lt;- 0.74\n\n\ndf &lt;- data.frame( Counts = c( samples, 0.74 * sum(samples), ( (1-0.74) * sum(samples) )),\n                    Fish = c(\"Catfish\", \"Other\", \"Catfish\", \"Other\"),\n                    Group = c(\"Observed\",\"Observed\", \"Expected\", \"Expected\") )\n\nggplot( df, \n        aes(Fish,Counts, fill=Group) )+ \n  geom_bar( position=\"dodge\", stat=\"identity\") \n\n\n\n\n\n\n\n\n\nfit &lt;- binom.test(x=samples, p = p_catfish )\nfit \n\n\n    Exact binomial test\n\ndata:  samples\nnumber of successes = 52, number of trials = 99, p-value = 5.007e-06\nalternative hypothesis: true probability of success is not equal to 0.74\n95 percent confidence interval:\n 0.4223973 0.6265570\nsample estimates:\nprobability of success \n             0.5252525 \n\n\nWhose results suggest that \\(p_{catfish} \\ne 0.74\\) and we reject the null hypothesis. Our data subbest, from the samples we provided, that the observed \\(p_{catfish}\\) falls within the interval of 0.422 - 0.627.\nFor examples with more than two “Group”s, there is a multinomial test as well, but the function is not one built into R and you’ll want to check out the Multinomial package for that function."
  },
  {
    "objectID": "counting_stats_narrative.html#contingency-tables",
    "href": "counting_stats_narrative.html#contingency-tables",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Contingency Tables",
    "text": "Contingency Tables\nContingency tables are another counting approach, where your data can be classified into one or more categories. Often it is taken as a comparison between populations. So, in keeping with our fish theme, let’s assume we went to another river and did another sampling run on catfish. In general, we are not limited to “Group A” vs “Group B” but we will use it for continuity here.\nOur sampling can be categorized as follows for the two populations of samples we’ve collected.\n\nThe values above are Observerd counts of observations in each pair of categories. Every fish is talled into the grouping (catfish vs not) and the location it was sampled (population 1 vs population 2). The total number of sampels at each population is \\(n_1\\) (in the data it was $n_1 =$99) and \\(n_2\\) (to be shown below) and the total number of samples that are catfish is \\(C_1 = O_{11} + O_{21}\\) and not catfish as \\(C_2\\) which also happens to be \\(N - C_1\\) (for completeness).\nAgain, here we have 2 rows and 2 columns, but this can be extended to larger numbers of groupings and sampling population as necessary and the analysis is the same.\nIn this simple case, the null hypothesis is not about the exact value of \\(p_{catfish}\\), as in the binomial, but rather that the frequency of catfish in Population 1, let’s define this as \\(p_1\\) is equal to the probabilty of sampling a catfish at Population 2, which we define as \\(p_2\\).\n\\(H_O: p_1 = p_2\\)"
  },
  {
    "objectID": "counting_stats_narrative.html#ill-expand-this-to-cases-beyond-a-2x2-contingency-below.",
    "href": "counting_stats_narrative.html#ill-expand-this-to-cases-beyond-a-2x2-contingency-below.",
    "title": "Non-Parametric & Counting Statistics",
    "section": "I’ll expand this to cases beyond a 2x2 contingency below.",
    "text": "I’ll expand this to cases beyond a 2x2 contingency below.\n\nThe Assumptions\nThe assumptions for a contingency table analysis are:\n\nEach sampling is a random sample.\n\nAny pair of samples are mutually independent.\n\nEach sample is unambiguously categorized into one, and only one, Grouping and Replicate.\n\nTo run this test in R we use the chisq.test() function, whose signature is as follows:\n\nWhere for our example, we will use it by passing a 2x2 matrix of observations as x. Let’s say I went out to another site and did 50 more samples and found 32 catfish.\nHere is a matrix with these numbers. I’m going to decorate it with row and column labels for clarity.\n\nObserved &lt;- matrix( c(52, 47, 32, 18), nrow=2, byrow = TRUE)\nrownames( Observed ) &lt;- c(\"Population 1\", \"Population 2\")\ncolnames( Observed ) &lt;- c(\"Catfish\", \"Other\")\nObserved\n\n             Catfish Other\nPopulation 1      52    47\nPopulation 2      32    18\n\n\nOur expectations are based on the \\(p_{catfish} = 0.74\\) and can be used to create an Expected value for each of the cells in the matrix above. For example, the expected value for the number of catfish from Population 1 would be \\(n_1 * 0.74\\) and for non-catfish would be \\((1-0.74) * n_1\\). We can do the same for the second site.\n\nn1 &lt;- 52 + 47\nn2 &lt;- 32 + 18 \n\nExpected &lt;- matrix( c( 0.74 * n1, \n                       (1-0.74) * n1, \n                       0.74 * n2,\n                       (1-0.74) * n2), nrow=2, byrow = TRUE)\nrownames( Expected ) &lt;- c(\"Population 1\", \"Population 2\")\ncolnames( Expected ) &lt;- c(\"Catfish\", \"Other\")\nExpected\n\n             Catfish Other\nPopulation 1   73.26 25.74\nPopulation 2   37.00 13.00\n\n\nSo, in some sense, we set the underlying probability of catfish in either locale to \\(0.74\\). If the observed values are close to the expected, then the data would be consistent with what is expected from the underlying hypothesis. If they are not close, then the data data are deviating. The amount of deviation is what we will be using as a test statistic here and it is a measure of:\n\nHow far apart \\(O_{xy}\\) is from \\(E_{xy}\\),\nStandardized by the magnitude of the \\(E_{xy}\\).\n\nGiving us the test statistic:\n\\[\nT = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nThe subscripts account for the number of rows (\\(r\\)) and columns (\\(c\\)) for any arbitrary sized table beyond the 2x2 example we have here. We can estimate the test statistic in R direclty, as:\n\nT &lt;- sum(  (Observed - Expected)^2 / Expected )\nT\n\n[1] 26.32813\n\n\nThis test statistic is expected to be equivalent to a \\(\\Chi^2\\) statistic with \\((r-1)(c-1)\\) degrees of freedom. We haven’t really estimated this directly yet, so here is how you’d figure out the probability of this test statistic.\n\nalpha &lt;- 0.05\np &lt;- 1.0 - alpha \ncritical.value &lt;- qchisq( p ,1)\ncritical.value\n\n[1] 3.841459\n\n\nYou’ve probably seen this value before; it is a common one that we use in teaching basic Biology courses when we do Mendel’s pea phenotypes. So, the test statistic is $T =$26.3281253 and the critical value is 3.8414588, so we would Reject H_O that the fraction of catfish/not-catfish from Population 1 is not the same as observed in Population 2.\nBut, we are still stuck with the \\(\\alpha = 0.05\\) issue and we’d probably rather have the actual p-value for the observed Test statistic, \\(T\\).\n\np.value &lt;- 1.0 - pchisq( T, df=1 )\np.value\n\n[1] 2.88063e-07\n\n\nNotice I’ve used the qschisq function to get hte critcal value for some probability and the pchisq to the probability associated with an observed value. Every distribution has p* and q* functions associated with them and can be used to find critical values and probabilities if you need to do it manually.\nIn the real world, you’ll probably be taking the data from a larger data.frame object where you collected additional data. Here is a sample, where the species and location are factors.\n\ndf &lt;- data.frame( Sample = 1:149,\n                  Species = factor( c( rep(\"Catfish\", 52), rep(\"Other\", 47), \n                                       rep(\"Catfish\", 32), rep(\"Other\",18) ) ),\n                  Site = factor( c( rep(\"Population 1\", 99), rep(\"Population 2\", 50))),\n                  Measurement = rnorm(149,mean=12, sd=3))\nsummary( df )\n\n     Sample       Species             Site     Measurement    \n Min.   :  1   Catfish:84   Population 1:99   Min.   : 3.834  \n 1st Qu.: 38   Other  :65   Population 2:50   1st Qu.: 9.761  \n Median : 75                                  Median :11.592  \n Mean   : 75                                  Mean   :11.553  \n 3rd Qu.:112                                  3rd Qu.:13.575  \n Max.   :149                                  Max.   :18.923  \n\n\n\nhead(df)\n\n  Sample Species         Site Measurement\n1      1 Catfish Population 1   11.841474\n2      2 Catfish Population 1   10.533547\n3      3 Catfish Population 1    8.853103\n4      4 Catfish Population 1   17.280004\n5      5 Catfish Population 1   11.536413\n6      6 Catfish Population 1   18.892669\n\n\nIt is easy to grab the tabular data, and configure it as we did above using the table() functions.\n\ntable( df$Site, df$Species )\n\n              \n               Catfish Other\n  Population 1      52    47\n  Population 2      32    18\n\n\nThere are a few caveats that we need to make here. First, this appraoch requires that we have a sufficient number of observations in each of the cells. The longest running recommendation comes form Cochran1, who suggested that any time you have an \\(E_{ij} \\le 1\\) or if more than 20% of the \\(E_{ij} \\le 5\\), there may be some issues with statistical power and the approximation for the \\(\\Chi^2\\) statistic may be poor. In those cases, perhaps a permutation approach would be more appropriate."
  },
  {
    "objectID": "counting_stats_narrative.html#nonparametric-eqivallent-approaches",
    "href": "counting_stats_narrative.html#nonparametric-eqivallent-approaches",
    "title": "Non-Parametric & Counting Statistics",
    "section": "NonParametric Eqivallent Approaches",
    "text": "NonParametric Eqivallent Approaches\nI’m going to use a couple of examples here to show how we can use counting statistics and ranks for methods where we intended to use parametric approaches like a t-test, correlation, regression, and analysis of variance but the data were not suitable for those methods.\n\nSpearman Test for Ranked Correlation\nWhen we discussed correlation data (e.g., continuous predictors and response variables), I did point out that there was another kind of correlation statistic that uses ranks (the mode=\"spearman\") instead of the raw data (the Pearson-Product Moment approach which is the default). As we saw before, there is an option in the normal cor.test function that allows you to use a rank-based analysis instead of the normal Pearson Product-Moment approach.\n\nThis approach, basically, orders the magnitude of both predictor and response and assigns a rank (e.g., \\(1^{st}, 2^{nd}, \\ldots N^{th}\\)) for both variables and then estimates the relationship by correlating the ranked values instead of the raw values.\nFor brevity, I’ll not give an example and have you look back at the correlation lecture for more information."
  },
  {
    "objectID": "counting_stats_narrative.html#non-parametric-regression",
    "href": "counting_stats_narrative.html#non-parametric-regression",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Non-Parametric Regression",
    "text": "Non-Parametric Regression\nThere are a few different methods for estimating linear regression models via non-parametric approaches. We’ve seen one common approach whenever we used the stat_smooth() function in ggplot(). By default, it uses the loess() local smoother approach.\n\niris |&gt; \n  ggplot( aes(Sepal.Length, Sepal.Width) ) + \n  geom_point() + \n  stat_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn a lm(), the goal is to use least cost approaches to minimize the error variance across all the data simultaneously.\n\n\nLOcal regrESSion\nThe loess() approach looks at a local window for estimating the formula of the line along with a moderatly high-dimensional polynomial.\n\\[\n\\hat{y} = \\hat{\\beta}_O^{x} + \\hat{\\beta}_1^{x}x_i\n\\]\nThe function has a signature similar to that we saw in lm() for normal regression.\n\nNotice the optional value for span, which defines the size of the local window.\n\n\n\n\n\n\n\n\n\n\n\nFitting A loess Model\n\nfit &lt;- loess( Sepal.Width ~ Sepal.Length, data=iris)\nfit\n\nCall:\nloess(formula = Sepal.Width ~ Sepal.Length, data = iris)\n\nNumber of Observations: 150 \nEquivalent Number of Parameters: 4.74 \nResidual Standard Error: 0.4172 \n\n\nLet’s look at the consequence of the span parameter.\n\nloess.spans &lt;- function(x, y, s){\n  nobs &lt;- length(y)\n  xs &lt;- sort(x, index.return = TRUE)\n  x &lt;- xs$x\n  y &lt;- y[xs$ix]\n  lo &lt;- loess(y ~ x, span = s)\n  data.frame(Sepal.Length = x, Sepal.Width = lo$fitted, span = s)\n}\n\n\nrbind( loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.2 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.3 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.4 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.5 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.6 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.7 ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.8 ) )  |&gt; \n  mutate( span = factor( span, ordered=TRUE )) |&gt;\n  ggplot( aes(Sepal.Length, Sepal.Width) ) + \n  geom_point( data = iris, ) + \n  geom_line( aes(color = span ), lwd=1)  \n\n\n\n\n\n\n\n\nOne method suggested to figure out what an optimal value of span would be is based upon a generalized cross validation (GCV) approach.\n\\[\ns = \\frac{ \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_{i,s})^2 }{ (1 - \\frac{v_s}{n} )^2 }\n\\]\nwhere \\(v_s = \\sum_{i=1}^N h_{ii,s}\\) is the trace of the idempotent \\(\\mathbf{H}\\) Hat Matrix.\nHere is a function that does this for the general loess model.\n\nloess.gcv &lt;- function(x, y){\n  nobs &lt;- length(y)\n  xs &lt;- sort(x, index.return = TRUE)\n  x &lt;- xs$x\n  y &lt;- y[xs$ix]\n  tune.loess &lt;- function(s){\n    lo &lt;- loess(y ~ x, span = s)\n    mean((lo$fitted - y)^2) / (1 - lo$trace.hat/nobs)^2\n  }\n  os &lt;- optimize(tune.loess, interval = c(.01, 99))$minimum\n  lo &lt;- loess(y ~ x, span = os)\n  data.frame(Sepal.Length = x, Sepal.Width = lo$fitted, span = os)\n}\n\nrbind( loess.gcv( iris$Sepal.Length, iris$Sepal.Width ),\n       loess.spans( iris$Sepal.Length, iris$Sepal.Width, s = 0.75 ) ) |&gt; \n  mutate( span = factor(round(span,digits = 2))) |&gt;\n  ggplot( aes(Sepal.Length, Sepal.Width, color=span) ) + \n  geom_line()"
  },
  {
    "objectID": "counting_stats_narrative.html#other-regression-approaches",
    "href": "counting_stats_narrative.html#other-regression-approaches",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Other Regression Approaches",
    "text": "Other Regression Approaches\nThere are several additional approaches including Kernel Regression (e.g., Nadaraya & Watson’s Kernel Smoother) and Local Averaging (e.g., Friedman’s Super Smoother). If you need to perform a regression approach, please explore the benefits and challenges with these alternative methods before deciding on which one to use.\n\nMann-Whitney Test For the Equivallence of Two Means\nSo, this is an extension of the t-test for the equivalence of the mean values of two data sets (e.g., \\(H_O: \\bar{x} = \\bar{y}\\)). This assume that two sets of data are independently sampled and tests for the equality of the samples based upon ranks of the data in each set.\n\nAssumptions\nThis approach has the following assumptions:\n\nBoth sets of data are random samples from their respective population.\n\nEach of the populations are similarly independent.\nThe measurement scale for each variable is at least ordinal (e.g., you can put them in order).\n\nFor this test, I’m just going to use the Motor Trends car data we’ve seen before and look at the mileage of cars that have a manual vs an automatic transmission. Here is the distribution of the data in a density plot.\n\nmtcars |&gt;\n  select( Transmission = am, MPG = mpg ) |&gt;\n  mutate( Transmission = as.factor( Transmission ) ) |&gt;\n  mutate( Transmission = fct_recode( Transmission, \"Automatic\"=\"0\", \"Manual\"=\"1\")) -&gt; data\n\ndata |&gt;\n  ggplot( aes(Transmission, MPG) )  + \n  geom_boxplot( notch=TRUE )\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nTo test this, we use the Wilcoxon Rank Sum test. The signature of this test is:\n\nThis will take the full set of data, assign them ranked values, and then determine if the ranked values are randomly distributed between the two groups.\n\nfit &lt;- wilcox.test( MPG ~ Transmission, data=data)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\nfit \n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  MPG by Transmission\nW = 42, p-value = 0.001871\nalternative hypothesis: true location shift is not equal to 0\n\n\nAs for other analyses, the object returned has a number of elements that may be helpful in writing about the analysis.\n\nnames(fit)\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"null.value\"  \"alternative\"\n[6] \"method\"      \"data.name\"  \n\n\n\n\n\nKruskal-Wallis Test for Equivallence of Several Means\nThe Kruskal-Wallis test is an extension on the Wilcoxon test in the same way that the aov() is an extension of the t-test(). The approach is again based upon ranks. Here is an example data set we’ve used once before based on air quality measured across five different months.\n\nairquality |&gt;\n  select( Month, Ozone ) |&gt; \n  mutate( Month = factor( Month ) ) |&gt; \n  mutate( Month = fct_recode(Month, \n                             May=\"5\",\n                             Jun=\"6\",\n                             Jul=\"7\",\n                             Aug=\"8\",\n                             Sep=\"9\" )) |&gt; \n  filter( !is.na( Ozone )) -&gt; data \n  \nggplot( data, aes(Month, Ozone) ) + \n  geom_boxplot(notch=TRUE)\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nTo test for the equality of ozone across months, we use kruskal.test()\n\nfit &lt;- kruskal.test( Ozone ~ Month, data=data )\nfit \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06\n\n\nAnd again, the object returned has the relevant parameters for inclusion in the text of your manuscript.\n\nnames(fit)\n\n[1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\""
  },
  {
    "objectID": "counting_stats_narrative.html#summary",
    "href": "counting_stats_narrative.html#summary",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Summary",
    "text": "Summary\nThese methods should get you going for pure counting statistics (\\(\\Chi^2\\)) or for replacements to parametric approaches when you find that your data do not conform to their expectations. There is a whole universe of non-parametric approaches and extensions to the methods I highlight here and I encourage you to explore these methods in the future if you have an odd set of data that needs to yield proper insights."
  },
  {
    "objectID": "counting_stats_narrative.html#footnotes",
    "href": "counting_stats_narrative.html#footnotes",
    "title": "Non-Parametric & Counting Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCochran, W.G. (1952) The \\(\\Chi^2\\) test of goodness of fit. Annals of Mathematical Statistics, 23, 315-345.↩︎"
  },
  {
    "objectID": "points_narrative.html",
    "href": "points_narrative.html",
    "title": "Spatial Point Data",
    "section": "",
    "text": "Photo from Unsplash\nLet’s start by loading in some of the libraries we’ll be using for this exercise.\nlibrary( sf )\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary( maps )\nlibrary( mapproj )\nlibrary( ggplot2 )\nlibrary( tidyverse )"
  },
  {
    "objectID": "points_narrative.html#learning-objectives",
    "href": "points_narrative.html#learning-objectives",
    "title": "Spatial Point Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThis topics is the first\n\nDescribe the importance of Ellipsoids & Datum in spatial data.\nUse both sf & ggplot in visualizing point data.\nBe able to transform point data from one projection to another."
  },
  {
    "objectID": "points_narrative.html#ellipsoids",
    "href": "points_narrative.html#ellipsoids",
    "title": "Spatial Point Data",
    "section": "Ellipsoids",
    "text": "Ellipsoids\nUnless you are in PHYS 101, the earth is not a perfect sphere (😉). It is an irregularly shaped object that we need to be able to characterize if we are going to develop a system of placing points onto it and doing things such as measuring distance, finding watersheds, or defining boundaries.\nThere has been a long history of ellipsoid research, all of which has been sought to increase our ability to map and move across the earth. The following table gives some historical and contemporary ellipsoids.\n\n\n\n\n\n\n\n\n\nEllipsoid\nEquatorial Radius (m)\nPolar Radius (m)\nUsed\n\n\n\n\nMaupertuis (1738)\n6,397,300\n6,363,806.283\nFrance\n\n\nPlessis (1817)\n6,376,523.0\n6,355,862.9333\nFrance\n\n\nEverest (1830)\n6,377,299.365\n6,356,098.359\nIndia\n\n\nEverest 1830 Modified (1967)\n6,377,304.063\n6,356,103.0390\nWest Malaysia & Singapore\n\n\nEverest 1830 (1967 Definition)\n6,377,298.556\n6,356,097.550\nBrunei & East Malaysia\n\n\nAiry (1830)\n6,377,563.396\n6,356,256.909\nBritain\n\n\nBessel (1841)\n6,377,397.155\n6,356,078.963\nEurope, Japan\n\n\nClarke (1866)\n6,378,206.4\n6,356,583.8\nNorth America\n\n\nClarke (1878)\n6,378,190\n6,356,456\nNorth America\n\n\nClarke (1880)\n6,378,249.145\n6,356,514.870\nFrance, Africa\n\n\nHelmert (1906)\n6,378,200\n6,356,818.17\nEgypt\n\n\nHayford (1910)\n6,378,388\n6,356,911.946\nUSA\n\n\nInternational (1924)\n6,378,388\n6,356,911.946\nEurope\n\n\nKrassovsky (1940)\n6,378,245\n6,356,863.019\nUSSR, Russia, Romania\n\n\nWGS66 (1966)\n6,378,145\n6,356,759.769\nUSA/DoD\n\n\nAustralian National (1966)\n6,378,160\n6,356,774.719\nAustralia\n\n\nNew International (1967)\n6,378,157.5\n6,356,772.2\n\n\n\nGRS-67 (1967)\n6,378,160\n6,356,774.516\n\n\n\nSouth American (1969)\n6,378,160\n6,356,774.719\nSouth America\n\n\nWGS-72 (1972)\n6,378,135\n6,356,750.52\nUSA/DoD\n\n\nGRS-80 (1979)\n6,378,137\n6,356,752.3141\nGlobal ITRS\n\n\nWGS-84 (1984)\n6,378,137\n6,356,752.3142\nGlobal GPS\n\n\nIERS (1989)\n6,378,136\n6,356,751.302\n\n\n\nIERS (2003)\n6,378,136.6\n6,356,751.9\n\n\n\n\nThe most common ones you will probably run across include GRS80/NAD83 (derived from satellite measurements of the distance of the surface to the core of the planet ) and WGS-84 (an ellipsoid based upon GPS).\n\nExample Data\nTo examine the differences between ellipsoids, let’s load in some data first. Here are some point data that can be interpreted as polygons and represent the lower 48 states of the US.\n\nstates &lt;- map_data(\"state\")\nhead( states )\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\nEach row is a point that is associated with a group (in this case the state) and is plot in a specific order (to make the outline of the state). There are 15,537 points required to make the plot, with the following 49 regions.\n\nunique( states$region )\n\n [1] \"alabama\"              \"arizona\"              \"arkansas\"            \n [4] \"california\"           \"colorado\"             \"connecticut\"         \n [7] \"delaware\"             \"district of columbia\" \"florida\"             \n[10] \"georgia\"              \"idaho\"                \"illinois\"            \n[13] \"indiana\"              \"iowa\"                 \"kansas\"              \n[16] \"kentucky\"             \"louisiana\"            \"maine\"               \n[19] \"maryland\"             \"massachusetts\"        \"michigan\"            \n[22] \"minnesota\"            \"mississippi\"          \"missouri\"            \n[25] \"montana\"              \"nebraska\"             \"nevada\"              \n[28] \"new hampshire\"        \"new jersey\"           \"new mexico\"          \n[31] \"new york\"             \"north carolina\"       \"north dakota\"        \n[34] \"ohio\"                 \"oklahoma\"             \"oregon\"              \n[37] \"pennsylvania\"         \"rhode island\"         \"south carolina\"      \n[40] \"south dakota\"         \"tennessee\"            \"texas\"               \n[43] \"utah\"                 \"vermont\"              \"virginia\"            \n[46] \"washington\"           \"west virginia\"        \"wisconsin\"           \n[49] \"wyoming\"             \n\n\nFortunately for us, our old friend ggplot has a bit of magic that can do this kind of plotting for us.\n\nlibrary( ggplot2 )\nggplot( states, aes( x = long, \n                     y = lat,\n                     group = group ) ) + \n  geom_polygon( fill = \"lightgray\", \n                color = \"black\", \n                lwd = 0.25) + \n  theme_void() -&gt; p\n\n\n\nAzimuth Projections\nAn Azimuth Projection is one that is formed by a 2-dimensional plane that is tangential to the surface of the earth at example one point. This point may be polar (north or south pole) or oblique (e.g., over Richmond, Virginia).\n\n\n\n\nAzequidistant\n\n\n\nWe can apply different ellipsoids to the map when we plot it by adjusting the coordinate space it is plot within using the coord_map() modification. For a whole list of available projections, see ?mapproject.\n\np + coord_map( \"azequalarea\")\n\n\n\n\n\n\n\n\n\n\nCylindrical Projection\nA cylindrical projection is one where a cylinder is wrapped around the earth creating straight lines for all parallel away from the equator.\n\n\n\n\nCylindrical Projection\n\n\n\n\np + coord_map(\"cylindrical\")\n\n\n\n\n\n\n\n\n\n\nConic Projections\nConic projections are symmetric around the prime meridian and all parallels are segments of conecntric circles.\n\n\n\n\nConic Projection\n\n\n\n\np + coord_map( \"conic\", lat0 = 30)"
  },
  {
    "objectID": "points_narrative.html#datum",
    "href": "points_narrative.html#datum",
    "title": "Spatial Point Data",
    "section": "Datum",
    "text": "Datum\nOnce we have an ellipsoid model to work with we must define a DATUM type that will represent the coordiante system used. Two common DATUM types include:\n\nLongitude & Latitude - The East/West & North/South position on the surface of the earth.\n\nPrime Meridian (0° Longitude) passes thorugh the Royal Observatory in Greenwich England, with positive values of longitude to the east and negative to the west.\nEquator (0° Latitude) and is defined as the point on the planet where both northern and southern hemisphers have equal amounts of day and night at the equinox (Sept. 21 & March 21).\nRichmond, Virginia: 37.533333 Latitude, -77.466667 Longitude\n\nUniversal Trans Mercator - A division of the earth into 60 zones (~6°longitude each, labeled 01 - 60) and 20 bands each of which is ~8° latitude (labeled C-X excluding I & O with A & B dividing up Antartica). See image here.\n\nCoordinates include Zone & band designation as well as coordinates in Easting and Northing (planar coordinates within the zone) measured in meters.\nRichmond, Virginia: 18S 282051 4156899\n\n\n\n\n\n\n⚠️\n\n\n \n\n\nYou must set both the ellipsoid and datum to be EXACTLY THE SAME for all of your data before you can do any work with it. If they are not on the same lumpy bumpy planet or in the same coordinate system, you will be screwed (that is a technical term)."
  },
  {
    "objectID": "points_narrative.html#sf-objects",
    "href": "points_narrative.html#sf-objects",
    "title": "Spatial Point Data",
    "section": "sf Objects",
    "text": "sf Objects\nSimple Features (hereafter abbreviated as sf) are an open standard developed by the Open Geospatial Consortium (OGC). They define the following basic types:\n\nPOINT\n\nLINESTRING\nPOLYGON\n\nMULTIPOINT\nMULTILINESTRING\nMULTIPOLYGON\nGEOMETRYCOLLECTION\n\nEach of these basic types can be represented within a single column of a data.frame. To do this, we need to tell the conversion function st_as_sf() which columns to consider as the datum and which ellipsoid to use.\n\nlibrary( sf )\ndata %&gt;%\n  st_as_sf( coords=c(\"Longitude\",\"Latitude\"),\n            crs = 4326 ) -&gt; data\nhead( data )\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -110.951 ymin: 23.2855 xmax: -109.8507 ymax: 24.21441\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 8\n  Site  Males Females Suitability MFRatio GenVarArapat GenVarEuphli\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Aqu      12       9       0.722   1.33         0.120       0.0968\n2 73       11       5       0.146   2.2          0.137       0.253 \n3 157      26      30       0.881   0.867        0.150       0.191 \n4 153      35      41       0.732   0.854        0.333       0.276 \n5 163      21      21       0.433   1            0.298       0.338 \n6 48       18      27       0.620   0.667        0.115       0.213 \n# ℹ 1 more variable: geometry &lt;POINT [°]&gt;\n\n\nThis conversion to an sf object adds attributes to the data.frame and tibble object.\n\nclass( data )\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThis additional sf attributes gives it more qualities such as a bounding box (e.g., the area within which all the poitns exist)\n\nst_bbox( data )\n\n      xmin       ymin       xmax       ymax \n-114.29353   23.28550 -109.32700   29.32541 \n\n\nDistances between objects.\n\nst_distance( data[1,], data[2,])\n\nUnits: [m]\n        [,1]\n[1,] 84376.8\n\n\nAs well as complex geospatial operations such as finding the convex hull (the minimal area containing all poitns).\n\ndata %&gt;%\n  st_union() %&gt;%\n  st_convex_hull() -&gt; hull\nhull\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -114.2935 ymin: 23.2855 xmax: -109.327 ymax: 29.32541\nGeodetic CRS:  WGS 84\n\n\nPOLYGON ((-114.2935 29.32541, -113.9914 28.6605...\n\n\nthe center of the all the points.\n\nhull %&gt;%\n  st_centroid()\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -111.3417 ymin: 26.37741 xmax: -111.3417 ymax: 26.37741\nGeodetic CRS:  WGS 84\n\n\nPOINT (-111.3417 26.37741)\n\n\nand the area enclosed by all the points (for various units).\n\nlibrary( units )\n\nudunits database from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/units/share/udunits/udunits2.xml\n\nhull %&gt;%\n  st_area() %&gt;%\n  set_units( km^2 )\n\n122130.5 [km^2]\n\n\n\nReprojecting\nIn addition to the operations above, properly created sf objects can easily be projected from one CRS into another (epsg 6372 is a common projection covering Mexico based upon the GRS80 elipsoid and the latest ITRF2008 datum standard based on the meter)4.\n\ndata %&gt;%\n  st_transform( 6372 ) %&gt;%\n  st_bbox()\n\n   xmin    ymin    xmax    ymax \n1307745 1274010 1773676 1968473 \n\n\nAgain, do this first to all your data to make sure it is put into a proper projection (and most of your headaches will disappear).\n\n\nPlotting sf Objects\nAnalogous to the duality between built-in R plotting and ggplot approaches, we can use either of these frameworks to plot sf objects.\nAs built-in objects, a sf data set that has a geometry coordinate is intrinsically linked to all the other data columns. If we plot the entire data frame, we see that for each non-geometry data column, we create an individual plot.\n\nplot( data )\n\n\n\n\n\n\n\n\nThe data with the data.frame can be accessed as normal.\n\nplot( data$Suitability )\n\n\n\n\n\n\n\n\nBut if we plot it using the square brackets and names of dat columns, we can link the geometry column to it and plot it as a spatial representation of those data (and adorn it with the normal plot() upgrades accordingly).\n\nplot( data[\"Suitability\"], pch=16, cex=2)\n\n\n\n\n\n\n\n\nPerhaps not surprisingly, ggplot() also works the same way, however, the geospatial coordiantes for the plot aare taken care of using geom_sf() and you are left with definining which of the data columns you want to put into the plot as a component of the aes() definition.\n\nggplot( data, aes(color=Suitability) ) + \n  geom_sf( )\n\n\n\n\n\n\n\n\nIt works the same ways for lables.\n\nggplot( data ) + \n  geom_sf_text( aes(label=Site) ) + \n  theme_void() + \n  coord_map()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "points_narrative.html#footnotes",
    "href": "points_narrative.html#footnotes",
    "title": "Spatial Point Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTobler, W. R. 1970. Economic Geography, 46, 234–240.↩︎\nTranslation from one CRS to another in all GIS software is handled by the open source proj.org library.↩︎\nThe EPSG standard was originally created in 1985 by the https://en.wikipedia.org/wiki/European_Petroleum_Survey_Group and made public in 1993.↩︎\nThis standard is defined by Sistema Nacional de Información Estadística y Geográfica.↩︎\nThis is because if we use the normal procedures, we mess up the order in which everything is plot in geom_polygon(), try it and see.↩︎"
  },
  {
    "objectID": "text_narrative.html#topic",
    "href": "text_narrative.html#topic",
    "title": "Text Based Data",
    "section": "Topic",
    "text": "Topic\nThis quick section will focus on the text—or, more aptly, character—data type. We commonly run across this as narrative or designations such as site names, locations, other bits of information that we need to"
  },
  {
    "objectID": "text_narrative.html#data-types",
    "href": "text_narrative.html#data-types",
    "title": "Text Based Data",
    "section": "Data Types",
    "text": "Data Types\n\nx &lt;- \"Rodney\"\ny &lt;- 'Dyer'\n\nYou could use either single or double quotes to define a character type de novo—both work just fine. Having two of them is really helpful when you want to use one of the quote symbols inside the data.\n\nz &lt;- 'Bob Marley once said, \"It is a foolish dog that barks at a passing bird\"'\nz\n\n[1] \"Bob Marley once said, \\\"It is a foolish dog that barks at a passing bird\\\"\"\n\n\nBut notice that when you print it out to the terminal (or in the output to your Quarto chunck), it uses the backslash-double quote format. It also show up if you use print\n\nprint(z)\n\n[1] \"Bob Marley once said, \\\"It is a foolish dog that barks at a passing bird\\\"\"\n\n\nBut not when you cat it:\n\ncat(z)\n\nBob Marley once said, \"It is a foolish dog that barks at a passing bird\"\n\n\nThis is called escaping a special character. And it is a valid way to embed a quoting character into a sequence.\n\nw &lt;- \"\\\"Learning R is Fun,\\\" said Rodney.\"\ncat(w)\n\n\"Learning R is Fun,\" said Rodney.\n\n\nThere are other special characters that you will run across such as:\n\nThe Tab character, \\t\nThe New Line character \\n\nThe Return character \\r. This is becoming obsolete, in the “olden days” it was used with new line when we were making a transition from the manual typewriter where if you think about the action of using a typewriter, you need to advance a line AND return the carrage—the part that makes the letters on the paper—to the beginning of the line. That is why you sometimes see CR for on the return key. Windows used this convention and you may run across it still form peole who use that platform as \\r\\n. Mostly it is just yet another annoyance from some Windows software.\n\nTo see more, visit ?\"'\" in R."
  },
  {
    "objectID": "text_narrative.html#the-stringr-library",
    "href": "text_narrative.html#the-stringr-library",
    "title": "Text Based Data",
    "section": "The stringr Library",
    "text": "The stringr Library\nAnother joy from the tidyverse folks is the stringr library that has made things a bit easier in handling string data. As usual, there is a cheatsheet linked in the assets on this topic.\n\nlibrary( tidyverse )"
  },
  {
    "objectID": "text_narrative.html#the-verbs",
    "href": "text_narrative.html#the-verbs",
    "title": "Text Based Data",
    "section": "The ’Verbs”",
    "text": "The ’Verbs”\nWhen dealing with text, there are some basic verbs that we should recognize as fundamental actions that you’ll apply across a wide variety of situations. For text data, these include:\n\nCreating new/composite text.\nFinding content inside a string.\nDeleting content within a string\nReplacing content in a string with some new character value.\nManipulating content in a string.\n\n\nCreating\nWe’ve already seen how to create a single string, here is how we can smush (yes that is a technical term) together several kinds of data.\n\npaste( \"This\",\"is\",\"fun\")\n\n[1] \"This is fun\"\n\n\nYou can also mix-and-match different data types, as long as they can be coerced into a string type (which all data types can).\n\nnum &lt;- 42\npaste(\"It is\", TRUE, \"that my favorite number is\", num, \".\")\n\n[1] \"It is TRUE that my favorite number is 42 .\"\n\n\nWhen we work with character data, we need to realize that from the context of indexing, such as when we use a vector or data.frame, the sequence of characters is all one object.\n\nlength(z)\n\n[1] 1\n\n\nEven if it is made up of several characters. If we are interested subsequences within the string, we need to ask more specifically about the string length, not the variable length.\n\nstr_length( z )\n\n[1] 72\n\n\nThere are times when we need to paste more than a couple of individual items together.\n\na &lt;- 1:10\npaste( a )\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\npaste( a, sep=\", \")\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\npaste( a, collapse=\", \")\n\n[1] \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"\n\n\nas separate columns\n\nb &lt;- LETTERS[1:10]\nc &lt;- rnorm(10,12, 1)\npaste( a,b,c, sep = \"-\" )\n\n [1] \"1-A-12.7912652771503\"  \"2-B-11.8937010477833\"  \"3-C-11.3898231358371\" \n [4] \"4-D-11.4967532001624\"  \"5-E-13.2612608777438\"  \"6-F-10.3265825835173\" \n [7] \"7-G-11.1325734875453\"  \"8-H-11.8791143979628\"  \"9-I-11.5220014537577\" \n[10] \"10-J-12.3076359305748\"\n\n\nA stringr version is also available—with fewer keystrokes!\n\nstr_c( a, collapse=\", \" )\n\n[1] \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"\n\n\nand\n\nstr_c( a,b,c)\n\n [1] \"1A12.7912652771503\"  \"2B11.8937010477833\"  \"3C11.3898231358371\" \n [4] \"4D11.4967532001624\"  \"5E13.2612608777438\"  \"6F10.3265825835173\" \n [7] \"7G11.1325734875453\"  \"8H11.8791143979628\"  \"9I11.5220014537577\" \n[10] \"10J12.3076359305748\"\n\nstr_c( a,b,c, sep=\"-\")\n\n [1] \"1-A-12.7912652771503\"  \"2-B-11.8937010477833\"  \"3-C-11.3898231358371\" \n [4] \"4-D-11.4967532001624\"  \"5-E-13.2612608777438\"  \"6-F-10.3265825835173\" \n [7] \"7-G-11.1325734875453\"  \"8-H-11.8791143979628\"  \"9-I-11.5220014537577\" \n[10] \"10-J-12.3076359305748\"\n\n\n\n\nFinding\nFinding text may be done in a few ways.\n\ncat(z)\n\nBob Marley once said, \"It is a foolish dog that barks at a passing bird\"\n\n\nWe can ask if:\n\nA particular sequence of characters exist in the string (TRUE/FALSE).\n\n\nstr_detect(z, \"Marley\")\n\n[1] TRUE\n\nstr_detect(z, \"marley\") # case sensitive\n\n[1] FALSE\n\n\n\nWe can ask for the number of times a sequences shows up in a string.\n\n\nstr_count(z, \"a\")\n\n[1] 8\n\n\n\nWe can ask where the first occurance of a subsequence of characters starts at:\n\n\nstr_locate( z, \"dog\")\n\n     start end\n[1,]    40  42\n\n\n\nWe can find all occurences of a substring.\n\n\nstr_locate_all( z, \"a\")\n\n[[1]]\n     start end\n[1,]     6   6\n[2,]    18  18\n[3,]    30  30\n[4,]    46  46\n[5,]    50  50\n[6,]    55  55\n[7,]    58  58\n[8,]    61  61\n\n\n\nIf we know the location of a substring, you can extract it. Here I use the negative for the second index, which is treated as “second from the end” of the string.\n\n\nstr_sub(z, 24, -2)\n\n[1] \"It is a foolish dog that barks at a passing bird\"\n\n\n\nIf we have several character objects in a vector, we can find the subset that contains a specific sequence.\n\n\ncharacter_vec &lt;- c(w,x,y,z)\ncharacter_vec\n\n[1] \"\\\"Learning R is Fun,\\\" said Rodney.\"                                       \n[2] \"Rodney\"                                                                    \n[3] \"Dyer\"                                                                      \n[4] \"Bob Marley once said, \\\"It is a foolish dog that barks at a passing bird\\\"\"\n\n\n\nstr_detect( character_vec, \"r\")\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n\nHowever, it is case sensitive\n\nstr_detect( character_vec, \"R\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\n\n\nDeleting\nThis is an easy one, if we want to remove one (the first occurence of) an item,\n\nstr_remove(z,\"dog\")\n\n[1] \"Bob Marley once said, \\\"It is a foolish  that barks at a passing bird\\\"\"\n\n\nOr all of them.\n\nstr_remove_all(z, \"a\")\n\n[1] \"Bob Mrley once sid, \\\"It is  foolish dog tht brks t  pssing bird\\\"\"\n\n\nWe can also remove compoennts by truncation\n\nstr_trunc( character_vec , 20)\n\n[1] \"\\\"Learning R is Fu...\" \"Rodney\"                \"Dyer\"                 \n[4] \"Bob Marley once s...\" \n\n\n\n\nReplacing\nThere are times when we are wanting to take some component within a string and replace it with another one—independent of the location of the item to be replaced within the string.\n\ncat( str_replace(z, \"Bob Marley\", \"Rodney\") )\n\nRodney once said, \"It is a foolish dog that barks at a passing bird\"\n\n\nIf we do know the location (character location) and size (str_length) of what we are replacing, then we can use those numerical values direction.\n\nstr_sub(z, 1, 10)\n\n[1] \"Bob Marley\"\n\n\n\n\nManipulating\n\nMaking the string all lower case.\n\n\nstr_to_lower(z)\n\n[1] \"bob marley once said, \\\"it is a foolish dog that barks at a passing bird\\\"\"\n\n\n\nMaking it all uppercase.\n\n\nstr_to_upper(z)\n\n[1] \"BOB MARLEY ONCE SAID, \\\"IT IS A FOOLISH DOG THAT BARKS AT A PASSING BIRD\\\"\"\n\n\n\nCreating Title Case text.\n\n\nstr_to_title(z)\n\n[1] \"Bob Marley Once Said, \\\"It Is A Foolish Dog That Barks At A Passing Bird\\\"\"\n\n\n\nCapitalizing it as if it were a sentence.\n\n\nstr_to_sentence( \"this is getting a bit old, isn't it?\")\n\n[1] \"This is getting a bit old, isn't it?\"\n\n\n\ntmp &lt;- str_to_sentence(\"rodney exclaimed, \\\"but it doesn't know about internal quoted sentence fragments!\\\" and then sat down.\") \ncat(tmp)\n\nRodney exclaimed, \"but it doesn't know about internal quoted sentence fragments!\" And then sat down."
  },
  {
    "objectID": "text_narrative.html#regular-expressions",
    "href": "text_narrative.html#regular-expressions",
    "title": "Text Based Data",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nA regular expression, or regex is a concise language used for detecting and describing patterns found in language. An entire course could be taught on the use of regex but here we’ll only spend enough time on it so that you know some kind of magic exists in the universe and can explore it if you need to in the future.\nFor this example, I’m going to use some text data that I have been playing with regarding curriculum development.\n\nlibrary( readr )\nurl &lt;- \"https://raw.githubusercontent.com/DyerlabTeaching/Textual-Data/refs/heads/main/data/ENVSclasses.txt?token=GHSAT0AAAAAACWO27UIA46V72P7DBZEP5EKZYZFFXQ\"\nread_lines( url ) -&gt; envs\n\nThis goes out and grabs the data, and reads it in as a vector of type character.\n\nclass( envs )\n\n[1] \"character\"\n\n\nIf we take a look at it, we can see these are the listings for the undergraduate courses in Environmental Studies at VCU.\n\nhead(envs,17)\n\n [1] \"ENVS 101. Introduction to Environmental Studies I. 3 Hours.\"                \n [2] \"Semester course; 3 lecture hours. 3 credits. Enrollment is restricted to\"   \n [3] \"environmental studies majors. Study of contemporary issues related to\"      \n [4] \"environmental studies including sustainability, biological conservation,\"   \n [5] \"global change and an overview of the core earth systems.\"                   \n [6] \"ENVS 102. Introduction to Environmental Studies II. 3 Hours.\"               \n [7] \"Semester course; 3 lecture hours. 3 credits. Prerequisite: ENVS 101 or\"     \n [8] \"permission of instructor. Enrollment is restricted to environmental studies\"\n [9] \"majors. Studies of contemporary issues related to government policy and\"    \n[10] \"environmental issues at local to international scales.\"                     \n[11] \"886 Undergraduate courses\"                                                  \n[12] \"ENVS 105. Physical Geology. 3 Hours.\"                                       \n[13] \"Semester course; 3 lecture hours. 3 credits. A descriptive approach to\"     \n[14] \"physical geology dealing with the history and structure of the earth,\"      \n[15] \"catastrophic events and geology as it relates to the contemporary\"          \n[16] \"environment. An optional laboratory, ENVZ 105, may be taken with this\"      \n[17] \"course.\"                                                                    \n\n\nThere are a couple of things to notice:\n\nThe lines are short and the entry for each course is spread across several lines.\n\nEach course has a 4-letter code, a 3-digit number, a title, and then ends with the number of hours for the class.\nThere are five lines necessary to describe ENVS 101 but six for ENVS 105.\nLine 11 appears to be a page number and page heading and not part of any course description.\n\nLet’s say we wanted to extract some information about the course number, name, and number of hours from these data. This subset of data only has 262 line of text but if we were looking at all the courses at VCU, we would be faced with 25,945 lines of text! It could be done by hand but… that does not scale too well and I’ve got a lot better things to do than spend a year working on this simple task.\n\nMatching\nLet’s start by matching. Let’s look at what we did above and see if there are any tools we can use.\nWe do know that str_detect() will give us a TRUE/FALSE for any match. Let’s try that and take a look at the results.\n\nidx &lt;- str_detect(envs,\"ENVS\")\nhead( envs[idx] )\n\n[1] \"ENVS 101. Introduction to Environmental Studies I. 3 Hours.\"           \n[2] \"ENVS 102. Introduction to Environmental Studies II. 3 Hours.\"          \n[3] \"Semester course; 3 lecture hours. 3 credits. Prerequisite: ENVS 101 or\"\n[4] \"ENVS 105. Physical Geology. 3 Hours.\"                                  \n[5] \"ENVS 201. Earth System Science. 3 Hours.\"                              \n[6] \"ENVS 222. Electronic Portfolios. 1 Hour.\"                              \n\n\nOK, except that line 3 isn’t a title line, it just has an ENVS in it.\nMoveover, it also assumes that we are going to only be using ENVS but if we look at the end of the data set, we see that the lab courses in Environmental Studies are encoded as ENVZ and these will be totally ignored. Moreover, we would need to know, a priori, what all the program codes were before we started if we were going to take this approach.\n\ntail( envs )\n\n[1] \"Laboratory exercises coordinated with ENVS 335 lectures.\"                 \n[2] \"ENVZ 401. Meteorology and Climatology Laboratory. 1 Hour.\"                \n[3] \"Semester course; 3 laboratory hours. 1 credit. Pre- or corequisite:\"      \n[4] \"ENVS 401. A series of laboratory and field experiments designed to\"       \n[5] \"quantify the elements of weather and climate and to interpret their local\"\n[6] \"temporal and spatial variations.\"                                         \n\n\nUsing regex we can define a character pattern to look for. Let’s start by just taking the first line of text and using that to learn about pattern matching.\n\nenvs101 &lt;- envs[1]\nenvs101\n\n[1] \"ENVS 101. Introduction to Environmental Studies I. 3 Hours.\"\n\n\nThe stringr library has a helper function that allows us to see what parts of a string are being matched by a specific pattern. This function is str_view() and it colors and puts into angle brackets, the part that is matched.\nSo, looking for the characters ENVS looks like:\n\nstr_view(envs101,\"ENVS\")\n\n[1] │ &lt;ENVS&gt; 101. Introduction to Environmental Studies I. 3 Hours.\n\n\nand looking for 101 yields.\n\nstr_view( envs101, \"101\")\n\n[1] │ ENVS &lt;101&gt;. Introduction to Environmental Studies I. 3 Hours.\n\n\nIf ther is no match, nothing is returned:\n\nstr_view( envs101, \"Rodney\")\n\nand if many things are matched, it will highlight each of them.\n\nstr_view( envs101, \"o\")\n\n[1] │ ENVS 101. Intr&lt;o&gt;ducti&lt;o&gt;n t&lt;o&gt; Envir&lt;o&gt;nmental Studies I. 3 H&lt;o&gt;urs.\n\n\nSo, let’s get more general and look for patterns. These patterns are encoded using square brackets.\n\nMatching on any digit, which is defined as [:digit:].\n\n\nstr_view( envs101, \"[:digit:]\")\n\n[1] │ ENVS &lt;1&gt;&lt;0&gt;&lt;1&gt;. Introduction to Environmental Studies I. &lt;3&gt; Hours.\n\n\n\nMatching any non-numeric character, [:alpha:]\n\n\nstr_view( envs101, \"[:alpha:]\")\n\n[1] │ &lt;E&gt;&lt;N&gt;&lt;V&gt;&lt;S&gt; 101. &lt;I&gt;&lt;n&gt;&lt;t&gt;&lt;r&gt;&lt;o&gt;&lt;d&gt;&lt;u&gt;&lt;c&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt; &lt;t&gt;&lt;o&gt; &lt;E&gt;&lt;n&gt;&lt;v&gt;&lt;i&gt;&lt;r&gt;&lt;o&gt;&lt;n&gt;&lt;m&gt;&lt;e&gt;&lt;n&gt;&lt;t&gt;&lt;a&gt;&lt;l&gt; &lt;S&gt;&lt;t&gt;&lt;u&gt;&lt;d&gt;&lt;i&gt;&lt;e&gt;&lt;s&gt; &lt;I&gt;. 3 &lt;H&gt;&lt;o&gt;&lt;u&gt;&lt;r&gt;&lt;s&gt;.\n\n\n\nMatching punctuation, [:punct:]\n\n\nstr_view( envs101, \"[:punct:]\")\n\n[1] │ ENVS 101&lt;.&gt; Introduction to Environmental Studies I&lt;.&gt; 3 Hours&lt;.&gt;\n\n\n\nWe can also specify the case of the punctuation.\n\n\nstr_view( envs101, \"[:lower:]\")\n\n[1] │ ENVS 101. I&lt;n&gt;&lt;t&gt;&lt;r&gt;&lt;o&gt;&lt;d&gt;&lt;u&gt;&lt;c&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt; &lt;t&gt;&lt;o&gt; E&lt;n&gt;&lt;v&gt;&lt;i&gt;&lt;r&gt;&lt;o&gt;&lt;n&gt;&lt;m&gt;&lt;e&gt;&lt;n&gt;&lt;t&gt;&lt;a&gt;&lt;l&gt; S&lt;t&gt;&lt;u&gt;&lt;d&gt;&lt;i&gt;&lt;e&gt;&lt;s&gt; I. 3 H&lt;o&gt;&lt;u&gt;&lt;r&gt;&lt;s&gt;.\n\nstr_view( envs101, \"[:upper:]\")\n\n[1] │ &lt;E&gt;&lt;N&gt;&lt;V&gt;&lt;S&gt; 101. &lt;I&gt;ntroduction to &lt;E&gt;nvironmental &lt;S&gt;tudies &lt;I&gt;. 3 &lt;H&gt;ours.\n\n\n\nOr even spaces\n\n\nstr_view( envs101, \"[:space:]\")\n\n[1] │ ENVS&lt; &gt;101.&lt; &gt;Introduction&lt; &gt;to&lt; &gt;Environmental&lt; &gt;Studies&lt; &gt;I.&lt; &gt;3&lt; &gt;Hours.\n\n\n\n\nCombining Matches\nThat is helpful in some cases. But now we can start combining these things.\n\nLet’s mix a pattern and a fixed set of characters.\n\n\n\nstr_view( envs101, \"[:digit:] Hours\")\n\n[1] │ ENVS 101. Introduction to Environmental Studies I. &lt;3 Hours&gt;.\n\n\n\nHow about multiple patterns.\n\n\nstr_view( envs101, \"[:upper:][:space:][:digit:]\")\n\n[1] │ ENV&lt;S 1&gt;01. Introduction to Environmental Studies I. 3 Hours.\n\n\n\nOr multiples of the same pattern. To have the exact number of items, use a single number enclosed in curly brackets right after ther pattern. Here we are matching 4-upper case digits.\n\n\nstr_view(envs101, \"[:upper:]{4}\")\n\n[1] │ &lt;ENVS&gt; 101. Introduction to Environmental Studies I. 3 Hours.\n\n\nWe could also match those followed by a space and three numbers\n\nstr_view( envs101, \"[:upper:]{4}[:space:][:digit:]{3}\")\n\n[1] │ &lt;ENVS 101&gt;. Introduction to Environmental Studies I. 3 Hours.\n\n\n\nWe can generalize this a bit by asking for “zero or one” or “zero or more”—IMHO this is a terrible thing to match as a single thing.\n\n\nstr_view( envs101, \"[:punct:]?\")\n\n[1] │ &lt;&gt;E&lt;&gt;N&lt;&gt;V&lt;&gt;S&lt;&gt; &lt;&gt;1&lt;&gt;0&lt;&gt;1&lt;.&gt;&lt;&gt; &lt;&gt;I&lt;&gt;n&lt;&gt;t&lt;&gt;r&lt;&gt;o&lt;&gt;d&lt;&gt;u&lt;&gt;c&lt;&gt;t&lt;&gt;i&lt;&gt;o&lt;&gt;n&lt;&gt; &lt;&gt;t&lt;&gt;o&lt;&gt; &lt;&gt;E&lt;&gt;n&lt;&gt;v&lt;&gt;i&lt;&gt;r&lt;&gt;o&lt;&gt;n&lt;&gt;m&lt;&gt;e&lt;&gt;n&lt;&gt;t&lt;&gt;a&lt;&gt;l&lt;&gt; &lt;&gt;S&lt;&gt;t&lt;&gt;u&lt;&gt;d&lt;&gt;i&lt;&gt;e&lt;&gt;s&lt;&gt; &lt;&gt;I&lt;.&gt;&lt;&gt; &lt;&gt;3&lt;&gt; &lt;&gt;H&lt;&gt;o&lt;&gt;u&lt;&gt;r&lt;&gt;s&lt;.&gt;&lt;&gt;\n\nstr_view( envs101, \"[:punct:]*\")\n\n[1] │ &lt;&gt;E&lt;&gt;N&lt;&gt;V&lt;&gt;S&lt;&gt; &lt;&gt;1&lt;&gt;0&lt;&gt;1&lt;.&gt;&lt;&gt; &lt;&gt;I&lt;&gt;n&lt;&gt;t&lt;&gt;r&lt;&gt;o&lt;&gt;d&lt;&gt;u&lt;&gt;c&lt;&gt;t&lt;&gt;i&lt;&gt;o&lt;&gt;n&lt;&gt; &lt;&gt;t&lt;&gt;o&lt;&gt; &lt;&gt;E&lt;&gt;n&lt;&gt;v&lt;&gt;i&lt;&gt;r&lt;&gt;o&lt;&gt;n&lt;&gt;m&lt;&gt;e&lt;&gt;n&lt;&gt;t&lt;&gt;a&lt;&gt;l&lt;&gt; &lt;&gt;S&lt;&gt;t&lt;&gt;u&lt;&gt;d&lt;&gt;i&lt;&gt;e&lt;&gt;s&lt;&gt; &lt;&gt;I&lt;.&gt;&lt;&gt; &lt;&gt;3&lt;&gt; &lt;&gt;H&lt;&gt;o&lt;&gt;u&lt;&gt;r&lt;&gt;s&lt;.&gt;&lt;&gt;\n\n\nBut for our purposes, we can ask for “one or more” by appending a plus sign.\n\nstr_view( envs101, \"[:digit:]+\")\n\n[1] │ ENVS &lt;101&gt;. Introduction to Environmental Studies I. &lt;3&gt; Hours.\n\n\nor match to digits, punctuation, or letters (one or more) using the shorthand .+ notation. It is the period that matches anything and the plus that does one or more of them.\n\nstr_view( envs101, \".+\")\n\n[1] │ &lt;ENVS 101. Introduction to Environmental Studies I. 3 Hours.&gt;\n\n\nThis becomes helpful a bit later when we are trying to anchor the course designation (e.g., ENVS 101) at the start, ONE OR MORE THING IN THE MIDDLE, and then the end of the line with the number of hours (e.g., 3 Hours.)\n\n\nPositional Matching\nWhere the items is in the string may be of importance to us. For example, consider another line in the data that specifies ENVS101 as a prerequisite. It does not come from a line of text that is the title of the course, it just also happens to match the 4 uppercase letters, space, and three digits pattern.\n\nstr_view( envs[37], \"[:upper:]{4}[:space:][:digit:]{3}\")\n\n[1] │ Semester course; 2 lecture hours. 2 credits. Prerequisites: &lt;ENVS 101&gt;\n\n\nThis is where the position in the line may be of interest.\n\nTo match things that occur at the beginning of the string, we prepend the pattern with the carat symbol.\n\n\nstr_view( envs101, \"^[:upper:]{4}[:space:][:digit:]{3}\")\n\n[1] │ &lt;ENVS 101&gt;. Introduction to Environmental Studies I. 3 Hours.\n\n\nThis matches our first line but not a line where this pattern does not occur in the beginning of the string.\n\nstr_view( envs[37], \"^[:upper:]{4}[:space:][:digit:]{3}\")\n\n\nFor the end of the string, we use the dollar sign to anchor it to the end.\n\n\nstr_view( envs[37], \"[:upper:]{4}[:space:][:digit:]{3}$\")\n\n[1] │ Semester course; 2 lecture hours. 2 credits. Prerequisites: &lt;ENVS 101&gt;\n\n\nwhich is not in the envs101 string\n\nstr_view( envs101, \"[:upper:]{4}[:space:][:digit:]{3}$\")\n\n\n\nPutting it Together\nSo, now let’s pull this all together and see if we can match: 1. The course designation at the start of the line. 2. The title 3. The end of the line with the number of hours.\n\nstr_view( envs101, \"^[:upper:]{4} [:digit:]{3}.*[:digit:] Hours.$\")\n\n[1] │ &lt;ENVS 101. Introduction to Environmental Studies I. 3 Hours.&gt;\n\n\nSo, there is one little extension—and this a common theme we’ve run across—and that has to do with the fact that programmers are a bit lazy. You can subsititue, the square-bracket-colon-word-colon-square-bracket for the following:\n\n[0-9] is shorthand for [:digits:].\n[a-z] is shorthand for [:lower:].\n\n[A-Z] is shorthand for [:upper:].\n\nWhich means that we can go from\n\npattern &lt;- \"^[:upper:]{4} [:digit:]{3}.*[:digit:] Hours.$\"\n\nto this\n\npattern &lt;- \"^[A-Z]{4} [0-9]{3}.+[0-9] Hours.$\"\n\nas our seach pattern.\n\nstr_view( envs101, pattern )\n\n[1] │ &lt;ENVS 101. Introduction to Environmental Studies I. 3 Hours.&gt;\n\n\nInstead of asking a single line, we need to apply this expressions to each line in the data an return the ones that match. For this, we can use grepl, which returns a TRUE/FALSE on matching. The tricky thing here is that the pattern comes first and the vector second (reverse from what we’ve been using.)\nSo for our data, we see\n\ngrepl(pattern, envs )\n\n  [1]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n [13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [49] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[169] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n[229] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[241]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nWhich is great as we can use it for extracting the lines of the data that have our information. Let’s use the grepl as indices and grab the titles from envs using it.\n\nidx &lt;- grepl( pattern, envs )\nenvs[idx] -&gt; titles\ntitles\n\n [1] \"ENVS 101. Introduction to Environmental Studies I. 3 Hours.\"     \n [2] \"ENVS 102. Introduction to Environmental Studies II. 3 Hours.\"    \n [3] \"ENVS 105. Physical Geology. 3 Hours.\"                            \n [4] \"ENVS 201. Earth System Science. 3 Hours.\"                        \n [5] \"ENVS 260. Outdoor Leadership. 3 Hours.\"                          \n [6] \"ENVS 265. Paths to Environmental Leadership. 2 Hours.\"           \n [7] \"ENVS 291. Special Topics in Environmental Studies. 1-4 Hours.\"   \n [8] \"ENVS 300. Sustainable Societies: James River Basin. 3 Hours.\"    \n [9] \"ENVS 301. Introduction to Meteorology. 3 Hours.\"                 \n[10] \"ENVS 310. Introduction to Oceanography. 3 Hours.\"                \n[11] \"ENVS 311. Politics of the Environment. 3 Hours.\"                 \n[12] \"ENVS 315. Energy and the Environment. 3 Hours.\"                  \n[13] \"ENVS 321. Cartography. 3 Hours.\"                                 \n[14] \"ENVS 330. Environmental Pollution. 3 Hours.\"                     \n[15] \"ENVS 332. Environmental Management. 3 Hours.\"                    \n[16] \"ENVS 335. Environmental Geology. 3 Hours.\"                       \n[17] \"ENVS 343. Data Literacy. 4 Hours.\"                               \n[18] \"ENVS 355. Water. 3 Hours.\"                                       \n[19] \"ENVS 360. Outdoor Programming and Event Management. 3 Hours.\"    \n[20] \"ENVS 361. Outdoor Team Building and Group Facilitation. 3 Hours.\"\n[21] \"ENVS 368. Nature Writing. 3 Hours.\"                              \n[22] \"ENVS 370. Applications of Conservation Science. 3 Hours.\"        \n[23] \"ENVS 391. Special Topics in Environmental Studies. 1-4 Hours.\"   \n[24] \"ENVS 401. Meteorology and Climatology. 3 Hours.\"                 \n[25] \"ENVS 411. Oceanography. 3 Hours.\"                                \n[26] \"ENVS 421. Environmental Data Visualization. 3 Hours.\"            \n[27] \"ENVS 430. Invasive Species Management. 3 Hours.\"                 \n[28] \"ENVS 460. Wilderness First Responder. 3 Hours.\"                  \n[29] \"ENVS 461. Wilderness Policy and Practice. 3 Hours.\"              \n[30] \"ENVS 490. Research Seminar in Environmental Studies. 3 Hours.\"   \n[31] \"ENVS 491. Topics in Environmental Studies. 1-4 Hours.\"           \n[32] \"ENVS 492. Independent Study. 1-3 Hours.\"                         \n[33] \"ENVS 493. Environmental Studies Internship. 1-3 Hours.\"          \n[34] \"ENVS 499. Environmental Studies Capstone Experience. 0 Hours.\"   \n\n\nThat looks pretty good! We are almost there.\n\nraw &lt;- str_split(titles, pattern=\"\\\\.\", simplify = TRUE)\ndim(raw)\n\n[1] 34  4\n\nhead(raw)\n\n     [,1]       [,2]                                        [,3]       [,4]\n[1,] \"ENVS 101\" \" Introduction to Environmental Studies I\"  \" 3 Hours\" \"\"  \n[2,] \"ENVS 102\" \" Introduction to Environmental Studies II\" \" 3 Hours\" \"\"  \n[3,] \"ENVS 105\" \" Physical Geology\"                         \" 3 Hours\" \"\"  \n[4,] \"ENVS 201\" \" Earth System Science\"                     \" 3 Hours\" \"\"  \n[5,] \"ENVS 260\" \" Outdoor Leadership\"                       \" 3 Hours\" \"\"  \n[6,] \"ENVS 265\" \" Paths to Environmental Leadership\"        \" 2 Hours\" \"\"  \n\n\nOK, so now we can easily grab these columns and put them into a data.frame.\n\nprogram &lt;- str_split( raw[,1], pattern=\" \", simplify=TRUE)[,1]\nprogram \n\n [1] \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\"\n[11] \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\"\n[21] \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\"\n[31] \"ENVS\" \"ENVS\" \"ENVS\" \"ENVS\"\n\n\n\ncode &lt;- str_split( raw[,1], pattern=\" \", simplify=TRUE)[,2]\ncode &lt;- as.numeric( code )\ncode \n\n [1] 101 102 105 201 260 265 291 300 301 310 311 315 321 330 332 335 343 355 360\n[20] 361 368 370 391 401 411 421 430 460 461 490 491 492 493 499\n\n\n\ntitle &lt;- raw[,2]\ntitle\n\n [1] \" Introduction to Environmental Studies I\"     \n [2] \" Introduction to Environmental Studies II\"    \n [3] \" Physical Geology\"                            \n [4] \" Earth System Science\"                        \n [5] \" Outdoor Leadership\"                          \n [6] \" Paths to Environmental Leadership\"           \n [7] \" Special Topics in Environmental Studies\"     \n [8] \" Sustainable Societies: James River Basin\"    \n [9] \" Introduction to Meteorology\"                 \n[10] \" Introduction to Oceanography\"                \n[11] \" Politics of the Environment\"                 \n[12] \" Energy and the Environment\"                  \n[13] \" Cartography\"                                 \n[14] \" Environmental Pollution\"                     \n[15] \" Environmental Management\"                    \n[16] \" Environmental Geology\"                       \n[17] \" Data Literacy\"                               \n[18] \" Water\"                                       \n[19] \" Outdoor Programming and Event Management\"    \n[20] \" Outdoor Team Building and Group Facilitation\"\n[21] \" Nature Writing\"                              \n[22] \" Applications of Conservation Science\"        \n[23] \" Special Topics in Environmental Studies\"     \n[24] \" Meteorology and Climatology\"                 \n[25] \" Oceanography\"                                \n[26] \" Environmental Data Visualization\"            \n[27] \" Invasive Species Management\"                 \n[28] \" Wilderness First Responder\"                  \n[29] \" Wilderness Policy and Practice\"              \n[30] \" Research Seminar in Environmental Studies\"   \n[31] \" Topics in Environmental Studies\"             \n[32] \" Independent Study\"                           \n[33] \" Environmental Studies Internship\"            \n[34] \" Environmental Studies Capstone Experience\"   \n\n\n\ncredits &lt;- raw[,3]\ncredits &lt;- str_replace(credits, \"Hours\", \"\")\ncredits &lt;- str_trim( credits )\ncredits\n\n [1] \"3\"   \"3\"   \"3\"   \"3\"   \"3\"   \"2\"   \"1-4\" \"3\"   \"3\"   \"3\"   \"3\"   \"3\"  \n[13] \"3\"   \"3\"   \"3\"   \"3\"   \"4\"   \"3\"   \"3\"   \"3\"   \"3\"   \"3\"   \"1-4\" \"3\"  \n[25] \"3\"   \"3\"   \"3\"   \"3\"   \"3\"   \"3\"   \"1-4\" \"1-3\" \"1-3\" \"0\"  \n\n\n\ndata.frame( program, code, title, credits) -&gt; df \nsummary(df)\n\n   program               code          title             credits         \n Length:34          Min.   :101.0   Length:34          Length:34         \n Class :character   1st Qu.:303.2   Class :character   Class :character  \n Mode  :character   Median :349.0   Mode  :character   Mode  :character  \n                    Mean   :346.4                                        \n                    3rd Qu.:418.5                                        \n                    Max.   :499.0                                        \n\n\n\nhead(df)\n\n  program code                                     title credits\n1    ENVS  101   Introduction to Environmental Studies I       3\n2    ENVS  102  Introduction to Environmental Studies II       3\n3    ENVS  105                          Physical Geology       3\n4    ENVS  201                      Earth System Science       3\n5    ENVS  260                        Outdoor Leadership       3\n6    ENVS  265         Paths to Environmental Leadership       2"
  },
  {
    "objectID": "text_narrative.html#extra-credit",
    "href": "text_narrative.html#extra-credit",
    "title": "Text Based Data",
    "section": "Extra Credit",
    "text": "Extra Credit\nLet’s close by doing something fun. Let’s make a wordcloud of the titles from ENVS classes.\n\nstr_split( title, \" \")\n\n[[1]]\n[1] \"\"              \"Introduction\"  \"to\"            \"Environmental\"\n[5] \"Studies\"       \"I\"            \n\n[[2]]\n[1] \"\"              \"Introduction\"  \"to\"            \"Environmental\"\n[5] \"Studies\"       \"II\"           \n\n[[3]]\n[1] \"\"         \"Physical\" \"Geology\" \n\n[[4]]\n[1] \"\"        \"Earth\"   \"System\"  \"Science\"\n\n[[5]]\n[1] \"\"           \"Outdoor\"    \"Leadership\"\n\n[[6]]\n[1] \"\"              \"Paths\"         \"to\"            \"Environmental\"\n[5] \"Leadership\"   \n\n[[7]]\n[1] \"\"              \"Special\"       \"Topics\"        \"in\"           \n[5] \"Environmental\" \"Studies\"      \n\n[[8]]\n[1] \"\"            \"Sustainable\" \"Societies:\"  \"James\"       \"River\"      \n[6] \"Basin\"      \n\n[[9]]\n[1] \"\"             \"Introduction\" \"to\"           \"Meteorology\" \n\n[[10]]\n[1] \"\"             \"Introduction\" \"to\"           \"Oceanography\"\n\n[[11]]\n[1] \"\"            \"Politics\"    \"of\"          \"the\"         \"Environment\"\n\n[[12]]\n[1] \"\"            \"Energy\"      \"and\"         \"the\"         \"Environment\"\n\n[[13]]\n[1] \"\"            \"Cartography\"\n\n[[14]]\n[1] \"\"              \"Environmental\" \"Pollution\"    \n\n[[15]]\n[1] \"\"              \"Environmental\" \"Management\"   \n\n[[16]]\n[1] \"\"              \"Environmental\" \"Geology\"      \n\n[[17]]\n[1] \"\"         \"Data\"     \"Literacy\"\n\n[[18]]\n[1] \"\"      \"Water\"\n\n[[19]]\n[1] \"\"            \"Outdoor\"     \"Programming\" \"and\"         \"Event\"      \n[6] \"Management\" \n\n[[20]]\n[1] \"\"             \"Outdoor\"      \"Team\"         \"Building\"     \"and\"         \n[6] \"Group\"        \"Facilitation\"\n\n[[21]]\n[1] \"\"        \"Nature\"  \"Writing\"\n\n[[22]]\n[1] \"\"             \"Applications\" \"of\"           \"Conservation\" \"Science\"     \n\n[[23]]\n[1] \"\"              \"Special\"       \"Topics\"        \"in\"           \n[5] \"Environmental\" \"Studies\"      \n\n[[24]]\n[1] \"\"            \"Meteorology\" \"and\"         \"Climatology\"\n\n[[25]]\n[1] \"\"             \"Oceanography\"\n\n[[26]]\n[1] \"\"              \"Environmental\" \"Data\"          \"Visualization\"\n\n[[27]]\n[1] \"\"           \"Invasive\"   \"Species\"    \"Management\"\n\n[[28]]\n[1] \"\"           \"Wilderness\" \"First\"      \"Responder\" \n\n[[29]]\n[1] \"\"           \"Wilderness\" \"Policy\"     \"and\"        \"Practice\"  \n\n[[30]]\n[1] \"\"              \"Research\"      \"Seminar\"       \"in\"           \n[5] \"Environmental\" \"Studies\"      \n\n[[31]]\n[1] \"\"              \"Topics\"        \"in\"            \"Environmental\"\n[5] \"Studies\"      \n\n[[32]]\n[1] \"\"            \"Independent\" \"Study\"      \n\n[[33]]\n[1] \"\"              \"Environmental\" \"Studies\"       \"Internship\"   \n\n[[34]]\n[1] \"\"              \"Environmental\" \"Studies\"       \"Capstone\"     \n[5] \"Experience\"   \n\nstr_split( title, \" \", simplify=TRUE)\n\n      [,1] [,2]            [,3]          [,4]            [,5]           \n [1,] \"\"   \"Introduction\"  \"to\"          \"Environmental\" \"Studies\"      \n [2,] \"\"   \"Introduction\"  \"to\"          \"Environmental\" \"Studies\"      \n [3,] \"\"   \"Physical\"      \"Geology\"     \"\"              \"\"             \n [4,] \"\"   \"Earth\"         \"System\"      \"Science\"       \"\"             \n [5,] \"\"   \"Outdoor\"       \"Leadership\"  \"\"              \"\"             \n [6,] \"\"   \"Paths\"         \"to\"          \"Environmental\" \"Leadership\"   \n [7,] \"\"   \"Special\"       \"Topics\"      \"in\"            \"Environmental\"\n [8,] \"\"   \"Sustainable\"   \"Societies:\"  \"James\"         \"River\"        \n [9,] \"\"   \"Introduction\"  \"to\"          \"Meteorology\"   \"\"             \n[10,] \"\"   \"Introduction\"  \"to\"          \"Oceanography\"  \"\"             \n[11,] \"\"   \"Politics\"      \"of\"          \"the\"           \"Environment\"  \n[12,] \"\"   \"Energy\"        \"and\"         \"the\"           \"Environment\"  \n[13,] \"\"   \"Cartography\"   \"\"            \"\"              \"\"             \n[14,] \"\"   \"Environmental\" \"Pollution\"   \"\"              \"\"             \n[15,] \"\"   \"Environmental\" \"Management\"  \"\"              \"\"             \n[16,] \"\"   \"Environmental\" \"Geology\"     \"\"              \"\"             \n[17,] \"\"   \"Data\"          \"Literacy\"    \"\"              \"\"             \n[18,] \"\"   \"Water\"         \"\"            \"\"              \"\"             \n[19,] \"\"   \"Outdoor\"       \"Programming\" \"and\"           \"Event\"        \n[20,] \"\"   \"Outdoor\"       \"Team\"        \"Building\"      \"and\"          \n[21,] \"\"   \"Nature\"        \"Writing\"     \"\"              \"\"             \n[22,] \"\"   \"Applications\"  \"of\"          \"Conservation\"  \"Science\"      \n[23,] \"\"   \"Special\"       \"Topics\"      \"in\"            \"Environmental\"\n[24,] \"\"   \"Meteorology\"   \"and\"         \"Climatology\"   \"\"             \n[25,] \"\"   \"Oceanography\"  \"\"            \"\"              \"\"             \n[26,] \"\"   \"Environmental\" \"Data\"        \"Visualization\" \"\"             \n[27,] \"\"   \"Invasive\"      \"Species\"     \"Management\"    \"\"             \n[28,] \"\"   \"Wilderness\"    \"First\"       \"Responder\"     \"\"             \n[29,] \"\"   \"Wilderness\"    \"Policy\"      \"and\"           \"Practice\"     \n[30,] \"\"   \"Research\"      \"Seminar\"     \"in\"            \"Environmental\"\n[31,] \"\"   \"Topics\"        \"in\"          \"Environmental\" \"Studies\"      \n[32,] \"\"   \"Independent\"   \"Study\"       \"\"              \"\"             \n[33,] \"\"   \"Environmental\" \"Studies\"     \"Internship\"    \"\"             \n[34,] \"\"   \"Environmental\" \"Studies\"     \"Capstone\"      \"Experience\"   \n      [,6]         [,7]          \n [1,] \"I\"          \"\"            \n [2,] \"II\"         \"\"            \n [3,] \"\"           \"\"            \n [4,] \"\"           \"\"            \n [5,] \"\"           \"\"            \n [6,] \"\"           \"\"            \n [7,] \"Studies\"    \"\"            \n [8,] \"Basin\"      \"\"            \n [9,] \"\"           \"\"            \n[10,] \"\"           \"\"            \n[11,] \"\"           \"\"            \n[12,] \"\"           \"\"            \n[13,] \"\"           \"\"            \n[14,] \"\"           \"\"            \n[15,] \"\"           \"\"            \n[16,] \"\"           \"\"            \n[17,] \"\"           \"\"            \n[18,] \"\"           \"\"            \n[19,] \"Management\" \"\"            \n[20,] \"Group\"      \"Facilitation\"\n[21,] \"\"           \"\"            \n[22,] \"\"           \"\"            \n[23,] \"Studies\"    \"\"            \n[24,] \"\"           \"\"            \n[25,] \"\"           \"\"            \n[26,] \"\"           \"\"            \n[27,] \"\"           \"\"            \n[28,] \"\"           \"\"            \n[29,] \"\"           \"\"            \n[30,] \"Studies\"    \"\"            \n[31,] \"\"           \"\"            \n[32,] \"\"           \"\"            \n[33,] \"\"           \"\"            \n[34,] \"\"           \"\"            \n\nas.vector( str_split( title, \" \", simplify=TRUE) ) -&gt; words\n\nwords &lt;- words[ str_length(words) &gt; 0 ]\n\nwords &lt;- sort( words )\nwords\n\n  [1] \"and\"           \"and\"           \"and\"           \"and\"          \n  [5] \"and\"           \"Applications\"  \"Basin\"         \"Building\"     \n  [9] \"Capstone\"      \"Cartography\"   \"Climatology\"   \"Conservation\" \n [13] \"Data\"          \"Data\"          \"Earth\"         \"Energy\"       \n [17] \"Environment\"   \"Environment\"   \"Environmental\" \"Environmental\"\n [21] \"Environmental\" \"Environmental\" \"Environmental\" \"Environmental\"\n [25] \"Environmental\" \"Environmental\" \"Environmental\" \"Environmental\"\n [29] \"Environmental\" \"Environmental\" \"Environmental\" \"Event\"        \n [33] \"Experience\"    \"Facilitation\"  \"First\"         \"Geology\"      \n [37] \"Geology\"       \"Group\"         \"I\"             \"II\"           \n [41] \"in\"            \"in\"            \"in\"            \"in\"           \n [45] \"Independent\"   \"Internship\"    \"Introduction\"  \"Introduction\" \n [49] \"Introduction\"  \"Introduction\"  \"Invasive\"      \"James\"        \n [53] \"Leadership\"    \"Leadership\"    \"Literacy\"      \"Management\"   \n [57] \"Management\"    \"Management\"    \"Meteorology\"   \"Meteorology\"  \n [61] \"Nature\"        \"Oceanography\"  \"Oceanography\"  \"of\"           \n [65] \"of\"            \"Outdoor\"       \"Outdoor\"       \"Outdoor\"      \n [69] \"Paths\"         \"Physical\"      \"Policy\"        \"Politics\"     \n [73] \"Pollution\"     \"Practice\"      \"Programming\"   \"Research\"     \n [77] \"Responder\"     \"River\"         \"Science\"       \"Science\"      \n [81] \"Seminar\"       \"Societies:\"    \"Special\"       \"Special\"      \n [85] \"Species\"       \"Studies\"       \"Studies\"       \"Studies\"      \n [89] \"Studies\"       \"Studies\"       \"Studies\"       \"Studies\"      \n [93] \"Studies\"       \"Study\"         \"Sustainable\"   \"System\"       \n [97] \"Team\"          \"the\"           \"the\"           \"to\"           \n[101] \"to\"            \"to\"            \"to\"            \"to\"           \n[105] \"Topics\"        \"Topics\"        \"Topics\"        \"Visualization\"\n[109] \"Water\"         \"Wilderness\"    \"Wilderness\"    \"Writing\"      \n\n\nWe need to have the data in the format of\nWord | Count\nso let’s use our dplyr skills.\n\ndata.frame( words, count = 1 ) |&gt; \n  mutate( word = factor( words ) ) |&gt; \n  group_by( word ) |&gt;\n  summarize( freq = sum( count )) |&gt; \n  arrange( -freq ) -&gt; tdm \n\ntdm \n\n# A tibble: 62 × 2\n   word           freq\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 Environmental    13\n 2 Studies           8\n 3 and               5\n 4 to                5\n 5 in                4\n 6 Introduction      4\n 7 Management        3\n 8 Outdoor           3\n 9 Topics            3\n10 Data              2\n# ℹ 52 more rows\n\n\n\nif( !require( wordcloud ) ) { \n  install.packages(\"wordcloud\")\n  install.packages(\"wordcloud2\")  # for more fancy\n} \n\nLoading required package: wordcloud\n\n\nLoading required package: RColorBrewer\n\n\n\nlibrary( wordcloud )\nwordcloud( words = tdm$word, \n           freq = tdm$freq )\n\n\n\n\n\n\n\n\n\nwordcloud( words = tdm$word, \n           freq = tdm$freq,\n           scale=c(3.5,0.25) ) \n\n\n\n\n\n\n\n\n\nwordcloud( words = tdm$word, \n           freq = tdm$freq,\n           scale=c(3.5,0.25),\n           min.freq = 1 ) \n\n\n\n\n\n\n\n\n\nwordcloud( words = tdm$word, \n           freq = tdm$freq,\n           scale=c(3.5,0.25),\n           min.freq = 2 ) \n\n\n\n\n\n\n\n\n\nwordcloud( words = tdm$word, \n           freq = tdm$freq,\n           scale=c(3.5,0.25),\n           min.freq = 2,\n           colors = brewer.pal(8,\"Dark2\")) \n\n\n\n\n\n\n\n\n\nlibrary( wordcloud2 )\nwordcloud2(data = tdm)"
  },
  {
    "objectID": "ggplot_narrative.html",
    "href": "ggplot_narrative.html",
    "title": "ggplot2 Graphics",
    "section": "",
    "text": "In base R, the graphics are generally produced by adding a lot of optional arguments to a single function such as plot() or barplot() or boxplot(). We can get some kinds of overlays using text() or points() or lines() but there is not a cohesive framework for setting this up. For even moderately complex graphical display, these approaches become unwieldy when we have to cram all that information into extra optional arguments.\nConsider the graph below whose data are from a 2011 article in The Economist measuring human development and perception of corruption for 173 countries (Figure 1). Both the amount of data and the way in which the data are displayed (physically and aesthetically) are somewhat complex.\nFigure 1: Corruption and Human Develoment among OECD countries from The Economist magazine.\nThis graphic is constructed from several additive components1 including:\nTruth be told (and you can look at the RMD of this file to verify), this one graphic required 42 relatively terse lines of code to construct! If all of that code was stuffed into the optional arguments for a few functions, I think I would go mad.\nLuckily for us, there are people who spend a lot of time working on these issues and thinking about how to best help us effectively display data. One of these individuals was Leland Wilkinson, whose book The Grammar of Graphics defined just such a system.\nThis philosophy has been inserted into the R Ecosystem by Hadley Wickham in the ggplot2 library, which is descbribed as:\nThroughout the majority of this course, we will be using this library and this approach for all but the most trivial of graphical displays."
  },
  {
    "objectID": "ggplot_narrative.html#basic-ggplot",
    "href": "ggplot_narrative.html#basic-ggplot",
    "title": "ggplot2 Graphics",
    "section": "Basic ggplot",
    "text": "Basic ggplot\nAs outlined above, the basis of this appraoch is an additive (and iterative) process of creating a graphic. This all starts with the data. For our purposes, we will use the same iris data.frame as in the previous section on base graphics.\n\n\n\nThe iris data\n\n\n\nsummary( iris )\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nWe start building a graphic using the ggplot() function and passing it the data.frame object. This will initialize the graphic, though it will not plot anything.\n\nlibrary(ggplot2)\n\nggplot( iris )\n\n\n\n\n\n\n\n\nNext, we need to tell the plot which variables it will be using from the data.frame. For simplicity, we do not need to make special data objects with just the variables we want to plot, we can pass around the whole data.frame object and just indicate to ggplot which ones we want to use by specifying the aesthetics to be used.\n\nggplot( iris , aes( x=Sepal.Length ) )\n\n\n\n\n\n\n\n\nAt this point, there is enough information to make an axis in the graph because the underlying data has been identified. What has not been specified to date is the way in which we want to represent the data. To do this, we add geometries to the graph. In this case, I’m going to add a histogram\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow we have a base graph!"
  },
  {
    "objectID": "ggplot_narrative.html#aestheics-and-scope",
    "href": "ggplot_narrative.html#aestheics-and-scope",
    "title": "ggplot2 Graphics",
    "section": "Aestheics and Scope",
    "text": "Aestheics and Scope\nThe location of the data and the aes() determines the scope of the assignment. What I mean by this is:\n\nIf the data and aes() is in the the ggplot() function, then everything in the whole plot inherits that assignment.\nIf you put them in one or more of the components you add to ggplot() then the they are localized to only those layers.\n\nSo the following statements are all identical for this most basic of plots.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\nggplot( iris ) + geom_historgram( aes(x=Sepal.Length) )\nggplot() + geom_histogram( aes(x=Sepal.Length), data=iris)\n\n\nIn the first case, the geom_histogram() inherits both data and aesthetics from ggplot().\n\nIn the second one, it inherits only the data but has it’s own specification for aesthetics.\nIn the last one, ggplot() only specifies the presence of a graph and all the data and aesthetics are localized within geom_histogram() function.\n\nWhere this becomes important is when we want to make more complicated graphics like the one above. The data that has the country CDI and HDI also has the names of the countries. However, only a subset of the country names are plot. This is because both the geometric layer and the text layer that has the names are using different data.frame objects.\nHere is a more simplistic example where I overlay a density plot (as a red line) on top of the histogram.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram() + geom_density( col=\"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBoth the geom_histogram and the geom_density use the same data and same specification for how to deal with the y-axis. However, the density is depicted as a frequency on the y-axis whereas the histogram uses counts. Also notice how the col=\"red\" is localized just for the geom_density() layer.\nWe can override the way in which geom_histogram uses the y-axis by changing the aesthetics for that particular geometric layer. Here, I’m goint to add another aes() just within the geom_histogram() function and have it treat y as the density rather than the count (yes that is two periods before and after the word density).\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram(aes(y=..density..)) + geom_density( col=\"red\" )\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBy default, everything inside the ggplot() function call is inherited by all the remaining components unless it is specifically overridden. Here is a more pedantic version where only the raw data.frame is in the ggplot and the rest is in each of the geometric layers.\n\nggplot( iris ) + \n  geom_histogram( aes(x=Sepal.Length, y=..density..) ) + \n  geom_density( aes(x=Sepal.Length), col=\"red\", lwd=2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "ggplot_narrative.html#labels-titles",
    "href": "ggplot_narrative.html#labels-titles",
    "title": "ggplot2 Graphics",
    "section": "Labels & Titles",
    "text": "Labels & Titles\nJust like we added geometric layers to the plot to make histograms and densities, we do the same for labels and titles.\n\nggplot( iris,  aes(x=Sepal.Length) ) + \n  geom_histogram( aes(y=..density..), bins = 10, fill=\"lightgray\", col=\"darkgrey\" ) + \n  geom_density( col=\"red\", lwd=1.5) + \n  xlab(\"Length\") + ylab(\"Density\") + \n  ggtitle(\"Sepal Lengths for Three Iris Species\")"
  },
  {
    "objectID": "ggplot_narrative.html#scatter-plots",
    "href": "ggplot_narrative.html#scatter-plots",
    "title": "ggplot2 Graphics",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nWith two columns of data, we can make the old scatter plot using the geom_point() function.\n\nggplot( iris, aes(x=Sepal.Length, y=Sepal.Width) ) + geom_point( col=\"purple\") \n\n\n\n\n\n\n\n\nIn this plot, we are hiding some of the information by having all the points be the same color and shape. We could have a geom_point for each species as follows:\n\nggplot(  ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 1:50,], col=\"red\") + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 51:100,], col=\"yellow\" ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ iris$Species == \"virginica\", ], col=\"darkgreen\" ) \n\n\n\n\n\n\n\n\nBut that is a lot of typing. In cases like this, where there is a an actual column of data that we want to use to change the appearance (e.g., in this case the Species column), we can put this within the aes() directly and ggplot() will handle the specifics for you. Anything we do to reduce the amount of typing we must do is going to help us be more accurate analysts.\n\nggplot( iris, aes( x = Sepal.Length, y = Sepal.Width, col=Species) ) + geom_point()\n\n\n\n\n\n\n\n\n\nIn or Out of aes()\nNotice in the last graph I put the name of the data column in the aesthetic but have the color (col) within the aes() function call in the graph before that, I put color outside of the aes() in the geom_point() function. What gives? Here is a simple rule.\n\nIf information from within the data.frame is needed to customize the display of data then it must be designated within the aes(), whereas if the display of the data is to be applied to the entire geometric layer, it is specified outside of the aes() call.\n\nHere is an example, where I have the color of the shapes determined by a value in the data.frame but have the shape2 applied to all the points, independent of any data in the data.frame.\n\nggplot( iris ) + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species), shape=5)\n\n\n\n\n\n\n\n\nWe can build these things in an iterative fashion making things easier to read. In what follows I will use the basic plot from above but assign it to the variable p as I add things to it. It can be as iterative as you like and you can add a bunch of stuff and wait until the end to display it.\n\np &lt;- ggplot( iris ) \np &lt;- p + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species, shape=Species), size=3, alpha=0.75 ) \np &lt;- p + xlab(\"Sepal Length\") \np &lt;- p + ylab(\"Sepal Width\")\n\nThe overall class of the plot varible is\n\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nAnd there is no plot output until we display it specifically.\n\np"
  },
  {
    "objectID": "ggplot_narrative.html#themes",
    "href": "ggplot_narrative.html#themes",
    "title": "ggplot2 Graphics",
    "section": "Themes",
    "text": "Themes\nThe overall coloration of the plot is determined by the theme.\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\n\n\np + theme_minimal()\n\n\n\n\n\n\n\n\n\np + theme_linedraw()\n\n\n\n\n\n\n\n\n\np + theme_void()\n\n\n\n\n\n\n\n\nYou can even define your own themes to customize all the text and lines.\nOne thing that I like to do is to specify a default theme for all my plots. You can accomplish this using theme_set() and from this point forward, this theme will be used as the default (again, we need to try as hard as possible to minimzie the amount of typing we do to minimize the amount of mistakes we make).\n\ntheme_set( theme_bw() )"
  },
  {
    "objectID": "ggplot_narrative.html#boxplots",
    "href": "ggplot_narrative.html#boxplots",
    "title": "ggplot2 Graphics",
    "section": "Boxplots",
    "text": "Boxplots\n\nggplot( iris, aes( x = Sepal.Length) ) + geom_boxplot( notch=TRUE )\n\n\n\n\n\n\n\n\n\nggplot( iris, aes(x=Species, y=Sepal.Length) )  + geom_boxplot( notch=TRUE )"
  },
  {
    "objectID": "ggplot_narrative.html#overlays",
    "href": "ggplot_narrative.html#overlays",
    "title": "ggplot2 Graphics",
    "section": "Overlays",
    "text": "Overlays\nJust like in the previous\n\np &lt;- ggplot( iris, aes(Sepal.Length, Sepal.Width) ) + \n  geom_point(col=\"red\") + \n  xlab(\"Sepal Length\") + \n  ylab(\"Sepal Width\")\n\nThe order by which you add the components to the ggplot() will determine the order of the layers from bottom to top—the. Layers added earlier will be covered by content in layers that are added later. Compare the following plot that takes the length and width of the sepals and overlays a linear regression line over the top.\n\np + geom_point(col=\"red\") + \n  stat_smooth( formula = y ~ x, method=\"lm\", alpha=1.0)\n\n\n\n\n\n\n\n\nCompare that plot to the one below. Notice how puting stat_smooth() in front of the call to geom_point() layes the regression smoothing line and error zone underneath the points.\n\np + stat_smooth(formula = y ~ x, method=\"lm\", alpha=1.0) + \n  geom_point(col=\"red\")"
  },
  {
    "objectID": "ggplot_narrative.html#labeling",
    "href": "ggplot_narrative.html#labeling",
    "title": "ggplot2 Graphics",
    "section": "Labeling",
    "text": "Labeling\nWe can create two kinds of annotations, text on the raw graph and text associated with some of the points. Labels of the first kind can be added direclty by placing raw data inside the aes() function.\nI’ll start by taking the correlation between sepal width and length.\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nAnd then grab the raw data from it and make a message.\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThat I’ll stick onto the graph directly\n\np + geom_text( aes(x=7.25, y=4.25, label=cor.text))\n\nWarning in geom_text(aes(x = 7.25, y = 4.25, label = cor.text)): All aesthetics have length 1, but the data has 150 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nAlternatively, we may want to label specific points. Here I find the mean values for each species.\n\nmean_Length &lt;- by( iris$Sepal.Length, iris$Species, mean, simplify = TRUE)\nmean_Width &lt;- by( iris$Sepal.Width, iris$Species, mean, simplify = TRUE)\nmean_Values &lt;- data.frame(  Species = levels( iris$Species), \n                            Sepal.Length = as.numeric( mean_Length ), \n                            Sepal.Width = as.numeric( mean_Width ) ) \nmean_Values\n\n     Species Sepal.Length Sepal.Width\n1     setosa        5.006       3.428\n2 versicolor        5.936       2.770\n3  virginica        6.588       2.974\n\n\nTo plot and label these mean values, I’m going to use two steps. First, since I named the columns of the new data.frame the same as before, we can just inherit the aes() but substitute in this new data.frame and add label=Species to the the aesthetics.\n\np + geom_text( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nBut that is a bit messy. Here is a slick helper library for that that will try to minimize the overlap.\n\nlibrary( ggrepel ) \np + geom_label_repel( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nSlick."
  },
  {
    "objectID": "ggplot_narrative.html#questions",
    "href": "ggplot_narrative.html#questions",
    "title": "ggplot2 Graphics",
    "section": "Questions",
    "text": "Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "ggplot_narrative.html#footnotes",
    "href": "ggplot_narrative.html#footnotes",
    "title": "ggplot2 Graphics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiterally, we add these toghter using the plus ‘+’ sign just like we were going to develop an equation.↩︎\nThe shapes are the same as the pch offerings covered in the lecture on graphing using Base R routines here.↩︎"
  }
]